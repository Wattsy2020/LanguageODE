{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a92153-258b-4d12-b136-c1d4ff34b715",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Requires an Nvidia GPU to run\n",
    "\n",
    "Create a new anaconda environment and run the following commands to install the required libraries \n",
    "```\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "conda install gensim\n",
    "pip install torchdyn\n",
    "pip install git+https://github.com/google-research/torchsde.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dfe412-887c-43b0-ae9e-68e68207a975",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993). Building a Large Annotated Corpus of English: The Penn Treebank\n",
    "\n",
    "```\n",
    "@article{poli2020torchdyn,\n",
    "  title={TorchDyn: A Neural Differential Equations Library},\n",
    "  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},\n",
    "  journal={arXiv preprint arXiv:2009.09346},\n",
    "  year={2020}\n",
    "}\n",
    "```\n",
    "\n",
    "- GloVe\n",
    "\n",
    "- GPT2 paper\n",
    "\n",
    "- Huggingface for their implementation of transformers? Not sure if this has a paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea389f-a316-483d-94f9-3e80e54e517d",
   "metadata": {},
   "source": [
    "# To do\n",
    "- Consider other variants of Neural ODE\n",
    "- Implement and see results from my continuous language modelling idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ff0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from functools import reduce\n",
    "from sklearn.metrics import *\n",
    "from time import time\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469c71b",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "- Build the vocab\n",
    "- Convert text corpus into padded word vector sequences\n",
    "\n",
    "To do\n",
    "- Use LSTM as baseline\n",
    "    - Examine perplexity of model on validation set\n",
    "- Implement Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c916364a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-adcef9dc1f4b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mglove\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"glove-wiki-gigaword-300\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\downloader.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, return_path)\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBASE_DIR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__import__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 502\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~/gensim-data\\glove-wiki-gigaword-300\\__init__.py\u001b[0m in \u001b[0;36mload_data\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove-wiki-gigaword-300'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'glove-wiki-gigaword-300.gz'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[0;32m   1545\u001b[0m         \"\"\"\n\u001b[0;32m   1546\u001b[0m         \u001b[1;31m# from gensim.models.word2vec import load_word2vec_format\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1547\u001b[1;33m         return _load_word2vec_format(\n\u001b[0m\u001b[0;32m   1548\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m             limit=limit, datatype=datatype)\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[1;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, binary_chunk_size)\u001b[0m\n\u001b[0;32m    286\u001b[0m                 vocab_size, vector_size, datatype, unicode_errors, binary_chunk_size)\n\u001b[0;32m    287\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m             \u001b[0m_word2vec_read_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\gensim\\models\\utils_any2vec.py\u001b[0m in \u001b[0;36m_word2vec_read_text\u001b[1;34m(fin, result, counts, vocab_size, vector_size, datatype, unicode_errors, encoding)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_word2vec_read_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcounts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatatype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mline_no\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 215\u001b[1;33m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    216\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"unexpected end of input; is count incorrect or file otherwise damaged?\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    388\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mview\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"B\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m             \u001b[1;31m# Read a chunk of data from the file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 485\u001b[1;33m             \u001b[0mbuf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0muncompress\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_decompressor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecompress\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\gzip.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m                    \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load word embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45e5e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8d4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10001\n"
     ]
    }
   ],
   "source": [
    "# build the vocab\n",
    "corpus = train + valid\n",
    "vocab = {\"<PAD>\": 0}\n",
    "index_vocab = {0 : \"<PAD>\"}\n",
    "for sentence in corpus:\n",
    "    for token in sentence.split(\" \")[1:]:\n",
    "        if token not in vocab:\n",
    "            index = len(vocab)\n",
    "            vocab[token] = index\n",
    "            index_vocab[index] = token\n",
    "\n",
    "# replace penn treebank end sentence token \"\\n\" with glove's end sentence token \".\"\n",
    "index = vocab[\"\\n\"]\n",
    "vocab.pop(\"\\n\")         \n",
    "vocab[\".\"] = index\n",
    "index_vocab[index] = \".\"\n",
    "\n",
    "# view size\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb19c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentences and convert words to their glove vector to get input features\n",
    "# convert to 1 hot vocab and shift 1 to the left to get output labels (converting to 1 hot takes too much memory, so just store indices and convert later)\n",
    "# use left padding, as we want the hidden state at the end (right) to ignore the padding\n",
    "# returns word_vector_dataset, labels\n",
    "def preprocess(dataset, sequence_length, wv):\n",
    "    embedding_size = wv[\"hello\"].shape[0]\n",
    "    processed = np.zeros((len(dataset), sequence_length, embedding_size))\n",
    "    labels = np.zeros((len(dataset), sequence_length, 1))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        tokens = dataset[i].split(\" \")[1:]\n",
    "        \n",
    "        # get the word vectors for all of the tokens, removing out of vocabulary (OOV) tokens\n",
    "        tokens_np = np.zeros((len(tokens), embedding_size))\n",
    "        labels_np = np.zeros((len(tokens), 1))\n",
    "        j = 0\n",
    "        for word in tokens:\n",
    "            if word == \"\\n\": word = \".\" # replace PennTreebank end sentence token '\\n' with glove end sentence token \".\"\n",
    "            if word not in wv: continue # ignore OOV tokens\n",
    "            if j < sequence_length - 1: # only add sequence_length - 1 tokens at max\n",
    "                # so that there is always a 0 vector at the start so the model learns most common starting words\n",
    "                tokens_np[j, :] = wv[word]\n",
    "            # we can look ahead to find the next word to set as the label for the last word\n",
    "            if j < sequence_length:\n",
    "                labels_np[j, :] = vocab[word]\n",
    "            else: break\n",
    "            j += 1\n",
    "            \n",
    "        tokens_np = tokens_np[:j-1, :]\n",
    "        labels_np = labels_np[:j, :]\n",
    "        \n",
    "        # add this sentence to the overall dataset, with left padding of 0 vectors\n",
    "        processed[i, sequence_length - tokens_np.shape[0]:, :] = tokens_np\n",
    "        labels[i, sequence_length - labels_np.shape[0]:, :] = labels_np\n",
    "    return processed, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162bc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "train_X, train_y = preprocess(train, sequence_length, glove)\n",
    "valid_X, valid_y = preprocess(valid, sequence_length, glove)\n",
    "test_X , test_y  = preprocess(test,  sequence_length, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bf2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test to check the labelling works\n",
    "assert preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0][-1] == 25, \"Output: {}\".format(preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c3221",
   "metadata": {},
   "source": [
    "# LSTM Baseline\n",
    "Create a baseline RNN and evaluate it's perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40b5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
    "        super().__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convert words to their vectors here\n",
    "        sequence_outputs, hidden_state = self.LSTM(x)\n",
    "        sequence_outputs = self.dropout(sequence_outputs)\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bcb3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_to_tensor(array):\n",
    "    return torch.from_numpy(array).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46abd825",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LSTMModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-5c76f6880941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LSTMModel' is not defined"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, input_size=300, layer_size=300, dropout=0.1)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c704a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 10001])\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# unit test to check that forward propagation works\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "print(model.forward(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441a5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
    "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
    "def perplexity(preds, ground_truth, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        probs.append(preds[0, i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs)\n",
    "\n",
    "# Calculate overall perplexity for a dataset\n",
    "def average_perplexity(model, X, y):\n",
    "    perplexities = [perplexity(model.predict(numpy_to_tensor(X[i:i+1])), y[i]) for i in range(X.shape[0])]\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fc95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "def train_model(model, train_X, train_y, epochs=10, learn_rate=0.01, weight_decay=0.001, minibatch_size=128, print_results=True):\n",
    "    # Prepare data\n",
    "    X = numpy_to_tensor(train_X)\n",
    "    y = numpy_to_tensor(train_y).long()[:, :, 0]\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Ensure this runs on gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        model.train() # set to train flag\n",
    "        start_ts = time()\n",
    "        \n",
    "        # shuffle the data\n",
    "        new_indices = torch.randperm(n_samples)\n",
    "        X = X[new_indices, :, :] \n",
    "        y = y[new_indices, :]\n",
    "        \n",
    "        for batch_n in range(int(np.ceil(n_samples/minibatch_size))):\n",
    "            # get the minibatch\n",
    "            start_index = batch_n * minibatch_size\n",
    "            end_index = min(start_index + minibatch_size, n_samples)\n",
    "            batch_X = X[start_index: end_index, :, :]\n",
    "            batch_y = y[start_index: end_index, :]\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X) \n",
    "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # evaluate performance on part of the data (for memory reasons we take a subsample)\n",
    "        if print_results:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                preds = np.argmax(X[:2000, :, :].detach().cpu().numpy(), axis=-1).flatten() # flatten the arrays so accuracy score works\n",
    "                targets = y[:2000].detach().cpu().numpy().flatten()\n",
    "                t_perplexity = average_perplexity(model, train_X[:2000], train_y[:2000])\n",
    "                v_perplexity = average_perplexity(model, valid_X, valid_y)\n",
    "                end_ts = time()\n",
    "                print(\"Epoch {}, Minibatch loss: {:.2f}, Subsample Accuracy: {:.2f}, Train Perplexity: {:.2f}, Validation Perplexity: {:.2f}, Epoch Time: {:.2f} seconds\".format(epoch, loss.item(),\n",
    "                    accuracy_score(targets, preds), t_perplexity, v_perplexity, end_ts - start_ts))\n",
    "    \n",
    "    del X\n",
    "    del y\n",
    "    torch.cuda.empty_cache()\n",
    "    if print_results:\n",
    "        print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d51665cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch loss: 5.84, Subsample Accuracy: 0.19, Train Perplexity: 5476.68, Validation Perplexity: 5392.49\n",
      "Epoch 1, Minibatch loss: 5.27, Subsample Accuracy: 0.19, Train Perplexity: 4591.04, Validation Perplexity: 4502.46\n",
      "Epoch 2, Minibatch loss: 5.18, Subsample Accuracy: 0.18, Train Perplexity: 3781.74, Validation Perplexity: 3725.83\n",
      "Epoch 3, Minibatch loss: 5.33, Subsample Accuracy: 0.18, Train Perplexity: 3172.56, Validation Perplexity: 3137.97\n",
      "Epoch 4, Minibatch loss: 5.11, Subsample Accuracy: 0.18, Train Perplexity: 2724.83, Validation Perplexity: 2705.27\n",
      "Epoch 5, Minibatch loss: 5.23, Subsample Accuracy: 0.19, Train Perplexity: 2389.16, Validation Perplexity: 2385.96\n",
      "Epoch 6, Minibatch loss: 5.16, Subsample Accuracy: 0.18, Train Perplexity: 2150.90, Validation Perplexity: 2162.16\n",
      "Epoch 7, Minibatch loss: 4.94, Subsample Accuracy: 0.19, Train Perplexity: 1951.61, Validation Perplexity: 1974.96\n",
      "Epoch 8, Minibatch loss: 4.84, Subsample Accuracy: 0.19, Train Perplexity: 1790.92, Validation Perplexity: 1823.86\n",
      "Epoch 9, Minibatch loss: 4.89, Subsample Accuracy: 0.18, Train Perplexity: 1666.38, Validation Perplexity: 1704.66\n",
      "Epoch 10, Minibatch loss: 4.88, Subsample Accuracy: 0.18, Train Perplexity: 1555.94, Validation Perplexity: 1599.37\n",
      "Epoch 11, Minibatch loss: 4.55, Subsample Accuracy: 0.19, Train Perplexity: 1456.27, Validation Perplexity: 1504.97\n",
      "Epoch 12, Minibatch loss: 4.69, Subsample Accuracy: 0.18, Train Perplexity: 1364.49, Validation Perplexity: 1418.45\n",
      "Epoch 13, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 1257.05, Validation Perplexity: 1313.14\n",
      "Epoch 14, Minibatch loss: 4.76, Subsample Accuracy: 0.19, Train Perplexity: 1187.09, Validation Perplexity: 1243.19\n",
      "Epoch 15, Minibatch loss: 4.81, Subsample Accuracy: 0.18, Train Perplexity: 1119.37, Validation Perplexity: 1178.98\n",
      "Epoch 16, Minibatch loss: 4.90, Subsample Accuracy: 0.18, Train Perplexity: 1057.85, Validation Perplexity: 1119.06\n",
      "Epoch 17, Minibatch loss: 4.98, Subsample Accuracy: 0.18, Train Perplexity: 1001.57, Validation Perplexity: 1064.10\n",
      "Epoch 18, Minibatch loss: 4.79, Subsample Accuracy: 0.18, Train Perplexity: 935.20, Validation Perplexity: 999.06\n",
      "Epoch 19, Minibatch loss: 4.74, Subsample Accuracy: 0.18, Train Perplexity: 876.00, Validation Perplexity: 938.18\n",
      "Epoch 20, Minibatch loss: 4.58, Subsample Accuracy: 0.18, Train Perplexity: 826.86, Validation Perplexity: 890.26\n",
      "Epoch 21, Minibatch loss: 4.57, Subsample Accuracy: 0.18, Train Perplexity: 779.72, Validation Perplexity: 844.10\n",
      "Epoch 22, Minibatch loss: 4.71, Subsample Accuracy: 0.18, Train Perplexity: 748.63, Validation Perplexity: 813.42\n",
      "Epoch 23, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 710.61, Validation Perplexity: 775.64\n",
      "Epoch 24, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 675.82, Validation Perplexity: 741.75\n",
      "Epoch 25, Minibatch loss: 4.67, Subsample Accuracy: 0.19, Train Perplexity: 640.35, Validation Perplexity: 707.65\n",
      "Epoch 26, Minibatch loss: 4.82, Subsample Accuracy: 0.18, Train Perplexity: 609.33, Validation Perplexity: 677.11\n",
      "Epoch 27, Minibatch loss: 4.22, Subsample Accuracy: 0.18, Train Perplexity: 584.62, Validation Perplexity: 650.31\n",
      "Epoch 28, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 555.64, Validation Perplexity: 621.89\n",
      "Epoch 29, Minibatch loss: 4.80, Subsample Accuracy: 0.18, Train Perplexity: 530.58, Validation Perplexity: 596.41\n",
      "Epoch 30, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 510.51, Validation Perplexity: 576.40\n",
      "Epoch 31, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 488.33, Validation Perplexity: 553.29\n",
      "Epoch 32, Minibatch loss: 4.54, Subsample Accuracy: 0.18, Train Perplexity: 464.88, Validation Perplexity: 529.08\n",
      "Epoch 33, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 447.38, Validation Perplexity: 510.87\n",
      "Epoch 34, Minibatch loss: 4.64, Subsample Accuracy: 0.19, Train Perplexity: 432.98, Validation Perplexity: 497.11\n",
      "Epoch 35, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 417.87, Validation Perplexity: 482.30\n",
      "Epoch 36, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 404.87, Validation Perplexity: 468.33\n",
      "Epoch 37, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 393.28, Validation Perplexity: 456.93\n",
      "Epoch 38, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 379.51, Validation Perplexity: 443.89\n",
      "Epoch 39, Minibatch loss: 4.75, Subsample Accuracy: 0.19, Train Perplexity: 370.21, Validation Perplexity: 435.40\n",
      "Epoch 40, Minibatch loss: 4.49, Subsample Accuracy: 0.19, Train Perplexity: 355.71, Validation Perplexity: 420.45\n",
      "Epoch 41, Minibatch loss: 4.42, Subsample Accuracy: 0.19, Train Perplexity: 348.89, Validation Perplexity: 412.52\n",
      "Epoch 42, Minibatch loss: 4.47, Subsample Accuracy: 0.19, Train Perplexity: 340.33, Validation Perplexity: 404.41\n",
      "Epoch 43, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 329.51, Validation Perplexity: 393.30\n",
      "Epoch 44, Minibatch loss: 4.38, Subsample Accuracy: 0.19, Train Perplexity: 322.76, Validation Perplexity: 387.37\n",
      "Epoch 45, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 314.91, Validation Perplexity: 378.22\n",
      "Epoch 46, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 308.03, Validation Perplexity: 371.69\n",
      "Epoch 47, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 301.45, Validation Perplexity: 364.70\n",
      "Epoch 48, Minibatch loss: 4.52, Subsample Accuracy: 0.18, Train Perplexity: 297.91, Validation Perplexity: 361.49\n",
      "Epoch 49, Minibatch loss: 4.44, Subsample Accuracy: 0.19, Train Perplexity: 293.07, Validation Perplexity: 355.72\n",
      "Epoch 50, Minibatch loss: 4.39, Subsample Accuracy: 0.17, Train Perplexity: 289.04, Validation Perplexity: 352.12\n",
      "Epoch 51, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 284.01, Validation Perplexity: 347.42\n",
      "Epoch 52, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 280.74, Validation Perplexity: 344.42\n",
      "Epoch 53, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 275.46, Validation Perplexity: 339.17\n",
      "Epoch 54, Minibatch loss: 4.37, Subsample Accuracy: 0.17, Train Perplexity: 272.82, Validation Perplexity: 335.91\n",
      "Epoch 55, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 269.64, Validation Perplexity: 333.87\n",
      "Epoch 56, Minibatch loss: 4.68, Subsample Accuracy: 0.18, Train Perplexity: 264.01, Validation Perplexity: 328.75\n",
      "Epoch 57, Minibatch loss: 4.41, Subsample Accuracy: 0.17, Train Perplexity: 263.00, Validation Perplexity: 328.00\n",
      "Epoch 58, Minibatch loss: 4.56, Subsample Accuracy: 0.18, Train Perplexity: 262.02, Validation Perplexity: 327.43\n",
      "Epoch 59, Minibatch loss: 4.21, Subsample Accuracy: 0.19, Train Perplexity: 260.36, Validation Perplexity: 323.63\n",
      "Epoch 60, Minibatch loss: 4.35, Subsample Accuracy: 0.19, Train Perplexity: 259.77, Validation Perplexity: 324.26\n",
      "Epoch 61, Minibatch loss: 4.29, Subsample Accuracy: 0.17, Train Perplexity: 259.03, Validation Perplexity: 323.57\n",
      "Epoch 62, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 257.25, Validation Perplexity: 321.99\n",
      "Epoch 63, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 256.43, Validation Perplexity: 321.51\n",
      "Epoch 64, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 255.57, Validation Perplexity: 321.01\n",
      "Epoch 65, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 254.66, Validation Perplexity: 320.84\n",
      "Epoch 66, Minibatch loss: 3.88, Subsample Accuracy: 0.19, Train Perplexity: 253.35, Validation Perplexity: 319.79\n",
      "Epoch 67, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 252.66, Validation Perplexity: 319.94\n",
      "Epoch 68, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 251.55, Validation Perplexity: 319.23\n",
      "Epoch 69, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 249.63, Validation Perplexity: 317.39\n",
      "Epoch 70, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 248.86, Validation Perplexity: 315.35\n",
      "Epoch 71, Minibatch loss: 4.00, Subsample Accuracy: 0.19, Train Perplexity: 248.79, Validation Perplexity: 317.27\n",
      "Epoch 72, Minibatch loss: 3.91, Subsample Accuracy: 0.19, Train Perplexity: 246.19, Validation Perplexity: 314.84\n",
      "Epoch 73, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 247.41, Validation Perplexity: 315.28\n",
      "Epoch 74, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 247.14, Validation Perplexity: 315.10\n",
      "Epoch 75, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 245.09, Validation Perplexity: 313.25\n",
      "Epoch 76, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 246.37, Validation Perplexity: 314.49\n",
      "Epoch 77, Minibatch loss: 3.89, Subsample Accuracy: 0.19, Train Perplexity: 246.49, Validation Perplexity: 315.38\n",
      "Epoch 78, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 246.09, Validation Perplexity: 315.03\n",
      "Epoch 79, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 245.91, Validation Perplexity: 315.10\n",
      "Epoch 80, Minibatch loss: 4.10, Subsample Accuracy: 0.18, Train Perplexity: 247.93, Validation Perplexity: 317.32\n",
      "Epoch 81, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 244.34, Validation Perplexity: 313.44\n",
      "Epoch 82, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 245.80, Validation Perplexity: 314.77\n",
      "Epoch 83, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 244.70, Validation Perplexity: 313.63\n",
      "Epoch 84, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 243.42, Validation Perplexity: 312.25\n",
      "Epoch 85, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 244.68, Validation Perplexity: 313.40\n",
      "Epoch 86, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 242.68, Validation Perplexity: 311.52\n",
      "Epoch 87, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 243.07, Validation Perplexity: 312.91\n",
      "Epoch 88, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 242.19, Validation Perplexity: 312.40\n",
      "Epoch 89, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 242.87, Validation Perplexity: 312.90\n",
      "Epoch 90, Minibatch loss: 4.42, Subsample Accuracy: 0.18, Train Perplexity: 240.35, Validation Perplexity: 309.62\n",
      "Epoch 91, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 240.76, Validation Perplexity: 309.31\n",
      "Epoch 92, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 241.14, Validation Perplexity: 311.39\n",
      "Epoch 93, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 241.42, Validation Perplexity: 310.48\n",
      "Epoch 94, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 239.01, Validation Perplexity: 309.35\n",
      "Epoch 95, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.80, Validation Perplexity: 308.66\n",
      "Epoch 96, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 238.24, Validation Perplexity: 308.37\n",
      "Epoch 97, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 238.68, Validation Perplexity: 308.45\n",
      "Epoch 98, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.69, Validation Perplexity: 308.85\n",
      "Epoch 99, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 237.43, Validation Perplexity: 307.66\n",
      "Epoch 100, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 237.74, Validation Perplexity: 308.04\n",
      "Epoch 101, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 238.30, Validation Perplexity: 308.48\n",
      "Epoch 102, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 236.60, Validation Perplexity: 306.11\n",
      "Epoch 103, Minibatch loss: 4.27, Subsample Accuracy: 0.17, Train Perplexity: 235.80, Validation Perplexity: 305.84\n",
      "Epoch 104, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 235.29, Validation Perplexity: 306.47\n",
      "Epoch 105, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 237.75, Validation Perplexity: 308.24\n",
      "Epoch 106, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 237.73, Validation Perplexity: 308.38\n",
      "Epoch 107, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 237.20, Validation Perplexity: 308.49\n",
      "Epoch 108, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 236.51, Validation Perplexity: 307.32\n",
      "Epoch 109, Minibatch loss: 4.12, Subsample Accuracy: 0.18, Train Perplexity: 235.98, Validation Perplexity: 307.06\n",
      "Epoch 110, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 236.37, Validation Perplexity: 306.64\n",
      "Epoch 111, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 234.71, Validation Perplexity: 304.71\n",
      "Epoch 112, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 233.82, Validation Perplexity: 304.40\n",
      "Epoch 113, Minibatch loss: 4.27, Subsample Accuracy: 0.18, Train Perplexity: 236.07, Validation Perplexity: 306.60\n",
      "Epoch 114, Minibatch loss: 4.40, Subsample Accuracy: 0.18, Train Perplexity: 234.30, Validation Perplexity: 306.02\n",
      "Epoch 115, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 234.13, Validation Perplexity: 304.99\n",
      "Epoch 116, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 234.27, Validation Perplexity: 304.26\n",
      "Epoch 117, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 234.63, Validation Perplexity: 305.40\n",
      "Epoch 118, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 234.81, Validation Perplexity: 305.67\n",
      "Epoch 119, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 235.66, Validation Perplexity: 306.63\n",
      "Epoch 120, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 235.57, Validation Perplexity: 305.68\n",
      "Epoch 121, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 232.76, Validation Perplexity: 303.14\n",
      "Epoch 122, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 232.50, Validation Perplexity: 303.04\n",
      "Epoch 123, Minibatch loss: 4.23, Subsample Accuracy: 0.20, Train Perplexity: 231.39, Validation Perplexity: 301.93\n",
      "Epoch 124, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 232.08, Validation Perplexity: 302.36\n",
      "Epoch 125, Minibatch loss: 4.28, Subsample Accuracy: 0.20, Train Perplexity: 231.29, Validation Perplexity: 300.98\n",
      "Epoch 126, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 229.45, Validation Perplexity: 300.27\n",
      "Epoch 127, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 228.29, Validation Perplexity: 298.52\n",
      "Epoch 128, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 228.74, Validation Perplexity: 299.92\n",
      "Epoch 129, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 228.19, Validation Perplexity: 298.40\n",
      "Epoch 130, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 228.62, Validation Perplexity: 298.51\n",
      "Epoch 131, Minibatch loss: 4.32, Subsample Accuracy: 0.17, Train Perplexity: 229.89, Validation Perplexity: 299.81\n",
      "Epoch 132, Minibatch loss: 4.40, Subsample Accuracy: 0.19, Train Perplexity: 229.14, Validation Perplexity: 300.06\n",
      "Epoch 133, Minibatch loss: 4.35, Subsample Accuracy: 0.17, Train Perplexity: 229.39, Validation Perplexity: 299.79\n",
      "Epoch 134, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 231.70, Validation Perplexity: 301.74\n",
      "Epoch 135, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 230.36, Validation Perplexity: 301.70\n",
      "Epoch 136, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.40, Validation Perplexity: 300.80\n",
      "Epoch 137, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.98, Validation Perplexity: 300.24\n",
      "Epoch 138, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 229.88, Validation Perplexity: 301.13\n",
      "Epoch 139, Minibatch loss: 4.46, Subsample Accuracy: 0.19, Train Perplexity: 228.50, Validation Perplexity: 299.10\n",
      "Epoch 140, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.70, Validation Perplexity: 297.74\n",
      "Epoch 141, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 228.97, Validation Perplexity: 300.98\n",
      "Epoch 142, Minibatch loss: 4.31, Subsample Accuracy: 0.18, Train Perplexity: 228.31, Validation Perplexity: 298.41\n",
      "Epoch 143, Minibatch loss: 4.09, Subsample Accuracy: 0.18, Train Perplexity: 228.53, Validation Perplexity: 299.20\n",
      "Epoch 144, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 226.93, Validation Perplexity: 298.40\n",
      "Epoch 145, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 226.72, Validation Perplexity: 297.92\n",
      "Epoch 146, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 225.46, Validation Perplexity: 295.54\n",
      "Epoch 147, Minibatch loss: 4.07, Subsample Accuracy: 0.19, Train Perplexity: 228.22, Validation Perplexity: 299.47\n",
      "Epoch 148, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 225.41, Validation Perplexity: 296.80\n",
      "Epoch 149, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 226.84, Validation Perplexity: 297.85\n",
      "Epoch 150, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 227.05, Validation Perplexity: 299.09\n",
      "Epoch 151, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 226.02, Validation Perplexity: 298.17\n",
      "Epoch 152, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 226.67, Validation Perplexity: 298.10\n",
      "Epoch 153, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 227.75, Validation Perplexity: 299.15\n",
      "Epoch 154, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 227.36, Validation Perplexity: 298.73\n",
      "Epoch 155, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 225.13, Validation Perplexity: 296.02\n",
      "Epoch 156, Minibatch loss: 4.23, Subsample Accuracy: 0.17, Train Perplexity: 226.37, Validation Perplexity: 297.95\n",
      "Epoch 157, Minibatch loss: 4.44, Subsample Accuracy: 0.17, Train Perplexity: 225.94, Validation Perplexity: 296.60\n",
      "Epoch 158, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 225.86, Validation Perplexity: 296.96\n",
      "Epoch 159, Minibatch loss: 4.37, Subsample Accuracy: 0.18, Train Perplexity: 224.68, Validation Perplexity: 295.32\n",
      "Epoch 160, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 224.34, Validation Perplexity: 295.34\n",
      "Epoch 161, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 222.58, Validation Perplexity: 293.00\n",
      "Epoch 162, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 222.39, Validation Perplexity: 292.02\n",
      "Epoch 163, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 223.47, Validation Perplexity: 293.71\n",
      "Epoch 164, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 225.33, Validation Perplexity: 295.93\n",
      "Epoch 165, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.07, Validation Perplexity: 293.74\n",
      "Epoch 166, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 223.23, Validation Perplexity: 293.61\n",
      "Epoch 167, Minibatch loss: 4.25, Subsample Accuracy: 0.18, Train Perplexity: 225.39, Validation Perplexity: 296.71\n",
      "Epoch 168, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 224.81, Validation Perplexity: 296.14\n",
      "Epoch 169, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.08, Validation Perplexity: 296.97\n",
      "Epoch 170, Minibatch loss: 4.24, Subsample Accuracy: 0.17, Train Perplexity: 224.61, Validation Perplexity: 295.71\n",
      "Epoch 171, Minibatch loss: 4.10, Subsample Accuracy: 0.19, Train Perplexity: 224.76, Validation Perplexity: 296.56\n",
      "Epoch 172, Minibatch loss: 4.15, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 295.98\n",
      "Epoch 173, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 224.21, Validation Perplexity: 296.21\n",
      "Epoch 174, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.11, Validation Perplexity: 296.44\n",
      "Epoch 175, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 223.87, Validation Perplexity: 295.96\n",
      "Epoch 176, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 296.70\n",
      "Epoch 177, Minibatch loss: 4.45, Subsample Accuracy: 0.19, Train Perplexity: 222.85, Validation Perplexity: 294.78\n",
      "Epoch 178, Minibatch loss: 4.18, Subsample Accuracy: 0.18, Train Perplexity: 223.25, Validation Perplexity: 294.85\n",
      "Epoch 179, Minibatch loss: 4.50, Subsample Accuracy: 0.19, Train Perplexity: 222.41, Validation Perplexity: 293.88\n",
      "Epoch 180, Minibatch loss: 3.95, Subsample Accuracy: 0.19, Train Perplexity: 222.83, Validation Perplexity: 294.24\n",
      "Epoch 181, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 223.08, Validation Perplexity: 294.20\n",
      "Epoch 182, Minibatch loss: 4.13, Subsample Accuracy: 0.18, Train Perplexity: 223.52, Validation Perplexity: 295.34\n",
      "Epoch 183, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 220.96, Validation Perplexity: 292.61\n",
      "Epoch 184, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 223.21, Validation Perplexity: 294.52\n",
      "Epoch 185, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 222.59, Validation Perplexity: 293.70\n",
      "Epoch 186, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 223.89, Validation Perplexity: 295.60\n",
      "Epoch 187, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 224.00, Validation Perplexity: 295.06\n",
      "Epoch 188, Minibatch loss: 4.25, Subsample Accuracy: 0.19, Train Perplexity: 222.02, Validation Perplexity: 294.40\n",
      "Epoch 189, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 221.55, Validation Perplexity: 293.86\n",
      "Epoch 190, Minibatch loss: 4.52, Subsample Accuracy: 0.19, Train Perplexity: 222.05, Validation Perplexity: 292.84\n",
      "Epoch 191, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 223.65, Validation Perplexity: 294.15\n",
      "Epoch 192, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 221.89, Validation Perplexity: 292.28\n",
      "Epoch 193, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 221.92, Validation Perplexity: 293.61\n",
      "Epoch 194, Minibatch loss: 4.32, Subsample Accuracy: 0.18, Train Perplexity: 221.51, Validation Perplexity: 292.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-619d9a32a5fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-43f7dfafae68>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_X, train_y, epochs, learn_rate, weight_decay, minibatch_size, print_results)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_X, train_y, epochs=200, learn_rate=0.001, minibatch_size=256, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a5e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTMBaseline.model\") # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fff3960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model, only run if the model hasn't already been trained\n",
    "model.load_state_dict(torch.load(\"LSTMBaseline.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab2088d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2e2dd",
   "metadata": {},
   "source": [
    "### Hyper-Parameter tuning findings\n",
    "- glove 300 dimension vectors are essential to not have a bias of 1000 perplexity on both train and validation\n",
    "- 2 layers of LSTM also gives high bias, perhaps there is not enough train data\n",
    "- Weight decay is essential in preventing Validation perplexity from skyrocketing\n",
    "- Dropout of 0.1 combined with weight decay 0.00001 works (around 250 validation perplexity)\n",
    "- Decreasing learning rate and increasing epochs has a minor benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db063f69",
   "metadata": {},
   "source": [
    "### Examine Performance of the model\n",
    "- Using both perplexity and qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "228f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view overall performance\n",
    "model.eval()\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "preds = model.predict(data)\n",
    "#preds = torch.nn.Softmax(dim=-1)(preds).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ad8a2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probably',\n",
       " 'help',\n",
       " 'begin',\n",
       " 'also',\n",
       " 'take',\n",
       " 'make',\n",
       " 'continue',\n",
       " 'the',\n",
       " 'have',\n",
       " 'be']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: index_vocab[x], np.argsort(preds[1, 11, :])[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f38177fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1733707e-07, 5.4392586e-07, 5.5573935e-07, ..., 1.3860598e-02,\n",
       "       3.9391726e-02, 7.3702824e-01], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(preds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d20a349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['<PAD>', '<PAD>', 'securities', \"'s\", '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '.', '.', '<PAD>', '<PAD>', '<PAD>', 'the', '<PAD>', '<PAD>', '<PAD>', \"'s\", '<PAD>']\n",
      "Input:     ['<PAD>', 'aer', 'banknote', 'berlitz', 'calloway', 'cluett', 'fromstein', 'gitano', 'guterman', 'ipo', 'kia', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'ssangyong', 'swapo', 'wachter']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'the', '.', '.', 'be', 'the', 'company', \"'s\", 'a', 'new', 'director', 'of', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'pierre', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'said', 'a', 'of', 'the', 'inc.', 'company', \"'s\", 'group', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'mr.', 'is', 'chairman', 'of', 'n.v.', 'the', 'dutch', 'publishing', 'group', '.']\n",
      "\n",
      "Predicted: ['<PAD>', \"'s\", 'ago', 'was', 'the', 'president', 'of', 'the', 'inc.', 'and', 'in', 'a', 'a', 'a', 'share', 'director', 'of', 'the', 'year', 'government']\n",
      "Input:     ['<PAD>', 'rudolph', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial']\n",
      "\n",
      "Predicted: ['<PAD>', 'new', 'of', 'the', 'and', 'the', 'to', 'the', 'the', 'the', 'and', 'to', 'been', 'by', 'share', 'court', 'of', 'the', 'institute', '.']\n",
      "Input:     ['<PAD>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among']\n",
      "\n",
      "Predicted: ['<PAD>', 'company', 'and', 'and', 'a', 'high', 'the', 'is', 'the', 'company', 'the', 'if', 'and', 'and', 'the', \"'s\", 'the', '.', 'the', '.']\n",
      "Input:     ['<PAD>', 'the', 'asbestos', 'fiber', 'is', 'unusually', 'once', 'it', 'enters', 'the', 'with', 'even', 'brief', 'exposures', 'to', 'it', 'causing', 'symptoms', 'that', 'show', 'up']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', 'said', 'company', 'of', 'the', 'york', 'company', 'said', 'the', 'the', \"'s\", 'and', 'the', 'the', 'the', 'own', 'and', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', 'inc.', 'the', 'unit', 'of', 'new', 'york-based', 'corp.', 'that', 'makes', 'kent', 'cigarettes', 'stopped', 'using', 'in', 'its', 'cigarette', 'filters', 'in', '.']\n",
      "\n",
      "Predicted: ['<PAD>', 'the', 'agreement', 'that', \"n't\", 'a', 'than', 'the', 'new', 'earlier', 'the', 'company', 'quarter', 'of', 'to', 'the', \"'s\", '.', 'york', '.']\n",
      "Input:     ['<PAD>', 'although', 'preliminary', 'findings', 'were', 'reported', 'more', 'than', 'a', 'year', 'ago', 'the', 'latest', 'results', 'appear', 'in', 'today', \"'s\", 'new', 'england', 'journal']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'share', 'the', 'year', 'a', 'ounce', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'a', 'said', 'this', 'is', 'an', 'old', 'story', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', \"'re\", 'going', 'about', 'the', 'ago', 'the', 'the', 'who', 'in', 'the', 'and', 'a', 'of', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_int = np.argmax(preds, axis=-1)\n",
    "for i in range(10):\n",
    "    sentence = list(map(index_vocab.get, preds_int[i]))\n",
    "    truth = list(map(lambda x: index_vocab[int(x)], train_y[i]))\n",
    "    input_sentence = ['<PAD>'] + truth\n",
    "    print(\"Predicted:\", sentence)\n",
    "    print(\"Input:    \",input_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75bd595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n',\n",
       " ' pierre <unk> N years old will join the board as a nonexecutive director nov. N \\n',\n",
       " ' mr. <unk> is chairman of <unk> n.v. the dutch publishing group \\n',\n",
       " ' rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \\n',\n",
       " ' a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \\n',\n",
       " ' the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \\n',\n",
       " ' <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \\n',\n",
       " \" although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\",\n",
       " ' a <unk> <unk> said this is an old story \\n',\n",
       " \" we 're talking about years ago before anyone heard of asbestos having any questionable properties \\n\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71ab8782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.610664"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model.predict(numpy_to_tensor(valid_X[0:1]))\n",
    "perplexity(pred1, valid_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92debfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.79262"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b8efd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.91684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4b94e-9fb6-4b4d-8a66-1b87cd9f41ec",
   "metadata": {},
   "source": [
    "# GPT2 Baseline\n",
    "Implement GPT2 as a language modelling baseline. GPT-3 is not publicly available and too large for practical purposes. BERT needs modification to work for language modelling, due to the fact that it is trained for bidirectional masked language modelling instead.\n",
    "\n",
    "This section makes use of several tutorials for fine tuning, including:\n",
    "- https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/\n",
    "- https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
    "- https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel (the documentation)\n",
    "- https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a61aafc-e915-4ff8-b371-3fc32331fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadGPT, BERT and support materials from huggingface\n",
    "# requires pip install transformers\n",
    "# if in jupyter notebook see here and you get an error mention ipython widgets see here: \n",
    "# https://stackoverflow.com/questions/53247985/tqdm-4-28-1-in-jupyter-notebook-intprogress-not-found-please-update-jupyter-an\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d485f60-6997-409f-80f2-f4a7a6a6987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42140794-4ad1-4999-b6ab-1bd13ba648d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models\n",
    "# Documentation for GPT: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12b27c-c788-40c8-9265-e724d5fd46ab",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa755a4d-7ad0-4e3c-b00c-669513e9c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face is based in DUMBO, New York City, and focuses on two characters who are either dead or are alive, the dead being an infant in its stomach'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2 example generation\n",
    "text = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "tokens_tensor = gpt_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Put everything on cuda\n",
    "gpt_model.eval()\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "gpt_model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "generated = tokens_tensor\n",
    "for i in range(20):\n",
    "    next_token_logits = gpt_model(generated).logits[:, -1, :]\n",
    "    # filter\n",
    "    filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "    # sample\n",
    "    probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "resulting_string = gpt_tokenizer.decode(generated.tolist()[0])\n",
    "resulting_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6795d64b-f959-450d-b9d2-2dce113b96a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' rud', 'olph', ' <', 'unk', '>', ' N', ' years', ' old', ' and', ' former', ' chairman', ' of', ' consolidated', ' gold', ' fields', ' pl', 'c', ' was', ' named', ' a', ' nonex', 'ec', 'utive', ' director', ' of', ' this', ' b', 'rit', 'ish', ' industrial', ' conglomerate', ' ', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens = gpt_tokenizer.encode(train[3])\n",
    "print([gpt_tokenizer.decode([x]) for x in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a79ac-e65d-4082-bc08-22cfeee91848",
   "metadata": {},
   "source": [
    "We can see that the gpt_tokenizer works differently to ours, splitting up names such as 'rudolph' into 'rud' and 'olph' and words such as nonexecutive and british. Hence our perplexity evaluation will have to be slightly different, using gpt_tokenizer to get the ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2463-2077-4fb4-867f-f2b928af469d",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ad00057-0e44-4206-b291-971fed2150a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for fine-tuning, it's a generator so we don't have to store the entire dataset in memory\n",
    "class GPT2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=40):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        # Encode all the text, padding and truncuating it along with adding attention masks to get the sequence length the same across all samples\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer.encode_plus('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(encodings_dict['input_ids'])\n",
    "            self.attn_masks.append(encodings_dict['attention_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The tutorial use a dictionary format that also stores labels \n",
    "        return_dict = {\"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "                       \"attention_mask\": torch.tensor(self.attn_masks[idx]), \n",
    "                       \"labels\": torch.tensor(self.input_ids[idx])} \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "943bca9e-d14d-4f87-9b50-d62e6c5efe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
       "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
       "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
       "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
       "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
       "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
       "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token # set the pad token\n",
    "gpt_sequence_length = 40 # gpt splits up words into smaller tokens, so the sequence length should be longer\n",
    "train_dataset = GPT2Dataset(train, gpt_tokenizer, max_length=gpt_sequence_length)\n",
    "val_dataset = GPT2Dataset(valid, gpt_tokenizer, max_length=gpt_sequence_length)\n",
    "test_dataset = GPT2Dataset(test, gpt_tokenizer, max_length=gpt_sequence_length)\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b4c8184-ad9d-473e-861f-51e7f265e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[-31.7761, -30.6999, -32.1561,  ..., -39.5131, -39.7474, -31.6402],\n",
       "        [-63.3376, -60.9081, -61.4334,  ..., -72.3227, -72.0701, -62.4869],\n",
       "        [-53.8055, -53.4089, -53.4207,  ..., -63.6040, -62.0605, -54.8142],\n",
       "        ...,\n",
       "        [-82.7951, -76.3731, -78.5615,  ..., -95.3905, -96.0068, -84.1115],\n",
       "        [-82.7967, -76.3781, -78.5659,  ..., -95.3878, -96.0029, -84.1119],\n",
       "        [-82.8332, -76.4163, -78.6067,  ..., -95.4225, -96.0345, -84.1454]],\n",
       "       device='cuda:0', grad_fn=<MmBackward>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get output by passing the ids and the attention mask\n",
    "gpt_model(input_ids=train_dataset[1]['input_ids'].to(device), attention_mask=train_dataset[1]['attention_mask'].to(device), use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a4e19c1-51c5-41ca-b369-297c7750d752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|> the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.decode(train_dataset[5]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1ea2b-d89b-4638-bcb2-cf63e7d71211",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "Do fine tuning of the gpt_model using the hugging face out of the box trainer https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainerfrom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51091355-c64c-4efb-ac21-68232c8e9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2630/2630 04:14, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.111200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2630, training_loss=2.287749967103675, metrics={'train_runtime': 254.6267, 'train_samples_per_second': 10.329, 'total_flos': 61761965506560.0, 'epoch': 1.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='gpt_finetuning',     # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs (1 is enough to get very low perplexity and perplexity increases at 2)\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    logging_dir='gpt_finetuning_logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt_model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e5ae3-672d-4c81-9695-8e01ad6adad3",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "Define a wrapper model that can use GPT2 both for standard next word prediction and language generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3923d864-8141-4525-a915-930e03c08ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model wrapper for gpt2 that uses the \"past\" variable and for language modelling\n",
    "# TODO: add the options for beam search\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, model=None, sequence_length=20):\n",
    "        super().__init__()\n",
    "        self.gpt = model.to(device)\n",
    "        self.tokenizer = gpt_tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    \n",
    "    # output the logits for the most likely next word at each position in the sentence and optionally the hidden states (used for the Neural ODE) \n",
    "    # note input_dataset must be an element taken from a GPT2Dataset class (e.g. train_dataset[0])\n",
    "    def forward(self, input_dataset, output_hidden_states=False):\n",
    "        output = self.gpt.forward(input_ids = input_dataset['input_ids'].to(device), \n",
    "                                  attention_mask=input_dataset['attention_mask'].to(device),\n",
    "                                  use_cache=False,\n",
    "                                  output_hidden_states = output_hidden_states)\n",
    "        if output_hidden_states:\n",
    "            return output[\"hidden_states\"]\n",
    "        return output[\"logits\"]\n",
    "    \n",
    "    # take in a sentence and output the predictions as in forward, but as the most likely sentence not logits\n",
    "    def forward_sentence(self, input_dataset):\n",
    "        preds = self.forward(input_dataset)\n",
    "        tokens = torch.argmax(preds, dim=-1)\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "    # generate a sentence by sampling the next word from the probability distribution\n",
    "    # set limit to an integer to generate `limit` number of words instead of ending at a full stop\n",
    "    def random_gen(self, x, limit=None):\n",
    "        # initialize variables\n",
    "        generated = self.tokenizer.encode_plus(x, return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "        next_token = [generated[0][-1]]\n",
    "        past = None\n",
    "        raw_output= None\n",
    "        \n",
    "        # generate until a \".\" is generated\n",
    "        while (limit is None and self.tokenizer.decode(next_token[0]) not in [\".\", \"?\", \"!\"]) or (limit is not None and len(generated[0]) < limit):\n",
    "            # get output of model, using past if available\n",
    "            if past is None:\n",
    "                raw_output = self.gpt(generated, past_key_values=past)\n",
    "            else:\n",
    "                raw_output = self.gpt(next_token, past_key_values=past)\n",
    "            output, past = raw_output['logits'], raw_output['past_key_values']\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # sample a token from the top 50 most likely words\n",
    "            filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0) # filter to the top 50 tokens\n",
    "            probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "        return self.tokenizer.decode(generated[0])\n",
    "    \n",
    "    # do beam_search to find the most likely sentence\n",
    "    def beam_search(self, x, beam=5): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a88b3953-f775-4979-940e-7733c5240072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6970e+02, -1.6412e+02, -1.6493e+02,  ..., -2.0270e+02,\n",
       "         -2.0474e+02, -1.6856e+02],\n",
       "        [-2.6901e+00, -1.6066e+00,  4.1629e-01,  ..., -9.9957e+00,\n",
       "         -1.0583e+01,  7.4749e-02],\n",
       "        [-2.3482e+01, -2.3870e+01, -2.1071e+01,  ..., -4.0187e+01,\n",
       "         -3.3782e+01, -2.0225e+01],\n",
       "        ...,\n",
       "        [-7.0412e+01, -7.0517e+01, -7.2410e+01,  ..., -7.6573e+01,\n",
       "         -7.6298e+01, -6.6031e+01],\n",
       "        [-6.7842e+01, -6.6881e+01, -6.5982e+01,  ..., -7.9229e+01,\n",
       "         -7.9465e+01, -6.2473e+01],\n",
       "        [-6.4527e+01, -6.3917e+01, -6.5069e+01,  ..., -7.2385e+01,\n",
       "         -7.2153e+01, -6.0531e+01]], device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelgpt = GPTModel(model=gpt_model, sequence_length=gpt_sequence_length)\n",
    "modelgpt.eval()\n",
    "modelgpt.forward(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f4366-5d71-4ad9-85b7-d9b6b5dd9964",
   "metadata": {},
   "source": [
    "View the next word output for a single example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66430c0-a4d8-4549-9861-7684003c3a78",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb0a5e37-8004-4bc0-a198-2a42741296bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The format is:\n",
      "(Ground Truth word, Predicted next word),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<', '\\n'),\n",
       " ('|', ' |'),\n",
       " ('start', '|'),\n",
       " ('of', '|'),\n",
       " ('text', '|'),\n",
       " ('|', '>'),\n",
       " ('>', '\\n'),\n",
       " (' the', ' first'),\n",
       " (' total', ' number'),\n",
       " (' of', ' the'),\n",
       " (' N', '.'),\n",
       " (' deaths', ' in'),\n",
       " (' from', ' all'),\n",
       " (' mal', 'ignant'),\n",
       " ('ignant', ' cancer'),\n",
       " (' <', '|'),\n",
       " ('unk', '>'),\n",
       " ('>', ' cancer'),\n",
       " (' lung', ' cancer'),\n",
       " (' cancer', '</'),\n",
       " (' and', ' <'),\n",
       " (' <', 'unk'),\n",
       " ('unk', '>'),\n",
       " ('>', ' lung'),\n",
       " (' was', ' <'),\n",
       " (' far', ' higher'),\n",
       " (' higher', ' than'),\n",
       " (' than', ' the'),\n",
       " (' expected', '.'),\n",
       " (' the', ' previous'),\n",
       " (' researchers', ' said'),\n",
       " (' said', '.'),\n",
       " (' ', '\\xa0'),\n",
       " ('\\n', '\\n'),\n",
       " ('<|endoftext|>', 'The'),\n",
       " ('<|endoftext|>', 'The'),\n",
       " ('<|endoftext|>', 'The'),\n",
       " ('<|endoftext|>', 'The'),\n",
       " ('<|endoftext|>', 'The'),\n",
       " ('<|endoftext|>', 'The')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 18\n",
    "input_t = [gpt_tokenizer.decode(t) for t in train_dataset[i]['input_ids']]\n",
    "preds = modelgpt.forward(train_dataset[i])\n",
    "tokens = torch.argmax(preds, dim=-1)\n",
    "output_t = [gpt_tokenizer.decode(t) for t in tokens]\n",
    "print(\"The format is:\")\n",
    "print(\"(Ground Truth word, Predicted next word),\")\n",
    "list(zip(input_t, output_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b5a30-3012-4bcf-b3a9-768fdbaac8f7",
   "metadata": {},
   "source": [
    "View the ability to generate without teacher forcing using the random_gen() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d5fd0e6-ea4c-428d-900d-87e68583c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A new study says the number of people dying because there is little or no way for people to escape poverty has increased by nearly 500 percent since 2009, the period ending with 2014. And that is according'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = modelgpt.random_gen(\"A new study says\", limit=40)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2996c849-0809-4fbe-8f16-211cdbbb1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The formula for calculating perplexity in language models can be found here: https://web.stanford.edu/~jurafsky/slp3/3.pdf (page 8)\n",
    "# An interesting detail is that the geometric mean of perplexity from each word is used\n",
    "# if the mask is 0 at index i don't use the value at index i to calculate perplexity\n",
    "def perplexity_gpt(preds, ground_truth, mask, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        if mask[i] != 0:\n",
    "            probs.append(preds[i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs).detach().cpu().numpy()\n",
    "\n",
    "# Can optionally define n_samples=int to limit the number of samples used for perplexity evaluation\n",
    "def average_perplexity_gpt(model, train, n_samples=None, print_results=False):\n",
    "    perplexities = []\n",
    "    n_samples = len(train) if n_samples is None else n_samples\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Compute perplexity for a single sample\n",
    "            labels = train[i]['input_ids'][1:]\n",
    "            mask = train[i]['attention_mask'][:-1]\n",
    "            preds = model.forward(train[i])[:-1] # remove the last prediction as there is no ground truth \n",
    "            preds = torch.nn.functional.softmax(preds, dim=-1)\n",
    "            perplexities.append(perplexity_gpt(preds[6:], labels[6:], mask[6:])) # remove the first 7 tokens that represent \"<|startoftext|>\"\n",
    "\n",
    "            if i % 100 == 0 and print_results:\n",
    "                print(\"Sentences analysed: {} Average perplexity: {}\".format(i, np.mean(perplexities)))\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f850a00-c2d8-4efd-827d-e026b698c38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-06c9a13f4666>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0maverage_perplexity_gpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelgpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-a591b907a0bb>\u001b[0m in \u001b[0;36maverage_perplexity_gpt\u001b[1;34m(model, train, n_samples, print_results)\u001b[0m\n\u001b[0;32m     20\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m# remove the last prediction as there is no ground truth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[0mpreds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[0mperplexities\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mperplexity_gpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# remove the first 7 tokens that represent \"<|startoftext|>\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-0c62984e6043>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_dataset, output_hidden_states)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# note input_dataset must be an element taken from a GPT2Dataset class (e.g. train_dataset[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         output = self.gpt.forward(input_ids = input_dataset['input_ids'].to(device), \n\u001b[0m\u001b[0;32m     15\u001b[0m                                   \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'attention_mask'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m                                   \u001b[0muse_cache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers-4.6.0-py3.8.egg\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 941\u001b[1;33m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[0;32m    942\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers-4.6.0-py3.8.egg\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 )\n\u001b[0;32m    788\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 outputs = block(\n\u001b[0m\u001b[0;32m    790\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m                     \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers-4.6.0-py3.8.egg\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m         attn_outputs = self.attn(\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[0mlayer_past\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlayer_past\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers-4.6.0-py3.8.egg\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[0;32m    256\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m         \u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers-4.6.0-py3.8.egg\\transformers\\models\\gpt2\\modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[1;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_attn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m         \u001b[0mattn_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscale_attn_weights\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "average_perplexity_gpt(modelgpt, train_dataset, n_samples=None, print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "236a0ec9-a3bc-4704-865b-4cfba976c022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.49069"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity_gpt(modelgpt, val_dataset, n_samples=None, print_results=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "14803271-64e3-4580-b468-65f8605cb0d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23.95521"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity_gpt(modelgpt, test_dataset, n_samples=None, print_results=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03f37-c70f-4e60-a223-345f58118a46",
   "metadata": {},
   "source": [
    "GPT2 Results\n",
    "- No fine tuning: 706.16187 perplexity on the training set\n",
    "- Fine tuning: 21.57 train, 26.49 validation, 23.95 testing    \n",
    "    - The near SOTA perplexity (higher than the current best for GPT2 https://paperswithcode.com/sota/language-modelling-on-penn-treebank-word ) is due to the fact that the GPT tokenizer splits up the some words into smaller pieces e.g. \"british\" becomes: \\[' b', 'rit', 'ish'\\] and it's easier to predict those smaller pieces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de890b",
   "metadata": {},
   "source": [
    "# Neural ODE Model\n",
    "Results\n",
    "- LSTM as a hidden state: perplexity 651.04. It wasn't fine tuned but it's clear we need better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1c08b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchdyn.models import *\n",
    "from torchdyn import *\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91be6636-c221-4672-8a4f-a64f919f41dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a wrapper for gpt that takes a torch.util.data.TensorDataset as input, needed for pytorch lightning\n",
    "class GPTModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, model=None, sequence_length=40):\n",
    "        super().__init__()\n",
    "        self.gpt = model.to(device)\n",
    "        self.tokenizer = gpt_tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    \n",
    "    # output the hidden states for the entire sequence used for the Neural ODE\n",
    "    def forward(self, input_dataset):\n",
    "        output = self.gpt.forward(input_ids = input_dataset[0].to(device), \n",
    "                                  attention_mask=input_dataset[1].to(device),\n",
    "                                  use_cache=False,\n",
    "                                  output_hidden_states=True)\n",
    "        return output[\"hidden_states\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0e9fbb-8803-4442-a1b5-9ea71865d4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines an ODE that uses a GPT to get a representation for the sentence\n",
    "class ODEGPT(pl.LightningModule):\n",
    "    def __init__(self, modelgpt, sequence_length=40):\n",
    "        super().__init__()\n",
    "        layer_size = 768 # the size of gpt's hidden state\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        # Freeze the GPT model's parameters to save training time\n",
    "        self.modelgpt = modelgpt\n",
    "        for param in self.modelgpt.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Define the derivative function\n",
    "        self.f = torch.nn.Sequential(\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "        )\n",
    "        \n",
    "        # Define the model itself\n",
    "        self.node = NeuralDE(self.f, sensitivity='adjoint', solver='dopri5').to(device)\n",
    "        self.linear = torch.nn.Linear(layer_size, self.modelgpt.vocab_size).to(device)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.timesteps = torch.arange(0, 40, 1, device=device).float() # define the number of output items of the Neural ODE\n",
    "    \n",
    "    # take in a single sample and feed forward, giving the logits as output\n",
    "    # note x must be an element of a GPT2Dataset class so that it can be fed to the GPT model\n",
    "    def forward(self, x):\n",
    "        # at the moment this feeds the entire sequence to LSTM and asks Neural ODE to reproduce it\n",
    "        # TODO: switch to feeding half the sequence and asking NeuralODE to extrapolate\n",
    "        hidden_states = self.modelgpt(x) \n",
    "        attention_mask = x[1].to(device)[0, :] # batching makes x[1] have a shape of (batch_size, features), we use batches of 1 so take the first\n",
    "        \n",
    "        # use the output of GPT2's 12th decoder, \"BERT Rediscovers the Classical NLP Pipeline\" has shown transformers' later layers represent high level meaning, which is \n",
    "        # what we want to input to the Neural ODE\n",
    "        # TODO: Perhaps consider the above paper's method of having a weighted sum of layers representations, with trainable weights\n",
    "        final_hidden = hidden_states[12] \n",
    "        final_hidden = final_hidden[0, attention_mask, :][-1, :] # Take the output of the last sequence item that isn't a pad token\n",
    "\n",
    "        # feed to neural ode\n",
    "        sequence_outputs = self.node.trajectory(final_hidden, self.timesteps) # output is of shape (sequence_length, gpt_hidden_layer_size)\n",
    "        \n",
    "        # Get final output\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # compute the loss on a batch, required by pytorch lightning\n",
    "    # note the batch must be an element of a tf.utils.data.TensorDataset, this function is only meant to be used with pytorch_lightning's training loop\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        labels = batch[2][0, 1:].to(device) # shift the input 1 step ahead to get the next word labels\n",
    "        preds = self.forward(batch)[:-1, :] # remove the prediction for the last token as there is no label\n",
    "        loss = self.loss(preds, labels) # crossentropy loss expects preds to be of size (batch, n_classes) so it handles our sequence model use case\n",
    "        return loss\n",
    "    \n",
    "    # configure the optimizer for pytorch lightning\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.00001, betas=(0.95, 0.999)) # low learning rate and momentum since this is stochastic optimisation\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf6b0d2b-690f-4b3a-903f-ce4edac5fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptmodel_wrapper = GPTModelWrapper(gpt_model)\n",
    "odemodel = ODEGPT(gptmodel_wrapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94d25e-6ab7-4c72-ace3-8791507fd035",
   "metadata": {},
   "source": [
    "Errors: setting num_workers = 1 causes DataLoader to hang. Setting num_workers = 0 causes a random CUDA error that doesn't happen in the above functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99ee5c-9ec8-4405-ba8c-24a6dfa1f8b6",
   "metadata": {},
   "source": [
    "### Training\n",
    "Use the pytorch lightning's training loop to speed up training. \n",
    "\n",
    "Important documentation\n",
    "- https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training\n",
    "- https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bc138d0-fc59-4e87-9a36-0027436aa333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb8d8a53-7a32-4c05-a9a3-1b80285a3b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 50257])\n",
      "tensor(3239.3477, device='cuda:0', grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Load the data into a new dataset, pytorch_lightning doesn't like our custom dataset\n",
    "full_dataset = train_dataset[:]\n",
    "train_tensor_dataset = torch.utils.data.TensorDataset(full_dataset['input_ids'], full_dataset['attention_mask'], full_dataset['labels'])\n",
    "train_dataloader = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=1, shuffle=True,\n",
    "                             num_workers=16, pin_memory=True)\n",
    "\n",
    "# Test run to check for errors\n",
    "for batch in train_dataloader:\n",
    "    print(odemodel.forward(batch).shape)\n",
    "    print(odemodel.training_step(batch, 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd793cab-8f30-4d6e-b922-a099b2ec906c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.distributed:GPU available: True, used: True\n",
      "INFO:pytorch_lightning.utilities.distributed:TPU available: False, using: 0 TPU cores\n",
      "INFO:pytorch_lightning.accelerators.gpu:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "INFO:pytorch_lightning.core.lightning:\n",
      "  | Name     | Type             | Params\n",
      "----------------------------------------------\n",
      "0 | loss     | CrossEntropyLoss | 0     \n",
      "1 | modelgpt | GPTModelWrapper  | 124 M \n",
      "2 | f        | Sequential       | 1.2 M \n",
      "3 | node     | NeuralDE         | 1.2 M \n",
      "4 | linear   | Linear           | 38.6 M\n",
      "5 | softmax  | Softmax          | 0     \n",
      "----------------------------------------------\n",
      "39.8 M    Trainable params\n",
      "124 M     Non-trainable params\n",
      "164 M     Total params\n",
      "657.074   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31e2678887e48e79460d107a0513d67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\liamw\\anaconda3\\envs\\torch\\lib\\site-packages\\pytorch_lightning\\utilities\\distributed.py:69: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer = pl.Trainer(max_epochs=1, gpus=1, progress_bar_refresh_rate=10)\n",
    "trainer.fit(odemodel, train_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b92cc-0228-4f59-9466-2601f457c121",
   "metadata": {},
   "source": [
    "Maybe turn off the dataloader parameters ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b051675-8705-4697-bc37-453b8af430d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11315364-9ca9-49ba-b5b2-409e2cd58052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
