{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a92153-258b-4d12-b136-c1d4ff34b715",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Requires an Nvidia GPU to run\n",
    "\n",
    "Create a new anaconda environment and run the following commands to install the required libraries \n",
    "```\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "conda install gensim\n",
    "pip install torchdyn\n",
    "pip install git+https://github.com/google-research/torchsde.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dfe412-887c-43b0-ae9e-68e68207a975",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993). Building a Large Annotated Corpus of English: The Penn Treebank\n",
    "\n",
    "```\n",
    "@article{poli2020torchdyn,\n",
    "  title={TorchDyn: A Neural Differential Equations Library},\n",
    "  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},\n",
    "  journal={arXiv preprint arXiv:2009.09346},\n",
    "  year={2020}\n",
    "}\n",
    "```\n",
    "\n",
    "- GloVe\n",
    "\n",
    "- GPT2 paper\n",
    "\n",
    "- Huggingface for their implementation of transformers? Not sure if this has a paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea389f-a316-483d-94f9-3e80e54e517d",
   "metadata": {},
   "source": [
    "# To do\n",
    "- Use BERT as a baseline and possibly an encoder\n",
    "- Examine mini batch calculation\n",
    "- Use LSTM function \n",
    "- Consider other variants of Neural ODE\n",
    "- Implement and see results from my continuous language modelling idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d65ff0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from functools import reduce\n",
    "from sklearn.metrics import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469c71b",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "- Build the vocab\n",
    "- Convert text corpus into padded word vector sequences\n",
    "\n",
    "To do\n",
    "- Use LSTM as baseline\n",
    "    - Examine perplexity of model on validation set\n",
    "- Implement Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c916364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e5e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8d4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10001\n"
     ]
    }
   ],
   "source": [
    "# build the vocab\n",
    "corpus = train + valid\n",
    "vocab = {\"<PAD>\": 0}\n",
    "index_vocab = {0 : \"<PAD>\"}\n",
    "for sentence in corpus:\n",
    "    for token in sentence.split(\" \")[1:]:\n",
    "        if token not in vocab:\n",
    "            index = len(vocab)\n",
    "            vocab[token] = index\n",
    "            index_vocab[index] = token\n",
    "\n",
    "# replace penn treebank end sentence token \"\\n\" with glove's end sentence token \".\"\n",
    "index = vocab[\"\\n\"]\n",
    "vocab.pop(\"\\n\")         \n",
    "vocab[\".\"] = index\n",
    "index_vocab[index] = \".\"\n",
    "\n",
    "# view size\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb19c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentences and convert words to their glove vector to get input features\n",
    "# convert to 1 hot vocab and shift 1 to the left to get output labels (converting to 1 hot takes too much memory, so just store indices and convert later)\n",
    "# use left padding, as we want the hidden state at the end (right) to ignore the padding\n",
    "# returns word_vector_dataset, labels\n",
    "def preprocess(dataset, sequence_length, wv):\n",
    "    embedding_size = wv[\"hello\"].shape[0]\n",
    "    processed = np.zeros((len(dataset), sequence_length, embedding_size))\n",
    "    labels = np.zeros((len(dataset), sequence_length, 1))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        tokens = dataset[i].split(\" \")[1:]\n",
    "        \n",
    "        # get the word vectors for all of the tokens, removing out of vocabulary (OOV) tokens\n",
    "        tokens_np = np.zeros((len(tokens), embedding_size))\n",
    "        labels_np = np.zeros((len(tokens), 1))\n",
    "        j = 0\n",
    "        for word in tokens:\n",
    "            if word == \"\\n\": word = \".\" # replace PennTreebank end sentence token '\\n' with glove end sentence token \".\"\n",
    "            if word not in wv: continue # ignore OOV tokens\n",
    "            if j < sequence_length - 1: # only add sequence_length - 1 tokens at max\n",
    "                # so that there is always a 0 vector at the start so the model learns most common starting words\n",
    "                tokens_np[j, :] = wv[word]\n",
    "            # we can look ahead to find the next word to set as the label for the last word\n",
    "            if j < sequence_length:\n",
    "                labels_np[j, :] = vocab[word]\n",
    "            else: break\n",
    "            j += 1\n",
    "            \n",
    "        tokens_np = tokens_np[:j-1, :]\n",
    "        labels_np = labels_np[:j, :]\n",
    "        \n",
    "        # add this sentence to the overall dataset, with left padding of 0 vectors\n",
    "        processed[i, sequence_length - tokens_np.shape[0]:, :] = tokens_np\n",
    "        labels[i, sequence_length - labels_np.shape[0]:, :] = labels_np\n",
    "    return processed, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162bc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "train_X, train_y = preprocess(train, sequence_length, glove)\n",
    "valid_X, valid_y = preprocess(valid, sequence_length, glove)\n",
    "test_X , test_y  = preprocess(test,  sequence_length, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bf2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test to check the labelling works\n",
    "assert preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0][-1] == 25, \"Output: {}\".format(preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c3221",
   "metadata": {},
   "source": [
    "# LSTM Baseline\n",
    "Create a baseline RNN and evaluate it's perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40b5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
    "        super().__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convert words to their vectors here\n",
    "        sequence_outputs, hidden_state = self.LSTM(x)\n",
    "        sequence_outputs = self.dropout(sequence_outputs)\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bcb3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def numpy_to_tensor(array):\n",
    "    return torch.from_numpy(array).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46abd825",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5c76f6880941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, input_size=300, layer_size=300, dropout=0.1)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c704a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 10001])\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# unit test to check that forward propagation works\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "print(model.forward(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441a5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
    "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
    "def perplexity(preds, ground_truth, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        probs.append(preds[0, i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs)\n",
    "\n",
    "# Calculate overall perplexity for a dataset\n",
    "def average_perplexity(model, X, y):\n",
    "    perplexities = [perplexity(model.predict(numpy_to_tensor(X[i:i+1])), y[i]) for i in range(X.shape[0])]\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fc95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "def train_model(model, train_X, train_y, epochs=10, learn_rate=0.01, weight_decay=0.001, minibatch_size=128, print_results=True):\n",
    "    # Prepare data\n",
    "    X = numpy_to_tensor(train_X)\n",
    "    y = numpy_to_tensor(train_y).long()[:, :, 0]\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Ensure this runs on gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        model.train() # set to train flag\n",
    "        start_ts = time()\n",
    "        \n",
    "        # shuffle the data\n",
    "        new_indices = torch.randperm(n_samples)\n",
    "        X = X[new_indices, :, :] \n",
    "        y = y[new_indices, :]\n",
    "        \n",
    "        for batch_n in range(int(np.ceil(n_samples/minibatch_size))):\n",
    "            # get the minibatch\n",
    "            start_index = batch_n * minibatch_size\n",
    "            end_index = min(start_index + minibatch_size, n_samples)\n",
    "            batch_X = X[start_index: end_index, :, :]\n",
    "            batch_y = y[start_index: end_index, :]\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X) \n",
    "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # evaluate performance on part of the data (for memory reasons we take a subsample)\n",
    "        if print_results:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                preds = np.argmax(X[:2000, :, :].detach().cpu().numpy(), axis=-1).flatten() # flatten the arrays so accuracy score works\n",
    "                targets = y[:2000].detach().cpu().numpy().flatten()\n",
    "                t_perplexity = average_perplexity(model, train_X[:2000], train_y[:2000])\n",
    "                v_perplexity = average_perplexity(model, valid_X, valid_y)\n",
    "                end_ts = time()\n",
    "                print(\"Epoch {}, Minibatch loss: {:.2f}, Subsample Accuracy: {:.2f}, Train Perplexity: {:.2f}, Validation Perplexity: {:.2f}, Epoch Time: {:.2f} seconds\".format(epoch, loss.item(),\n",
    "                    accuracy_score(targets, preds), t_perplexity, v_perplexity, end_ts - start_ts))\n",
    "    \n",
    "    del X\n",
    "    del y\n",
    "    torch.cuda.empty_cache()\n",
    "    if print_results:\n",
    "        print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d51665cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch loss: 5.84, Subsample Accuracy: 0.19, Train Perplexity: 5476.68, Validation Perplexity: 5392.49\n",
      "Epoch 1, Minibatch loss: 5.27, Subsample Accuracy: 0.19, Train Perplexity: 4591.04, Validation Perplexity: 4502.46\n",
      "Epoch 2, Minibatch loss: 5.18, Subsample Accuracy: 0.18, Train Perplexity: 3781.74, Validation Perplexity: 3725.83\n",
      "Epoch 3, Minibatch loss: 5.33, Subsample Accuracy: 0.18, Train Perplexity: 3172.56, Validation Perplexity: 3137.97\n",
      "Epoch 4, Minibatch loss: 5.11, Subsample Accuracy: 0.18, Train Perplexity: 2724.83, Validation Perplexity: 2705.27\n",
      "Epoch 5, Minibatch loss: 5.23, Subsample Accuracy: 0.19, Train Perplexity: 2389.16, Validation Perplexity: 2385.96\n",
      "Epoch 6, Minibatch loss: 5.16, Subsample Accuracy: 0.18, Train Perplexity: 2150.90, Validation Perplexity: 2162.16\n",
      "Epoch 7, Minibatch loss: 4.94, Subsample Accuracy: 0.19, Train Perplexity: 1951.61, Validation Perplexity: 1974.96\n",
      "Epoch 8, Minibatch loss: 4.84, Subsample Accuracy: 0.19, Train Perplexity: 1790.92, Validation Perplexity: 1823.86\n",
      "Epoch 9, Minibatch loss: 4.89, Subsample Accuracy: 0.18, Train Perplexity: 1666.38, Validation Perplexity: 1704.66\n",
      "Epoch 10, Minibatch loss: 4.88, Subsample Accuracy: 0.18, Train Perplexity: 1555.94, Validation Perplexity: 1599.37\n",
      "Epoch 11, Minibatch loss: 4.55, Subsample Accuracy: 0.19, Train Perplexity: 1456.27, Validation Perplexity: 1504.97\n",
      "Epoch 12, Minibatch loss: 4.69, Subsample Accuracy: 0.18, Train Perplexity: 1364.49, Validation Perplexity: 1418.45\n",
      "Epoch 13, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 1257.05, Validation Perplexity: 1313.14\n",
      "Epoch 14, Minibatch loss: 4.76, Subsample Accuracy: 0.19, Train Perplexity: 1187.09, Validation Perplexity: 1243.19\n",
      "Epoch 15, Minibatch loss: 4.81, Subsample Accuracy: 0.18, Train Perplexity: 1119.37, Validation Perplexity: 1178.98\n",
      "Epoch 16, Minibatch loss: 4.90, Subsample Accuracy: 0.18, Train Perplexity: 1057.85, Validation Perplexity: 1119.06\n",
      "Epoch 17, Minibatch loss: 4.98, Subsample Accuracy: 0.18, Train Perplexity: 1001.57, Validation Perplexity: 1064.10\n",
      "Epoch 18, Minibatch loss: 4.79, Subsample Accuracy: 0.18, Train Perplexity: 935.20, Validation Perplexity: 999.06\n",
      "Epoch 19, Minibatch loss: 4.74, Subsample Accuracy: 0.18, Train Perplexity: 876.00, Validation Perplexity: 938.18\n",
      "Epoch 20, Minibatch loss: 4.58, Subsample Accuracy: 0.18, Train Perplexity: 826.86, Validation Perplexity: 890.26\n",
      "Epoch 21, Minibatch loss: 4.57, Subsample Accuracy: 0.18, Train Perplexity: 779.72, Validation Perplexity: 844.10\n",
      "Epoch 22, Minibatch loss: 4.71, Subsample Accuracy: 0.18, Train Perplexity: 748.63, Validation Perplexity: 813.42\n",
      "Epoch 23, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 710.61, Validation Perplexity: 775.64\n",
      "Epoch 24, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 675.82, Validation Perplexity: 741.75\n",
      "Epoch 25, Minibatch loss: 4.67, Subsample Accuracy: 0.19, Train Perplexity: 640.35, Validation Perplexity: 707.65\n",
      "Epoch 26, Minibatch loss: 4.82, Subsample Accuracy: 0.18, Train Perplexity: 609.33, Validation Perplexity: 677.11\n",
      "Epoch 27, Minibatch loss: 4.22, Subsample Accuracy: 0.18, Train Perplexity: 584.62, Validation Perplexity: 650.31\n",
      "Epoch 28, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 555.64, Validation Perplexity: 621.89\n",
      "Epoch 29, Minibatch loss: 4.80, Subsample Accuracy: 0.18, Train Perplexity: 530.58, Validation Perplexity: 596.41\n",
      "Epoch 30, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 510.51, Validation Perplexity: 576.40\n",
      "Epoch 31, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 488.33, Validation Perplexity: 553.29\n",
      "Epoch 32, Minibatch loss: 4.54, Subsample Accuracy: 0.18, Train Perplexity: 464.88, Validation Perplexity: 529.08\n",
      "Epoch 33, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 447.38, Validation Perplexity: 510.87\n",
      "Epoch 34, Minibatch loss: 4.64, Subsample Accuracy: 0.19, Train Perplexity: 432.98, Validation Perplexity: 497.11\n",
      "Epoch 35, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 417.87, Validation Perplexity: 482.30\n",
      "Epoch 36, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 404.87, Validation Perplexity: 468.33\n",
      "Epoch 37, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 393.28, Validation Perplexity: 456.93\n",
      "Epoch 38, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 379.51, Validation Perplexity: 443.89\n",
      "Epoch 39, Minibatch loss: 4.75, Subsample Accuracy: 0.19, Train Perplexity: 370.21, Validation Perplexity: 435.40\n",
      "Epoch 40, Minibatch loss: 4.49, Subsample Accuracy: 0.19, Train Perplexity: 355.71, Validation Perplexity: 420.45\n",
      "Epoch 41, Minibatch loss: 4.42, Subsample Accuracy: 0.19, Train Perplexity: 348.89, Validation Perplexity: 412.52\n",
      "Epoch 42, Minibatch loss: 4.47, Subsample Accuracy: 0.19, Train Perplexity: 340.33, Validation Perplexity: 404.41\n",
      "Epoch 43, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 329.51, Validation Perplexity: 393.30\n",
      "Epoch 44, Minibatch loss: 4.38, Subsample Accuracy: 0.19, Train Perplexity: 322.76, Validation Perplexity: 387.37\n",
      "Epoch 45, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 314.91, Validation Perplexity: 378.22\n",
      "Epoch 46, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 308.03, Validation Perplexity: 371.69\n",
      "Epoch 47, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 301.45, Validation Perplexity: 364.70\n",
      "Epoch 48, Minibatch loss: 4.52, Subsample Accuracy: 0.18, Train Perplexity: 297.91, Validation Perplexity: 361.49\n",
      "Epoch 49, Minibatch loss: 4.44, Subsample Accuracy: 0.19, Train Perplexity: 293.07, Validation Perplexity: 355.72\n",
      "Epoch 50, Minibatch loss: 4.39, Subsample Accuracy: 0.17, Train Perplexity: 289.04, Validation Perplexity: 352.12\n",
      "Epoch 51, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 284.01, Validation Perplexity: 347.42\n",
      "Epoch 52, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 280.74, Validation Perplexity: 344.42\n",
      "Epoch 53, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 275.46, Validation Perplexity: 339.17\n",
      "Epoch 54, Minibatch loss: 4.37, Subsample Accuracy: 0.17, Train Perplexity: 272.82, Validation Perplexity: 335.91\n",
      "Epoch 55, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 269.64, Validation Perplexity: 333.87\n",
      "Epoch 56, Minibatch loss: 4.68, Subsample Accuracy: 0.18, Train Perplexity: 264.01, Validation Perplexity: 328.75\n",
      "Epoch 57, Minibatch loss: 4.41, Subsample Accuracy: 0.17, Train Perplexity: 263.00, Validation Perplexity: 328.00\n",
      "Epoch 58, Minibatch loss: 4.56, Subsample Accuracy: 0.18, Train Perplexity: 262.02, Validation Perplexity: 327.43\n",
      "Epoch 59, Minibatch loss: 4.21, Subsample Accuracy: 0.19, Train Perplexity: 260.36, Validation Perplexity: 323.63\n",
      "Epoch 60, Minibatch loss: 4.35, Subsample Accuracy: 0.19, Train Perplexity: 259.77, Validation Perplexity: 324.26\n",
      "Epoch 61, Minibatch loss: 4.29, Subsample Accuracy: 0.17, Train Perplexity: 259.03, Validation Perplexity: 323.57\n",
      "Epoch 62, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 257.25, Validation Perplexity: 321.99\n",
      "Epoch 63, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 256.43, Validation Perplexity: 321.51\n",
      "Epoch 64, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 255.57, Validation Perplexity: 321.01\n",
      "Epoch 65, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 254.66, Validation Perplexity: 320.84\n",
      "Epoch 66, Minibatch loss: 3.88, Subsample Accuracy: 0.19, Train Perplexity: 253.35, Validation Perplexity: 319.79\n",
      "Epoch 67, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 252.66, Validation Perplexity: 319.94\n",
      "Epoch 68, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 251.55, Validation Perplexity: 319.23\n",
      "Epoch 69, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 249.63, Validation Perplexity: 317.39\n",
      "Epoch 70, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 248.86, Validation Perplexity: 315.35\n",
      "Epoch 71, Minibatch loss: 4.00, Subsample Accuracy: 0.19, Train Perplexity: 248.79, Validation Perplexity: 317.27\n",
      "Epoch 72, Minibatch loss: 3.91, Subsample Accuracy: 0.19, Train Perplexity: 246.19, Validation Perplexity: 314.84\n",
      "Epoch 73, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 247.41, Validation Perplexity: 315.28\n",
      "Epoch 74, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 247.14, Validation Perplexity: 315.10\n",
      "Epoch 75, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 245.09, Validation Perplexity: 313.25\n",
      "Epoch 76, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 246.37, Validation Perplexity: 314.49\n",
      "Epoch 77, Minibatch loss: 3.89, Subsample Accuracy: 0.19, Train Perplexity: 246.49, Validation Perplexity: 315.38\n",
      "Epoch 78, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 246.09, Validation Perplexity: 315.03\n",
      "Epoch 79, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 245.91, Validation Perplexity: 315.10\n",
      "Epoch 80, Minibatch loss: 4.10, Subsample Accuracy: 0.18, Train Perplexity: 247.93, Validation Perplexity: 317.32\n",
      "Epoch 81, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 244.34, Validation Perplexity: 313.44\n",
      "Epoch 82, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 245.80, Validation Perplexity: 314.77\n",
      "Epoch 83, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 244.70, Validation Perplexity: 313.63\n",
      "Epoch 84, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 243.42, Validation Perplexity: 312.25\n",
      "Epoch 85, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 244.68, Validation Perplexity: 313.40\n",
      "Epoch 86, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 242.68, Validation Perplexity: 311.52\n",
      "Epoch 87, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 243.07, Validation Perplexity: 312.91\n",
      "Epoch 88, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 242.19, Validation Perplexity: 312.40\n",
      "Epoch 89, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 242.87, Validation Perplexity: 312.90\n",
      "Epoch 90, Minibatch loss: 4.42, Subsample Accuracy: 0.18, Train Perplexity: 240.35, Validation Perplexity: 309.62\n",
      "Epoch 91, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 240.76, Validation Perplexity: 309.31\n",
      "Epoch 92, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 241.14, Validation Perplexity: 311.39\n",
      "Epoch 93, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 241.42, Validation Perplexity: 310.48\n",
      "Epoch 94, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 239.01, Validation Perplexity: 309.35\n",
      "Epoch 95, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.80, Validation Perplexity: 308.66\n",
      "Epoch 96, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 238.24, Validation Perplexity: 308.37\n",
      "Epoch 97, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 238.68, Validation Perplexity: 308.45\n",
      "Epoch 98, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.69, Validation Perplexity: 308.85\n",
      "Epoch 99, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 237.43, Validation Perplexity: 307.66\n",
      "Epoch 100, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 237.74, Validation Perplexity: 308.04\n",
      "Epoch 101, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 238.30, Validation Perplexity: 308.48\n",
      "Epoch 102, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 236.60, Validation Perplexity: 306.11\n",
      "Epoch 103, Minibatch loss: 4.27, Subsample Accuracy: 0.17, Train Perplexity: 235.80, Validation Perplexity: 305.84\n",
      "Epoch 104, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 235.29, Validation Perplexity: 306.47\n",
      "Epoch 105, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 237.75, Validation Perplexity: 308.24\n",
      "Epoch 106, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 237.73, Validation Perplexity: 308.38\n",
      "Epoch 107, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 237.20, Validation Perplexity: 308.49\n",
      "Epoch 108, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 236.51, Validation Perplexity: 307.32\n",
      "Epoch 109, Minibatch loss: 4.12, Subsample Accuracy: 0.18, Train Perplexity: 235.98, Validation Perplexity: 307.06\n",
      "Epoch 110, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 236.37, Validation Perplexity: 306.64\n",
      "Epoch 111, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 234.71, Validation Perplexity: 304.71\n",
      "Epoch 112, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 233.82, Validation Perplexity: 304.40\n",
      "Epoch 113, Minibatch loss: 4.27, Subsample Accuracy: 0.18, Train Perplexity: 236.07, Validation Perplexity: 306.60\n",
      "Epoch 114, Minibatch loss: 4.40, Subsample Accuracy: 0.18, Train Perplexity: 234.30, Validation Perplexity: 306.02\n",
      "Epoch 115, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 234.13, Validation Perplexity: 304.99\n",
      "Epoch 116, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 234.27, Validation Perplexity: 304.26\n",
      "Epoch 117, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 234.63, Validation Perplexity: 305.40\n",
      "Epoch 118, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 234.81, Validation Perplexity: 305.67\n",
      "Epoch 119, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 235.66, Validation Perplexity: 306.63\n",
      "Epoch 120, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 235.57, Validation Perplexity: 305.68\n",
      "Epoch 121, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 232.76, Validation Perplexity: 303.14\n",
      "Epoch 122, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 232.50, Validation Perplexity: 303.04\n",
      "Epoch 123, Minibatch loss: 4.23, Subsample Accuracy: 0.20, Train Perplexity: 231.39, Validation Perplexity: 301.93\n",
      "Epoch 124, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 232.08, Validation Perplexity: 302.36\n",
      "Epoch 125, Minibatch loss: 4.28, Subsample Accuracy: 0.20, Train Perplexity: 231.29, Validation Perplexity: 300.98\n",
      "Epoch 126, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 229.45, Validation Perplexity: 300.27\n",
      "Epoch 127, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 228.29, Validation Perplexity: 298.52\n",
      "Epoch 128, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 228.74, Validation Perplexity: 299.92\n",
      "Epoch 129, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 228.19, Validation Perplexity: 298.40\n",
      "Epoch 130, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 228.62, Validation Perplexity: 298.51\n",
      "Epoch 131, Minibatch loss: 4.32, Subsample Accuracy: 0.17, Train Perplexity: 229.89, Validation Perplexity: 299.81\n",
      "Epoch 132, Minibatch loss: 4.40, Subsample Accuracy: 0.19, Train Perplexity: 229.14, Validation Perplexity: 300.06\n",
      "Epoch 133, Minibatch loss: 4.35, Subsample Accuracy: 0.17, Train Perplexity: 229.39, Validation Perplexity: 299.79\n",
      "Epoch 134, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 231.70, Validation Perplexity: 301.74\n",
      "Epoch 135, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 230.36, Validation Perplexity: 301.70\n",
      "Epoch 136, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.40, Validation Perplexity: 300.80\n",
      "Epoch 137, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.98, Validation Perplexity: 300.24\n",
      "Epoch 138, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 229.88, Validation Perplexity: 301.13\n",
      "Epoch 139, Minibatch loss: 4.46, Subsample Accuracy: 0.19, Train Perplexity: 228.50, Validation Perplexity: 299.10\n",
      "Epoch 140, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.70, Validation Perplexity: 297.74\n",
      "Epoch 141, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 228.97, Validation Perplexity: 300.98\n",
      "Epoch 142, Minibatch loss: 4.31, Subsample Accuracy: 0.18, Train Perplexity: 228.31, Validation Perplexity: 298.41\n",
      "Epoch 143, Minibatch loss: 4.09, Subsample Accuracy: 0.18, Train Perplexity: 228.53, Validation Perplexity: 299.20\n",
      "Epoch 144, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 226.93, Validation Perplexity: 298.40\n",
      "Epoch 145, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 226.72, Validation Perplexity: 297.92\n",
      "Epoch 146, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 225.46, Validation Perplexity: 295.54\n",
      "Epoch 147, Minibatch loss: 4.07, Subsample Accuracy: 0.19, Train Perplexity: 228.22, Validation Perplexity: 299.47\n",
      "Epoch 148, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 225.41, Validation Perplexity: 296.80\n",
      "Epoch 149, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 226.84, Validation Perplexity: 297.85\n",
      "Epoch 150, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 227.05, Validation Perplexity: 299.09\n",
      "Epoch 151, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 226.02, Validation Perplexity: 298.17\n",
      "Epoch 152, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 226.67, Validation Perplexity: 298.10\n",
      "Epoch 153, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 227.75, Validation Perplexity: 299.15\n",
      "Epoch 154, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 227.36, Validation Perplexity: 298.73\n",
      "Epoch 155, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 225.13, Validation Perplexity: 296.02\n",
      "Epoch 156, Minibatch loss: 4.23, Subsample Accuracy: 0.17, Train Perplexity: 226.37, Validation Perplexity: 297.95\n",
      "Epoch 157, Minibatch loss: 4.44, Subsample Accuracy: 0.17, Train Perplexity: 225.94, Validation Perplexity: 296.60\n",
      "Epoch 158, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 225.86, Validation Perplexity: 296.96\n",
      "Epoch 159, Minibatch loss: 4.37, Subsample Accuracy: 0.18, Train Perplexity: 224.68, Validation Perplexity: 295.32\n",
      "Epoch 160, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 224.34, Validation Perplexity: 295.34\n",
      "Epoch 161, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 222.58, Validation Perplexity: 293.00\n",
      "Epoch 162, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 222.39, Validation Perplexity: 292.02\n",
      "Epoch 163, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 223.47, Validation Perplexity: 293.71\n",
      "Epoch 164, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 225.33, Validation Perplexity: 295.93\n",
      "Epoch 165, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.07, Validation Perplexity: 293.74\n",
      "Epoch 166, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 223.23, Validation Perplexity: 293.61\n",
      "Epoch 167, Minibatch loss: 4.25, Subsample Accuracy: 0.18, Train Perplexity: 225.39, Validation Perplexity: 296.71\n",
      "Epoch 168, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 224.81, Validation Perplexity: 296.14\n",
      "Epoch 169, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.08, Validation Perplexity: 296.97\n",
      "Epoch 170, Minibatch loss: 4.24, Subsample Accuracy: 0.17, Train Perplexity: 224.61, Validation Perplexity: 295.71\n",
      "Epoch 171, Minibatch loss: 4.10, Subsample Accuracy: 0.19, Train Perplexity: 224.76, Validation Perplexity: 296.56\n",
      "Epoch 172, Minibatch loss: 4.15, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 295.98\n",
      "Epoch 173, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 224.21, Validation Perplexity: 296.21\n",
      "Epoch 174, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.11, Validation Perplexity: 296.44\n",
      "Epoch 175, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 223.87, Validation Perplexity: 295.96\n",
      "Epoch 176, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 296.70\n",
      "Epoch 177, Minibatch loss: 4.45, Subsample Accuracy: 0.19, Train Perplexity: 222.85, Validation Perplexity: 294.78\n",
      "Epoch 178, Minibatch loss: 4.18, Subsample Accuracy: 0.18, Train Perplexity: 223.25, Validation Perplexity: 294.85\n",
      "Epoch 179, Minibatch loss: 4.50, Subsample Accuracy: 0.19, Train Perplexity: 222.41, Validation Perplexity: 293.88\n",
      "Epoch 180, Minibatch loss: 3.95, Subsample Accuracy: 0.19, Train Perplexity: 222.83, Validation Perplexity: 294.24\n",
      "Epoch 181, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 223.08, Validation Perplexity: 294.20\n",
      "Epoch 182, Minibatch loss: 4.13, Subsample Accuracy: 0.18, Train Perplexity: 223.52, Validation Perplexity: 295.34\n",
      "Epoch 183, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 220.96, Validation Perplexity: 292.61\n",
      "Epoch 184, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 223.21, Validation Perplexity: 294.52\n",
      "Epoch 185, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 222.59, Validation Perplexity: 293.70\n",
      "Epoch 186, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 223.89, Validation Perplexity: 295.60\n",
      "Epoch 187, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 224.00, Validation Perplexity: 295.06\n",
      "Epoch 188, Minibatch loss: 4.25, Subsample Accuracy: 0.19, Train Perplexity: 222.02, Validation Perplexity: 294.40\n",
      "Epoch 189, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 221.55, Validation Perplexity: 293.86\n",
      "Epoch 190, Minibatch loss: 4.52, Subsample Accuracy: 0.19, Train Perplexity: 222.05, Validation Perplexity: 292.84\n",
      "Epoch 191, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 223.65, Validation Perplexity: 294.15\n",
      "Epoch 192, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 221.89, Validation Perplexity: 292.28\n",
      "Epoch 193, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 221.92, Validation Perplexity: 293.61\n",
      "Epoch 194, Minibatch loss: 4.32, Subsample Accuracy: 0.18, Train Perplexity: 221.51, Validation Perplexity: 292.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-619d9a32a5fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-43f7dfafae68>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_X, train_y, epochs, learn_rate, weight_decay, minibatch_size, print_results)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_X, train_y, epochs=200, learn_rate=0.001, minibatch_size=256, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a5e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTMBaseline.model\") # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fff3960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model, only run if the model hasn't already been trained\n",
    "model.load_state_dict(torch.load(\"LSTMBaseline.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab2088d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2e2dd",
   "metadata": {},
   "source": [
    "### Hyper-Parameter tuning findings\n",
    "- glove 300 dimension vectors are essential to not have a bias of 1000 perplexity on both train and validation\n",
    "- 2 layers of LSTM also gives high bias, perhaps there is not enough train data\n",
    "- Weight decay is essential in preventing Validation perplexity from skyrocketing\n",
    "- Dropout of 0.1 combined with weight decay 0.00001 works (around 250 validation perplexity)\n",
    "- Decreasing learning rate and increasing epochs has a minor benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db063f69",
   "metadata": {},
   "source": [
    "### Examine Performance of the model\n",
    "- Using both perplexity and qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "228f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view overall performance\n",
    "model.eval()\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "preds = model.predict(data)\n",
    "#preds = torch.nn.Softmax(dim=-1)(preds).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ad8a2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probably',\n",
       " 'help',\n",
       " 'begin',\n",
       " 'also',\n",
       " 'take',\n",
       " 'make',\n",
       " 'continue',\n",
       " 'the',\n",
       " 'have',\n",
       " 'be']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: index_vocab[x], np.argsort(preds[1, 11, :])[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f38177fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1733707e-07, 5.4392586e-07, 5.5573935e-07, ..., 1.3860598e-02,\n",
       "       3.9391726e-02, 7.3702824e-01], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(preds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d20a349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['<PAD>', '<PAD>', 'securities', \"'s\", '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '.', '.', '<PAD>', '<PAD>', '<PAD>', 'the', '<PAD>', '<PAD>', '<PAD>', \"'s\", '<PAD>']\n",
      "Input:     ['<PAD>', 'aer', 'banknote', 'berlitz', 'calloway', 'cluett', 'fromstein', 'gitano', 'guterman', 'ipo', 'kia', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'ssangyong', 'swapo', 'wachter']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'the', '.', '.', 'be', 'the', 'company', \"'s\", 'a', 'new', 'director', 'of', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'pierre', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'said', 'a', 'of', 'the', 'inc.', 'company', \"'s\", 'group', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'mr.', 'is', 'chairman', 'of', 'n.v.', 'the', 'dutch', 'publishing', 'group', '.']\n",
      "\n",
      "Predicted: ['<PAD>', \"'s\", 'ago', 'was', 'the', 'president', 'of', 'the', 'inc.', 'and', 'in', 'a', 'a', 'a', 'share', 'director', 'of', 'the', 'year', 'government']\n",
      "Input:     ['<PAD>', 'rudolph', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial']\n",
      "\n",
      "Predicted: ['<PAD>', 'new', 'of', 'the', 'and', 'the', 'to', 'the', 'the', 'the', 'and', 'to', 'been', 'by', 'share', 'court', 'of', 'the', 'institute', '.']\n",
      "Input:     ['<PAD>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among']\n",
      "\n",
      "Predicted: ['<PAD>', 'company', 'and', 'and', 'a', 'high', 'the', 'is', 'the', 'company', 'the', 'if', 'and', 'and', 'the', \"'s\", 'the', '.', 'the', '.']\n",
      "Input:     ['<PAD>', 'the', 'asbestos', 'fiber', 'is', 'unusually', 'once', 'it', 'enters', 'the', 'with', 'even', 'brief', 'exposures', 'to', 'it', 'causing', 'symptoms', 'that', 'show', 'up']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', 'said', 'company', 'of', 'the', 'york', 'company', 'said', 'the', 'the', \"'s\", 'and', 'the', 'the', 'the', 'own', 'and', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', 'inc.', 'the', 'unit', 'of', 'new', 'york-based', 'corp.', 'that', 'makes', 'kent', 'cigarettes', 'stopped', 'using', 'in', 'its', 'cigarette', 'filters', 'in', '.']\n",
      "\n",
      "Predicted: ['<PAD>', 'the', 'agreement', 'that', \"n't\", 'a', 'than', 'the', 'new', 'earlier', 'the', 'company', 'quarter', 'of', 'to', 'the', \"'s\", '.', 'york', '.']\n",
      "Input:     ['<PAD>', 'although', 'preliminary', 'findings', 'were', 'reported', 'more', 'than', 'a', 'year', 'ago', 'the', 'latest', 'results', 'appear', 'in', 'today', \"'s\", 'new', 'england', 'journal']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'share', 'the', 'year', 'a', 'ounce', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'a', 'said', 'this', 'is', 'an', 'old', 'story', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', \"'re\", 'going', 'about', 'the', 'ago', 'the', 'the', 'who', 'in', 'the', 'and', 'a', 'of', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_int = np.argmax(preds, axis=-1)\n",
    "for i in range(10):\n",
    "    sentence = list(map(index_vocab.get, preds_int[i]))\n",
    "    truth = list(map(lambda x: index_vocab[int(x)], train_y[i]))\n",
    "    input_sentence = ['<PAD>'] + truth\n",
    "    print(\"Predicted:\", sentence)\n",
    "    print(\"Input:    \",input_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75bd595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n',\n",
       " ' pierre <unk> N years old will join the board as a nonexecutive director nov. N \\n',\n",
       " ' mr. <unk> is chairman of <unk> n.v. the dutch publishing group \\n',\n",
       " ' rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \\n',\n",
       " ' a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \\n',\n",
       " ' the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \\n',\n",
       " ' <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \\n',\n",
       " \" although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\",\n",
       " ' a <unk> <unk> said this is an old story \\n',\n",
       " \" we 're talking about years ago before anyone heard of asbestos having any questionable properties \\n\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71ab8782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.610664"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model.predict(numpy_to_tensor(valid_X[0:1]))\n",
    "perplexity(pred1, valid_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92debfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.79262"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b8efd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.91684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4b94e-9fb6-4b4d-8a66-1b87cd9f41ec",
   "metadata": {},
   "source": [
    "# GPT2 Baseline\n",
    "Implement GPT2 as a language modelling baseline. GPT-3 is not publicly available and too large for practical purposes. BERT needs modification to work for language modelling, due to the fact that it is trained for bidirectional masked language modelling instead.\n",
    "\n",
    "This section makes use of several tutorials for fine tuning, including:\n",
    "- https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/\n",
    "- https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
    "- https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel (the documentation)\n",
    "- https://huggingface.co/transformers/custom_datasets.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a61aafc-e915-4ff8-b371-3fc32331fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loadGPT, BERT and support materials from huggingface\n",
    "# requires pip install transformers\n",
    "# if in jupyter notebook see here and you get an error mention ipython widgets see here: \n",
    "# https://stackoverflow.com/questions/53247985/tqdm-4-28-1-in-jupyter-notebook-intprogress-not-found-please-update-jupyter-an\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7d485f60-6997-409f-80f2-f4a7a6a6987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "42140794-4ad1-4999-b6ab-1bd13ba648d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models\n",
    "# Documentation for GPT: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e12b27c-c788-40c8-9265-e724d5fd46ab",
   "metadata": {},
   "source": [
    "### Experimenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa755a4d-7ad0-4e3c-b00c-669513e9c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hugging Face is based in DUMBO, New York City, and is about a middle-aged man named John. When he wakes up in his dorm room to find'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2 example generation\n",
    "text = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "tokens_tensor = gpt_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Put everything on cuda\n",
    "gpt_model.eval()\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "gpt_model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "generated = tokens_tensor\n",
    "for i in range(20):\n",
    "    next_token_logits = gpt_model(generated).logits[:, -1, :]\n",
    "    # filter\n",
    "    filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "    # sample\n",
    "    probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "resulting_string = gpt_tokenizer.decode(generated.tolist()[0])\n",
    "resulting_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6795d64b-f959-450d-b9d2-2dce113b96a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' rud', 'olph', ' <', 'unk', '>', ' N', ' years', ' old', ' and', ' former', ' chairman', ' of', ' consolidated', ' gold', ' fields', ' pl', 'c', ' was', ' named', ' a', ' nonex', 'ec', 'utive', ' director', ' of', ' this', ' b', 'rit', 'ish', ' industrial', ' conglomerate', ' ', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens = gpt_tokenizer.encode(train[3])\n",
    "print([gpt_tokenizer.decode([x]) for x in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a79ac-e65d-4082-bc08-22cfeee91848",
   "metadata": {},
   "source": [
    "We can see that the gpt_tokenizer works differently to ours, splitting up names such as 'rudolph' into 'rud' and 'olph' and words such as nonexecutive and british. Hence our perplexity evaluation will have to be slightly different, using gpt_tokenizer to get the ground truth labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612a2463-2077-4fb4-867f-f2b928af469d",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3ad00057-0e44-4206-b291-971fed2150a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataset class for fine-tuning, it's a generator so we don't have to store the entire dataset in memory\n",
    "class GPT2Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=40):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.input_ids = []\n",
    "        self.attn_masks = []\n",
    "\n",
    "        # Encode all the text, padding and truncuating it along with adding attention masks to get the sequence length the same across all samples\n",
    "        for txt in txt_list:\n",
    "            encodings_dict = tokenizer.encode_plus('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
    "            self.input_ids.append(encodings_dict['input_ids'])\n",
    "            self.attn_masks.append(encodings_dict['attention_mask'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The tutorial use a dictionary format that also stores labels \n",
    "        return_dict = {\"input_ids\": torch.tensor(self.input_ids[idx]),\n",
    "                       \"attention_mask\": torch.tensor(self.attn_masks[idx]), \n",
    "                       \"labels\": torch.tensor(self.input_ids[idx])} \n",
    "        return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "943bca9e-d14d-4f87-9b50-d62e6c5efe90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
       "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
       "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
       "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
       " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " 'labels': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
       "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
       "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
       "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token # set the pad token\n",
    "gpt_sequence_length = 40 # gpt splits up words into smaller tokens, so the sequence length should be longer\n",
    "train_dataset = GPT2Dataset(train, gpt_tokenizer, max_length=gpt_sequence_length)\n",
    "val_dataset = GPT2Dataset(valid, gpt_tokenizer, max_length=gpt_sequence_length)\n",
    "train_dataset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8b4c8184-ad9d-473e-861f-51e7f265e7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutputWithCrossAttentions(loss=None, logits=tensor([[-31.7761, -30.6999, -32.1561,  ..., -39.5131, -39.7474, -31.6402],\n",
       "        [-63.3376, -60.9081, -61.4334,  ..., -72.3227, -72.0701, -62.4869],\n",
       "        [-53.8055, -53.4089, -53.4207,  ..., -63.6040, -62.0605, -54.8142],\n",
       "        ...,\n",
       "        [-82.7951, -76.3731, -78.5615,  ..., -95.3905, -96.0068, -84.1115],\n",
       "        [-82.7967, -76.3781, -78.5659,  ..., -95.3878, -96.0029, -84.1119],\n",
       "        [-82.8332, -76.4163, -78.6067,  ..., -95.4225, -96.0345, -84.1454]],\n",
       "       device='cuda:0', grad_fn=<MmBackward>), past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get output by passing the ids and the attention mask\n",
    "gpt_model(input_ids=train_dataset[1]['input_ids'].to(device), attention_mask=train_dataset[1]['attention_mask'].to(device), use_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6a4e19c1-51c5-41ca-b369-297c7750d752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|startoftext|> the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_tokenizer.decode(train_dataset[5]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1ea2b-d89b-4638-bcb2-cf63e7d71211",
   "metadata": {},
   "source": [
    "### Fine Tuning\n",
    "\n",
    "Do fine tuning using the hugging face out of the box trainer https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainerfrom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "51091355-c64c-4efb-ac21-68232c8e9d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2630/2630 04:25, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.079300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.627500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.437800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.208600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.202500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.159200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.132100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>2.115800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>2.134400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.137000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>2.103500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>2.097800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>2.074500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>2.119900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>2.111600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>2.067200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>2.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>2.063600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.064300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>2.111200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2630, training_loss=2.287749967103675, metrics={'train_runtime': 265.3678, 'train_samples_per_second': 9.911, 'total_flos': 61761965506560.0, 'epoch': 1.0})"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='gpt_finetuning',     # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs (1 is enough to get very low perplexity and perplexity increases at 2)\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.001,               # strength of weight decay\n",
    "    logging_dir='gpt_finetuning_logs',            # directory for storing logs\n",
    "    logging_steps=100,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=gpt_model,                         # the instantiated  Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=val_dataset             # evaluation dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00e5ae3-672d-4c81-9695-8e01ad6adad3",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "3923d864-8141-4525-a915-930e03c08ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model wrapper for gpt2 that uses the \"past\" variable and for language modelling\n",
    "# TODO: Fine tuning - need to add padding for backprop in torch to work (just add '.' at the end for padding)\n",
    "# TODO: add the options for beam search\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, model=None, sequence_length=20):\n",
    "        super().__init__()\n",
    "        self.gpt = model.to(device)\n",
    "        self.tokenizer = gpt_tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    \n",
    "    # output the logits for the most likely next word at each position in the sentence\n",
    "    # note input_dataset must be an element taken from a GPT2Dataset class (e.g. train_dataset[0])\n",
    "    def forward(self, input_dataset):\n",
    "        output = self.gpt.forward(input_ids = input_dataset['input_ids'].to(device), \n",
    "                                  attention_mask=input_dataset['attention_mask'].to(device),\n",
    "                                  use_cache=False)\n",
    "        return output[\"logits\"]\n",
    "    \n",
    "    # take in a sentence and output the predictions as in forward, but as the most likely sentence not logits\n",
    "    def forward_sentence(self, input_dataset):\n",
    "        preds = self.forward(input_dataset)\n",
    "        tokens = torch.argmax(preds, dim=-1)\n",
    "        return self.tokenizer.decode(tokens)\n",
    "    \n",
    "    # generate a sentence by sampling the next word from the probability distribution\n",
    "    # set limit to an integer to generate `limit` number of words instead of ending at a full stop\n",
    "    def random_gen(self, x, limit=None):\n",
    "        # initialize variables\n",
    "        generated = self.tokenizer.encode_plus(x, return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "        next_token = [generated[0][-1]]\n",
    "        past = None\n",
    "        raw_output= None\n",
    "        \n",
    "        # generate until a \".\" is generated\n",
    "        while (limit is None and self.tokenizer.decode(next_token[0]) not in [\".\", \"?\", \"!\"]) or (limit is not None and len(generated[0]) < limit):\n",
    "            # get output of model, using past if available\n",
    "            if past is None:\n",
    "                raw_output = self.gpt(generated, past_key_values=past)\n",
    "            else:\n",
    "                raw_output = self.gpt(next_token, past_key_values=past)\n",
    "            output, past = raw_output['logits'], raw_output['past_key_values']\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # sample a token from the top 50 most likely words\n",
    "            filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0) # filter to the top 50 tokens\n",
    "            probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "        return self.tokenizer.decode(generated[0])\n",
    "    \n",
    "    # do beam_search to find the most likely sentence\n",
    "    def beam_search(self, x, beam=5): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a88b3953-f775-4979-940e-7733c5240072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6970e+02, -1.6412e+02, -1.6493e+02,  ..., -2.0270e+02,\n",
       "         -2.0474e+02, -1.6856e+02],\n",
       "        [-2.6901e+00, -1.6066e+00,  4.1629e-01,  ..., -9.9957e+00,\n",
       "         -1.0583e+01,  7.4749e-02],\n",
       "        [-2.3482e+01, -2.3870e+01, -2.1071e+01,  ..., -4.0187e+01,\n",
       "         -3.3782e+01, -2.0225e+01],\n",
       "        ...,\n",
       "        [-7.0412e+01, -7.0517e+01, -7.2410e+01,  ..., -7.6573e+01,\n",
       "         -7.6298e+01, -6.6031e+01],\n",
       "        [-6.7842e+01, -6.6881e+01, -6.5982e+01,  ..., -7.9229e+01,\n",
       "         -7.9465e+01, -6.2473e+01],\n",
       "        [-6.4527e+01, -6.3917e+01, -6.5069e+01,  ..., -7.2385e+01,\n",
       "         -7.2153e+01, -6.0531e+01]], device='cuda:0', grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelgpt = GPTModel(model=gpt_model, sequence_length=gpt_sequence_length)\n",
    "modelgpt.eval()\n",
    "modelgpt.forward(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164f4366-5d71-4ad9-85b7-d9b6b5dd9964",
   "metadata": {},
   "source": [
    "View the next word output for a single example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66430c0-a4d8-4549-9861-7684003c3a78",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fb0a5e37-8004-4bc0-a198-2a42741296bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The format is:\n",
      "(Ground Truth word, Predicted next word),\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('<', '|'),\n",
       " ('|', 'start'),\n",
       " ('start', 'of'),\n",
       " ('of', 'text'),\n",
       " ('text', '|'),\n",
       " ('|', '>'),\n",
       " ('>', ' the'),\n",
       " (' the', ' <'),\n",
       " (' total', ' value'),\n",
       " (' of', ' the'),\n",
       " (' N', ' N'),\n",
       " (' deaths', ' from'),\n",
       " (' from', ' <'),\n",
       " (' mal', 'ignant'),\n",
       " ('ignant', ' <'),\n",
       " (' <', 'unk'),\n",
       " ('unk', '>'),\n",
       " ('>', ' <'),\n",
       " (' lung', ' cancer'),\n",
       " (' cancer', ' in'),\n",
       " (' and', ' <'),\n",
       " (' <', 'unk'),\n",
       " ('unk', '>'),\n",
       " ('>', ' <'),\n",
       " (' was', ' N'),\n",
       " (' far', ' below'),\n",
       " (' higher', ' than'),\n",
       " (' than', ' the'),\n",
       " (' expected', ' '),\n",
       " (' the', ' study'),\n",
       " (' researchers', ' said'),\n",
       " (' said', ' '),\n",
       " (' ', '\\n'),\n",
       " ('\\n', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>'),\n",
       " ('<|endoftext|>', '<|endoftext|>')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 18\n",
    "input_t = [gpt_tokenizer.decode(t) for t in train_dataset[i]['input_ids']]\n",
    "preds = modelgpt.forward(train_dataset[i])\n",
    "tokens = torch.argmax(preds, dim=-1)\n",
    "output_t = [gpt_tokenizer.decode(t) for t in tokens]\n",
    "print(\"The format is:\")\n",
    "print(\"(Ground Truth word, Predicted next word),\")\n",
    "list(zip(input_t, output_t))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b5a30-3012-4bcf-b3a9-768fdbaac8f7",
   "metadata": {},
   "source": [
    "View the ability to generate without teacher forcing using the random_gen() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d5fd0e6-ea4c-428d-900d-87e68583c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A new study says that the N to N share of the sales growth in the two sectors is far more than the number of sales that would be achieved in the N sector only when sales were concentrated in'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = modelgpt.random_gen(\"A new study says\", limit=40)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2996c849-0809-4fbe-8f16-211cdbbb1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the mask is 0 at index i don't use the value at index i to calculate perplexity\n",
    "def perplexity(preds, ground_truth, mask, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[0]):\n",
    "        if mask[i] != 0:\n",
    "            probs.append(preds[i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs).detach().cpu().numpy()\n",
    "\n",
    "# Can optionally define n_samples=int to limit the number of samples used for perplexity evaluation\n",
    "def average_perplexity_gpt(model, train, n_samples=None, print_results=False):\n",
    "    perplexities = []\n",
    "    n_samples = len(train) if n_samples is None else n_samples\n",
    "    with torch.no_grad():\n",
    "        for i in range(n_samples):\n",
    "            # Compute perplexity for a single sample\n",
    "            labels = train[i]['input_ids'][1:]\n",
    "            mask = train[i]['attention_mask'][:-1]\n",
    "            preds = model.forward(train[i])[:-1] # remove the last prediction as there is no ground truth \n",
    "            preds = torch.nn.functional.softmax(preds, dim=-1)\n",
    "            perplexities.append(perplexity(preds, labels, mask))\n",
    "\n",
    "            if i % 100 == 0 and print_results:\n",
    "                print(\"Sentences analysed: {} Average perplexity: {}\".format(i, np.mean(perplexities)))\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7f850a00-c2d8-4efd-827d-e026b698c38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences analysed: 0 Average perplexity: 1289.648193359375\n",
      "Sentences analysed: 100 Average perplexity: 23.94414710998535\n",
      "Sentences analysed: 200 Average perplexity: 18.348875045776367\n",
      "Sentences analysed: 300 Average perplexity: 15.814547538757324\n",
      "Sentences analysed: 400 Average perplexity: 14.741545677185059\n",
      "Sentences analysed: 500 Average perplexity: 13.91878890991211\n",
      "Sentences analysed: 600 Average perplexity: 13.654435157775879\n",
      "Sentences analysed: 700 Average perplexity: 13.4156494140625\n",
      "Sentences analysed: 800 Average perplexity: 13.6211519241333\n",
      "Sentences analysed: 900 Average perplexity: 13.486576080322266\n",
      "Sentences analysed: 1000 Average perplexity: 13.251569747924805\n",
      "Sentences analysed: 1100 Average perplexity: 12.975820541381836\n",
      "Sentences analysed: 1200 Average perplexity: 12.911108016967773\n",
      "Sentences analysed: 1300 Average perplexity: 12.780496597290039\n",
      "Sentences analysed: 1400 Average perplexity: 12.537127494812012\n",
      "Sentences analysed: 1500 Average perplexity: 12.428074836730957\n",
      "Sentences analysed: 1600 Average perplexity: 12.531476974487305\n",
      "Sentences analysed: 1700 Average perplexity: 12.64409351348877\n",
      "Sentences analysed: 1800 Average perplexity: 12.575571060180664\n",
      "Sentences analysed: 1900 Average perplexity: 12.461880683898926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12.501836"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity_gpt(modelgpt, train_dataset, n_samples=2000, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03f37-c70f-4e60-a223-345f58118a46",
   "metadata": {},
   "source": [
    "GPT2 Results\n",
    "- No fine tuning: 706.16187 perplexity\n",
    "- Fine tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de890b",
   "metadata": {},
   "source": [
    "# Neural ODE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1c08b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../')\n",
    "from torchdyn.models import *\n",
    "from torchdyn.datasets import *\n",
    "from torchdyn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b901011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Neural ODE that uses an LSTMCell as the derivative function\n",
    "class ODELSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
    "        super().__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False) # this encodes the sequence\n",
    "        #self.fnode = torch.nn.LSTMCell(input_size, layer_size, 1, bidirectional=False)\n",
    "        self.f = torch.nn.Sequential(\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "        )\n",
    "        self.node = NeuralDE(self.f, sensitivity='adjoint', solver='dopri5').to(device)\n",
    "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # at the moment this feeds the entire sequence to LSTM and asks Neural ODE to reproduce it\n",
    "        # TODO: switch to feeding half the sequence and asking NeuralODE to extrapolate\n",
    "        sequence_outputs, hidden_state = self.LSTM(x)\n",
    "        final_hidden = sequence_outputs[:, -1, :]\n",
    "\n",
    "        # feed to neural ode\n",
    "        timesteps = torch.linspace(0, sequence_length-1, sequence_length).to(device)\n",
    "        sequence_outputs = self.node.trajectory(final_hidden, timesteps) # input is 128 final hidden states of dimension 300\n",
    "        sequence_outputs = torch.swapaxes(sequence_outputs, 0, 1)        # output is output across 20 timesteps giving as (20, 128, 300) output, so swap the sequence and batch dimension\n",
    "        \n",
    "        # Get final output\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17d802e-e447-4295-a3ed-e8f5d3fed787",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODEmodel = ODELSTM(vocab_size, input_size=300, layer_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0b8361-ec7b-4561-a2f5-93d6b35960b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch loss: 5.62, Subsample Accuracy: 0.19, Train Perplexity: 992.81, Validation Perplexity: 949.61, Epoch Time: 334.68 seconds\n",
      "Epoch 1, Minibatch loss: 5.38, Subsample Accuracy: 0.18, Train Perplexity: 825.13, Validation Perplexity: 815.57, Epoch Time: 438.83 seconds\n",
      "Epoch 2, Minibatch loss: 5.46, Subsample Accuracy: 0.19, Train Perplexity: 718.59, Validation Perplexity: 734.18, Epoch Time: 517.79 seconds\n",
      "Epoch 3, Minibatch loss: 5.50, Subsample Accuracy: 0.19, Train Perplexity: 699.71, Validation Perplexity: 762.84, Epoch Time: 598.56 seconds\n",
      "Epoch 4, Minibatch loss: 5.32, Subsample Accuracy: 0.18, Train Perplexity: 602.00, Validation Perplexity: 666.84, Epoch Time: 600.94 seconds\n",
      "Epoch 5, Minibatch loss: 5.17, Subsample Accuracy: 0.18, Train Perplexity: 601.95, Validation Perplexity: 663.96, Epoch Time: 599.20 seconds\n",
      "Epoch 6, Minibatch loss: 5.11, Subsample Accuracy: 0.18, Train Perplexity: 1000.37, Validation Perplexity: 742.20, Epoch Time: 607.65 seconds\n",
      "Epoch 7, Minibatch loss: 5.13, Subsample Accuracy: 0.19, Train Perplexity: 650.42, Validation Perplexity: 778.69, Epoch Time: 626.19 seconds\n",
      "Epoch 8, Minibatch loss: 4.74, Subsample Accuracy: 0.18, Train Perplexity: 620.08, Validation Perplexity: 753.30, Epoch Time: 672.27 seconds\n",
      "Epoch 9, Minibatch loss: 4.98, Subsample Accuracy: 0.19, Train Perplexity: 586.12, Validation Perplexity: 651.04, Epoch Time: 722.94 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NODEmodel = train_model(NODEmodel, train_X, train_y, epochs=10, learn_rate=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb801211-f4bf-47ad-83cc-a22996691a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "del NODEmodel\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
