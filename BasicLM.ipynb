{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60a92153-258b-4d12-b136-c1d4ff34b715",
   "metadata": {},
   "source": [
    "# Requirements\n",
    "Requires an Nvidia GPU to run\n",
    "\n",
    "Create a new anaconda environment and run the following commands to install the required libraries \n",
    "```\n",
    "conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n",
    "conda install gensim\n",
    "pip install torchdyn\n",
    "pip install git+https://github.com/google-research/torchsde.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dfe412-887c-43b0-ae9e-68e68207a975",
   "metadata": {},
   "source": [
    "# Citations\n",
    "- Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993). Building a Large Annotated Corpus of English: The Penn Treebank\n",
    "\n",
    "```\n",
    "@article{poli2020torchdyn,\n",
    "  title={TorchDyn: A Neural Differential Equations Library},\n",
    "  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},\n",
    "  journal={arXiv preprint arXiv:2009.09346},\n",
    "  year={2020}\n",
    "}\n",
    "```\n",
    "\n",
    "- GloVe\n",
    "\n",
    "- GPT2 paper\n",
    "\n",
    "- Huggingface for their implementation of transformers? Not sure if this has a paper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dea389f-a316-483d-94f9-3e80e54e517d",
   "metadata": {},
   "source": [
    "# To do\n",
    "- Use BERT as a baseline and possibly an encoder\n",
    "- Examine mini batch calculation\n",
    "- Use LSTM function \n",
    "- Consider other variants of Neural ODE\n",
    "- Implement and see results from my continuous language modelling idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ff0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import torchtext\n",
    "import numpy as np\n",
    "import gensim.downloader as api\n",
    "from functools import reduce\n",
    "from sklearn.metrics import *\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469c71b",
   "metadata": {},
   "source": [
    "# Pre Processing\n",
    "- Build the vocab\n",
    "- Convert text corpus into padded word vector sequences\n",
    "\n",
    "To do\n",
    "- Use LSTM as baseline\n",
    "    - Examine perplexity of model on validation set\n",
    "- Implement Neural ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c916364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load word embeddings\n",
    "glove = api.load(\"glove-wiki-gigaword-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e5e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd8d4c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  10001\n"
     ]
    }
   ],
   "source": [
    "# build the vocab\n",
    "corpus = train + valid\n",
    "vocab = {\"<PAD>\": 0}\n",
    "index_vocab = {0 : \"<PAD>\"}\n",
    "for sentence in corpus:\n",
    "    for token in sentence.split(\" \")[1:]:\n",
    "        if token not in vocab:\n",
    "            index = len(vocab)\n",
    "            vocab[token] = index\n",
    "            index_vocab[index] = token\n",
    "\n",
    "# replace penn treebank end sentence token \"\\n\" with glove's end sentence token \".\"\n",
    "index = vocab[\"\\n\"]\n",
    "vocab.pop(\"\\n\")         \n",
    "vocab[\".\"] = index\n",
    "index_vocab[index] = \".\"\n",
    "\n",
    "# view size\n",
    "vocab_size = len(vocab)\n",
    "print(\"Vocab size: \", vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb19c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pad sentences and convert words to their glove vector to get input features\n",
    "# convert to 1 hot vocab and shift 1 to the left to get output labels (converting to 1 hot takes too much memory, so just store indices and convert later)\n",
    "# use left padding, as we want the hidden state at the end (right) to ignore the padding\n",
    "# returns word_vector_dataset, labels\n",
    "def preprocess(dataset, sequence_length, wv):\n",
    "    embedding_size = wv[\"hello\"].shape[0]\n",
    "    processed = np.zeros((len(dataset), sequence_length, embedding_size))\n",
    "    labels = np.zeros((len(dataset), sequence_length, 1))\n",
    "    \n",
    "    for i in range(len(dataset)):\n",
    "        tokens = dataset[i].split(\" \")[1:]\n",
    "        \n",
    "        # get the word vectors for all of the tokens, removing out of vocabulary (OOV) tokens\n",
    "        tokens_np = np.zeros((len(tokens), embedding_size))\n",
    "        labels_np = np.zeros((len(tokens), 1))\n",
    "        j = 0\n",
    "        for word in tokens:\n",
    "            if word == \"\\n\": word = \".\" # replace PennTreebank end sentence token '\\n' with glove end sentence token \".\"\n",
    "            if word not in wv: continue # ignore OOV tokens\n",
    "            if j < sequence_length - 1: # only add sequence_length - 1 tokens at max\n",
    "                # so that there is always a 0 vector at the start so the model learns most common starting words\n",
    "                tokens_np[j, :] = wv[word]\n",
    "            # we can look ahead to find the next word to set as the label for the last word\n",
    "            if j < sequence_length:\n",
    "                labels_np[j, :] = vocab[word]\n",
    "            else: break\n",
    "            j += 1\n",
    "            \n",
    "        tokens_np = tokens_np[:j-1, :]\n",
    "        labels_np = labels_np[:j, :]\n",
    "        \n",
    "        # add this sentence to the overall dataset, with left padding of 0 vectors\n",
    "        processed[i, sequence_length - tokens_np.shape[0]:, :] = tokens_np\n",
    "        labels[i, sequence_length - labels_np.shape[0]:, :] = labels_np\n",
    "    return processed, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "162bc724",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 20\n",
    "train_X, train_y = preprocess(train, sequence_length, glove)\n",
    "valid_X, valid_y = preprocess(valid, sequence_length, glove)\n",
    "test_X , test_y  = preprocess(test,  sequence_length, glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37bf2ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test to check the labelling works\n",
    "assert preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0][-1] == 25, \"Output: {}\".format(preprocess([\"hello there how are you doing \\n\"], 20, glove)[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267c3221",
   "metadata": {},
   "source": [
    "# LSTM Baseline\n",
    "Create a baseline RNN and evaluate it's perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c40b5ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
    "        super().__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # convert words to their vectors here\n",
    "        sequence_outputs, hidden_state = self.LSTM(x)\n",
    "        sequence_outputs = self.dropout(sequence_outputs)\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7bcb3341",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def numpy_to_tensor(array):\n",
    "    return torch.from_numpy(array).to(device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "46abd825",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-5c76f6880941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTMModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayer_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocab_size' is not defined"
     ]
    }
   ],
   "source": [
    "model = LSTMModel(vocab_size, input_size=300, layer_size=300, dropout=0.1)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c704a4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 20, 10001])\n",
      "Wall time: 642 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# unit test to check that forward propagation works\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "print(model.forward(data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "025faae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear memory\n",
    "del data\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "441a5611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
    "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
    "def perplexity(preds, ground_truth, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        probs.append(preds[0, i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs)\n",
    "\n",
    "# Calculate overall perplexity for a dataset\n",
    "def average_perplexity(model, X, y):\n",
    "    perplexities = [perplexity(model.predict(numpy_to_tensor(X[i:i+1])), y[i]) for i in range(X.shape[0])]\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fc95fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model\n",
    "def train_model(model, train_X, train_y, epochs=10, learn_rate=0.01, weight_decay=0.001, minibatch_size=128, print_results=True):\n",
    "    # Prepare data\n",
    "    X = numpy_to_tensor(train_X)\n",
    "    y = numpy_to_tensor(train_y).long()[:, :, 0]\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Ensure this runs on gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        model.train() # set to train flag\n",
    "        start_ts = time()\n",
    "        \n",
    "        # shuffle the data\n",
    "        new_indices = torch.randperm(n_samples)\n",
    "        X = X[new_indices, :, :] \n",
    "        y = y[new_indices, :]\n",
    "        \n",
    "        for batch_n in range(int(np.ceil(n_samples/minibatch_size))):\n",
    "            # get the minibatch\n",
    "            start_index = batch_n * minibatch_size\n",
    "            end_index = min(start_index + minibatch_size, n_samples)\n",
    "            batch_X = X[start_index: end_index, :, :]\n",
    "            batch_y = y[start_index: end_index, :]\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_X) \n",
    "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        # evaluate performance on part of the data (for memory reasons we take a subsample)\n",
    "        if print_results:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                preds = np.argmax(X[:2000, :, :].detach().cpu().numpy(), axis=-1).flatten() # flatten the arrays so accuracy score works\n",
    "                targets = y[:2000].detach().cpu().numpy().flatten()\n",
    "                t_perplexity = average_perplexity(model, train_X[:2000], train_y[:2000])\n",
    "                v_perplexity = average_perplexity(model, valid_X, valid_y)\n",
    "                end_ts = time()\n",
    "                print(\"Epoch {}, Minibatch loss: {:.2f}, Subsample Accuracy: {:.2f}, Train Perplexity: {:.2f}, Validation Perplexity: {:.2f}, Epoch Time: {:.2f} seconds\".format(epoch, loss.item(),\n",
    "                    accuracy_score(targets, preds), t_perplexity, v_perplexity, end_ts - start_ts))\n",
    "    \n",
    "    del X\n",
    "    del y\n",
    "    torch.cuda.empty_cache()\n",
    "    if print_results:\n",
    "        print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d51665cc",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch loss: 5.84, Subsample Accuracy: 0.19, Train Perplexity: 5476.68, Validation Perplexity: 5392.49\n",
      "Epoch 1, Minibatch loss: 5.27, Subsample Accuracy: 0.19, Train Perplexity: 4591.04, Validation Perplexity: 4502.46\n",
      "Epoch 2, Minibatch loss: 5.18, Subsample Accuracy: 0.18, Train Perplexity: 3781.74, Validation Perplexity: 3725.83\n",
      "Epoch 3, Minibatch loss: 5.33, Subsample Accuracy: 0.18, Train Perplexity: 3172.56, Validation Perplexity: 3137.97\n",
      "Epoch 4, Minibatch loss: 5.11, Subsample Accuracy: 0.18, Train Perplexity: 2724.83, Validation Perplexity: 2705.27\n",
      "Epoch 5, Minibatch loss: 5.23, Subsample Accuracy: 0.19, Train Perplexity: 2389.16, Validation Perplexity: 2385.96\n",
      "Epoch 6, Minibatch loss: 5.16, Subsample Accuracy: 0.18, Train Perplexity: 2150.90, Validation Perplexity: 2162.16\n",
      "Epoch 7, Minibatch loss: 4.94, Subsample Accuracy: 0.19, Train Perplexity: 1951.61, Validation Perplexity: 1974.96\n",
      "Epoch 8, Minibatch loss: 4.84, Subsample Accuracy: 0.19, Train Perplexity: 1790.92, Validation Perplexity: 1823.86\n",
      "Epoch 9, Minibatch loss: 4.89, Subsample Accuracy: 0.18, Train Perplexity: 1666.38, Validation Perplexity: 1704.66\n",
      "Epoch 10, Minibatch loss: 4.88, Subsample Accuracy: 0.18, Train Perplexity: 1555.94, Validation Perplexity: 1599.37\n",
      "Epoch 11, Minibatch loss: 4.55, Subsample Accuracy: 0.19, Train Perplexity: 1456.27, Validation Perplexity: 1504.97\n",
      "Epoch 12, Minibatch loss: 4.69, Subsample Accuracy: 0.18, Train Perplexity: 1364.49, Validation Perplexity: 1418.45\n",
      "Epoch 13, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 1257.05, Validation Perplexity: 1313.14\n",
      "Epoch 14, Minibatch loss: 4.76, Subsample Accuracy: 0.19, Train Perplexity: 1187.09, Validation Perplexity: 1243.19\n",
      "Epoch 15, Minibatch loss: 4.81, Subsample Accuracy: 0.18, Train Perplexity: 1119.37, Validation Perplexity: 1178.98\n",
      "Epoch 16, Minibatch loss: 4.90, Subsample Accuracy: 0.18, Train Perplexity: 1057.85, Validation Perplexity: 1119.06\n",
      "Epoch 17, Minibatch loss: 4.98, Subsample Accuracy: 0.18, Train Perplexity: 1001.57, Validation Perplexity: 1064.10\n",
      "Epoch 18, Minibatch loss: 4.79, Subsample Accuracy: 0.18, Train Perplexity: 935.20, Validation Perplexity: 999.06\n",
      "Epoch 19, Minibatch loss: 4.74, Subsample Accuracy: 0.18, Train Perplexity: 876.00, Validation Perplexity: 938.18\n",
      "Epoch 20, Minibatch loss: 4.58, Subsample Accuracy: 0.18, Train Perplexity: 826.86, Validation Perplexity: 890.26\n",
      "Epoch 21, Minibatch loss: 4.57, Subsample Accuracy: 0.18, Train Perplexity: 779.72, Validation Perplexity: 844.10\n",
      "Epoch 22, Minibatch loss: 4.71, Subsample Accuracy: 0.18, Train Perplexity: 748.63, Validation Perplexity: 813.42\n",
      "Epoch 23, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 710.61, Validation Perplexity: 775.64\n",
      "Epoch 24, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 675.82, Validation Perplexity: 741.75\n",
      "Epoch 25, Minibatch loss: 4.67, Subsample Accuracy: 0.19, Train Perplexity: 640.35, Validation Perplexity: 707.65\n",
      "Epoch 26, Minibatch loss: 4.82, Subsample Accuracy: 0.18, Train Perplexity: 609.33, Validation Perplexity: 677.11\n",
      "Epoch 27, Minibatch loss: 4.22, Subsample Accuracy: 0.18, Train Perplexity: 584.62, Validation Perplexity: 650.31\n",
      "Epoch 28, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 555.64, Validation Perplexity: 621.89\n",
      "Epoch 29, Minibatch loss: 4.80, Subsample Accuracy: 0.18, Train Perplexity: 530.58, Validation Perplexity: 596.41\n",
      "Epoch 30, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 510.51, Validation Perplexity: 576.40\n",
      "Epoch 31, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 488.33, Validation Perplexity: 553.29\n",
      "Epoch 32, Minibatch loss: 4.54, Subsample Accuracy: 0.18, Train Perplexity: 464.88, Validation Perplexity: 529.08\n",
      "Epoch 33, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 447.38, Validation Perplexity: 510.87\n",
      "Epoch 34, Minibatch loss: 4.64, Subsample Accuracy: 0.19, Train Perplexity: 432.98, Validation Perplexity: 497.11\n",
      "Epoch 35, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 417.87, Validation Perplexity: 482.30\n",
      "Epoch 36, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 404.87, Validation Perplexity: 468.33\n",
      "Epoch 37, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 393.28, Validation Perplexity: 456.93\n",
      "Epoch 38, Minibatch loss: 4.49, Subsample Accuracy: 0.18, Train Perplexity: 379.51, Validation Perplexity: 443.89\n",
      "Epoch 39, Minibatch loss: 4.75, Subsample Accuracy: 0.19, Train Perplexity: 370.21, Validation Perplexity: 435.40\n",
      "Epoch 40, Minibatch loss: 4.49, Subsample Accuracy: 0.19, Train Perplexity: 355.71, Validation Perplexity: 420.45\n",
      "Epoch 41, Minibatch loss: 4.42, Subsample Accuracy: 0.19, Train Perplexity: 348.89, Validation Perplexity: 412.52\n",
      "Epoch 42, Minibatch loss: 4.47, Subsample Accuracy: 0.19, Train Perplexity: 340.33, Validation Perplexity: 404.41\n",
      "Epoch 43, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 329.51, Validation Perplexity: 393.30\n",
      "Epoch 44, Minibatch loss: 4.38, Subsample Accuracy: 0.19, Train Perplexity: 322.76, Validation Perplexity: 387.37\n",
      "Epoch 45, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 314.91, Validation Perplexity: 378.22\n",
      "Epoch 46, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 308.03, Validation Perplexity: 371.69\n",
      "Epoch 47, Minibatch loss: 4.59, Subsample Accuracy: 0.18, Train Perplexity: 301.45, Validation Perplexity: 364.70\n",
      "Epoch 48, Minibatch loss: 4.52, Subsample Accuracy: 0.18, Train Perplexity: 297.91, Validation Perplexity: 361.49\n",
      "Epoch 49, Minibatch loss: 4.44, Subsample Accuracy: 0.19, Train Perplexity: 293.07, Validation Perplexity: 355.72\n",
      "Epoch 50, Minibatch loss: 4.39, Subsample Accuracy: 0.17, Train Perplexity: 289.04, Validation Perplexity: 352.12\n",
      "Epoch 51, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 284.01, Validation Perplexity: 347.42\n",
      "Epoch 52, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 280.74, Validation Perplexity: 344.42\n",
      "Epoch 53, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 275.46, Validation Perplexity: 339.17\n",
      "Epoch 54, Minibatch loss: 4.37, Subsample Accuracy: 0.17, Train Perplexity: 272.82, Validation Perplexity: 335.91\n",
      "Epoch 55, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 269.64, Validation Perplexity: 333.87\n",
      "Epoch 56, Minibatch loss: 4.68, Subsample Accuracy: 0.18, Train Perplexity: 264.01, Validation Perplexity: 328.75\n",
      "Epoch 57, Minibatch loss: 4.41, Subsample Accuracy: 0.17, Train Perplexity: 263.00, Validation Perplexity: 328.00\n",
      "Epoch 58, Minibatch loss: 4.56, Subsample Accuracy: 0.18, Train Perplexity: 262.02, Validation Perplexity: 327.43\n",
      "Epoch 59, Minibatch loss: 4.21, Subsample Accuracy: 0.19, Train Perplexity: 260.36, Validation Perplexity: 323.63\n",
      "Epoch 60, Minibatch loss: 4.35, Subsample Accuracy: 0.19, Train Perplexity: 259.77, Validation Perplexity: 324.26\n",
      "Epoch 61, Minibatch loss: 4.29, Subsample Accuracy: 0.17, Train Perplexity: 259.03, Validation Perplexity: 323.57\n",
      "Epoch 62, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 257.25, Validation Perplexity: 321.99\n",
      "Epoch 63, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 256.43, Validation Perplexity: 321.51\n",
      "Epoch 64, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 255.57, Validation Perplexity: 321.01\n",
      "Epoch 65, Minibatch loss: 4.33, Subsample Accuracy: 0.18, Train Perplexity: 254.66, Validation Perplexity: 320.84\n",
      "Epoch 66, Minibatch loss: 3.88, Subsample Accuracy: 0.19, Train Perplexity: 253.35, Validation Perplexity: 319.79\n",
      "Epoch 67, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 252.66, Validation Perplexity: 319.94\n",
      "Epoch 68, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 251.55, Validation Perplexity: 319.23\n",
      "Epoch 69, Minibatch loss: 4.06, Subsample Accuracy: 0.19, Train Perplexity: 249.63, Validation Perplexity: 317.39\n",
      "Epoch 70, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 248.86, Validation Perplexity: 315.35\n",
      "Epoch 71, Minibatch loss: 4.00, Subsample Accuracy: 0.19, Train Perplexity: 248.79, Validation Perplexity: 317.27\n",
      "Epoch 72, Minibatch loss: 3.91, Subsample Accuracy: 0.19, Train Perplexity: 246.19, Validation Perplexity: 314.84\n",
      "Epoch 73, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 247.41, Validation Perplexity: 315.28\n",
      "Epoch 74, Minibatch loss: 4.45, Subsample Accuracy: 0.18, Train Perplexity: 247.14, Validation Perplexity: 315.10\n",
      "Epoch 75, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 245.09, Validation Perplexity: 313.25\n",
      "Epoch 76, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 246.37, Validation Perplexity: 314.49\n",
      "Epoch 77, Minibatch loss: 3.89, Subsample Accuracy: 0.19, Train Perplexity: 246.49, Validation Perplexity: 315.38\n",
      "Epoch 78, Minibatch loss: 4.14, Subsample Accuracy: 0.19, Train Perplexity: 246.09, Validation Perplexity: 315.03\n",
      "Epoch 79, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 245.91, Validation Perplexity: 315.10\n",
      "Epoch 80, Minibatch loss: 4.10, Subsample Accuracy: 0.18, Train Perplexity: 247.93, Validation Perplexity: 317.32\n",
      "Epoch 81, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 244.34, Validation Perplexity: 313.44\n",
      "Epoch 82, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 245.80, Validation Perplexity: 314.77\n",
      "Epoch 83, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 244.70, Validation Perplexity: 313.63\n",
      "Epoch 84, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 243.42, Validation Perplexity: 312.25\n",
      "Epoch 85, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 244.68, Validation Perplexity: 313.40\n",
      "Epoch 86, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 242.68, Validation Perplexity: 311.52\n",
      "Epoch 87, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 243.07, Validation Perplexity: 312.91\n",
      "Epoch 88, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 242.19, Validation Perplexity: 312.40\n",
      "Epoch 89, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 242.87, Validation Perplexity: 312.90\n",
      "Epoch 90, Minibatch loss: 4.42, Subsample Accuracy: 0.18, Train Perplexity: 240.35, Validation Perplexity: 309.62\n",
      "Epoch 91, Minibatch loss: 4.37, Subsample Accuracy: 0.19, Train Perplexity: 240.76, Validation Perplexity: 309.31\n",
      "Epoch 92, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 241.14, Validation Perplexity: 311.39\n",
      "Epoch 93, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 241.42, Validation Perplexity: 310.48\n",
      "Epoch 94, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 239.01, Validation Perplexity: 309.35\n",
      "Epoch 95, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.80, Validation Perplexity: 308.66\n",
      "Epoch 96, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 238.24, Validation Perplexity: 308.37\n",
      "Epoch 97, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 238.68, Validation Perplexity: 308.45\n",
      "Epoch 98, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 238.69, Validation Perplexity: 308.85\n",
      "Epoch 99, Minibatch loss: 4.46, Subsample Accuracy: 0.18, Train Perplexity: 237.43, Validation Perplexity: 307.66\n",
      "Epoch 100, Minibatch loss: 4.39, Subsample Accuracy: 0.18, Train Perplexity: 237.74, Validation Perplexity: 308.04\n",
      "Epoch 101, Minibatch loss: 4.26, Subsample Accuracy: 0.18, Train Perplexity: 238.30, Validation Perplexity: 308.48\n",
      "Epoch 102, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 236.60, Validation Perplexity: 306.11\n",
      "Epoch 103, Minibatch loss: 4.27, Subsample Accuracy: 0.17, Train Perplexity: 235.80, Validation Perplexity: 305.84\n",
      "Epoch 104, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 235.29, Validation Perplexity: 306.47\n",
      "Epoch 105, Minibatch loss: 4.23, Subsample Accuracy: 0.18, Train Perplexity: 237.75, Validation Perplexity: 308.24\n",
      "Epoch 106, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 237.73, Validation Perplexity: 308.38\n",
      "Epoch 107, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 237.20, Validation Perplexity: 308.49\n",
      "Epoch 108, Minibatch loss: 4.39, Subsample Accuracy: 0.19, Train Perplexity: 236.51, Validation Perplexity: 307.32\n",
      "Epoch 109, Minibatch loss: 4.12, Subsample Accuracy: 0.18, Train Perplexity: 235.98, Validation Perplexity: 307.06\n",
      "Epoch 110, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 236.37, Validation Perplexity: 306.64\n",
      "Epoch 111, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 234.71, Validation Perplexity: 304.71\n",
      "Epoch 112, Minibatch loss: 4.31, Subsample Accuracy: 0.19, Train Perplexity: 233.82, Validation Perplexity: 304.40\n",
      "Epoch 113, Minibatch loss: 4.27, Subsample Accuracy: 0.18, Train Perplexity: 236.07, Validation Perplexity: 306.60\n",
      "Epoch 114, Minibatch loss: 4.40, Subsample Accuracy: 0.18, Train Perplexity: 234.30, Validation Perplexity: 306.02\n",
      "Epoch 115, Minibatch loss: 4.24, Subsample Accuracy: 0.18, Train Perplexity: 234.13, Validation Perplexity: 304.99\n",
      "Epoch 116, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 234.27, Validation Perplexity: 304.26\n",
      "Epoch 117, Minibatch loss: 4.26, Subsample Accuracy: 0.19, Train Perplexity: 234.63, Validation Perplexity: 305.40\n",
      "Epoch 118, Minibatch loss: 4.28, Subsample Accuracy: 0.18, Train Perplexity: 234.81, Validation Perplexity: 305.67\n",
      "Epoch 119, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 235.66, Validation Perplexity: 306.63\n",
      "Epoch 120, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 235.57, Validation Perplexity: 305.68\n",
      "Epoch 121, Minibatch loss: 4.15, Subsample Accuracy: 0.17, Train Perplexity: 232.76, Validation Perplexity: 303.14\n",
      "Epoch 122, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 232.50, Validation Perplexity: 303.04\n",
      "Epoch 123, Minibatch loss: 4.23, Subsample Accuracy: 0.20, Train Perplexity: 231.39, Validation Perplexity: 301.93\n",
      "Epoch 124, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 232.08, Validation Perplexity: 302.36\n",
      "Epoch 125, Minibatch loss: 4.28, Subsample Accuracy: 0.20, Train Perplexity: 231.29, Validation Perplexity: 300.98\n",
      "Epoch 126, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 229.45, Validation Perplexity: 300.27\n",
      "Epoch 127, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 228.29, Validation Perplexity: 298.52\n",
      "Epoch 128, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 228.74, Validation Perplexity: 299.92\n",
      "Epoch 129, Minibatch loss: 4.48, Subsample Accuracy: 0.19, Train Perplexity: 228.19, Validation Perplexity: 298.40\n",
      "Epoch 130, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 228.62, Validation Perplexity: 298.51\n",
      "Epoch 131, Minibatch loss: 4.32, Subsample Accuracy: 0.17, Train Perplexity: 229.89, Validation Perplexity: 299.81\n",
      "Epoch 132, Minibatch loss: 4.40, Subsample Accuracy: 0.19, Train Perplexity: 229.14, Validation Perplexity: 300.06\n",
      "Epoch 133, Minibatch loss: 4.35, Subsample Accuracy: 0.17, Train Perplexity: 229.39, Validation Perplexity: 299.79\n",
      "Epoch 134, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 231.70, Validation Perplexity: 301.74\n",
      "Epoch 135, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 230.36, Validation Perplexity: 301.70\n",
      "Epoch 136, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.40, Validation Perplexity: 300.80\n",
      "Epoch 137, Minibatch loss: 4.34, Subsample Accuracy: 0.18, Train Perplexity: 229.98, Validation Perplexity: 300.24\n",
      "Epoch 138, Minibatch loss: 4.22, Subsample Accuracy: 0.19, Train Perplexity: 229.88, Validation Perplexity: 301.13\n",
      "Epoch 139, Minibatch loss: 4.46, Subsample Accuracy: 0.19, Train Perplexity: 228.50, Validation Perplexity: 299.10\n",
      "Epoch 140, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.70, Validation Perplexity: 297.74\n",
      "Epoch 141, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 228.97, Validation Perplexity: 300.98\n",
      "Epoch 142, Minibatch loss: 4.31, Subsample Accuracy: 0.18, Train Perplexity: 228.31, Validation Perplexity: 298.41\n",
      "Epoch 143, Minibatch loss: 4.09, Subsample Accuracy: 0.18, Train Perplexity: 228.53, Validation Perplexity: 299.20\n",
      "Epoch 144, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 226.93, Validation Perplexity: 298.40\n",
      "Epoch 145, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 226.72, Validation Perplexity: 297.92\n",
      "Epoch 146, Minibatch loss: 4.38, Subsample Accuracy: 0.17, Train Perplexity: 225.46, Validation Perplexity: 295.54\n",
      "Epoch 147, Minibatch loss: 4.07, Subsample Accuracy: 0.19, Train Perplexity: 228.22, Validation Perplexity: 299.47\n",
      "Epoch 148, Minibatch loss: 4.29, Subsample Accuracy: 0.18, Train Perplexity: 225.41, Validation Perplexity: 296.80\n",
      "Epoch 149, Minibatch loss: 4.41, Subsample Accuracy: 0.18, Train Perplexity: 226.84, Validation Perplexity: 297.85\n",
      "Epoch 150, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 227.05, Validation Perplexity: 299.09\n",
      "Epoch 151, Minibatch loss: 4.17, Subsample Accuracy: 0.18, Train Perplexity: 226.02, Validation Perplexity: 298.17\n",
      "Epoch 152, Minibatch loss: 4.16, Subsample Accuracy: 0.17, Train Perplexity: 226.67, Validation Perplexity: 298.10\n",
      "Epoch 153, Minibatch loss: 4.19, Subsample Accuracy: 0.19, Train Perplexity: 227.75, Validation Perplexity: 299.15\n",
      "Epoch 154, Minibatch loss: 4.19, Subsample Accuracy: 0.18, Train Perplexity: 227.36, Validation Perplexity: 298.73\n",
      "Epoch 155, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 225.13, Validation Perplexity: 296.02\n",
      "Epoch 156, Minibatch loss: 4.23, Subsample Accuracy: 0.17, Train Perplexity: 226.37, Validation Perplexity: 297.95\n",
      "Epoch 157, Minibatch loss: 4.44, Subsample Accuracy: 0.17, Train Perplexity: 225.94, Validation Perplexity: 296.60\n",
      "Epoch 158, Minibatch loss: 4.31, Subsample Accuracy: 0.17, Train Perplexity: 225.86, Validation Perplexity: 296.96\n",
      "Epoch 159, Minibatch loss: 4.37, Subsample Accuracy: 0.18, Train Perplexity: 224.68, Validation Perplexity: 295.32\n",
      "Epoch 160, Minibatch loss: 4.30, Subsample Accuracy: 0.18, Train Perplexity: 224.34, Validation Perplexity: 295.34\n",
      "Epoch 161, Minibatch loss: 4.60, Subsample Accuracy: 0.19, Train Perplexity: 222.58, Validation Perplexity: 293.00\n",
      "Epoch 162, Minibatch loss: 3.97, Subsample Accuracy: 0.18, Train Perplexity: 222.39, Validation Perplexity: 292.02\n",
      "Epoch 163, Minibatch loss: 4.29, Subsample Accuracy: 0.19, Train Perplexity: 223.47, Validation Perplexity: 293.71\n",
      "Epoch 164, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 225.33, Validation Perplexity: 295.93\n",
      "Epoch 165, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.07, Validation Perplexity: 293.74\n",
      "Epoch 166, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 223.23, Validation Perplexity: 293.61\n",
      "Epoch 167, Minibatch loss: 4.25, Subsample Accuracy: 0.18, Train Perplexity: 225.39, Validation Perplexity: 296.71\n",
      "Epoch 168, Minibatch loss: 4.21, Subsample Accuracy: 0.18, Train Perplexity: 224.81, Validation Perplexity: 296.14\n",
      "Epoch 169, Minibatch loss: 4.20, Subsample Accuracy: 0.18, Train Perplexity: 226.08, Validation Perplexity: 296.97\n",
      "Epoch 170, Minibatch loss: 4.24, Subsample Accuracy: 0.17, Train Perplexity: 224.61, Validation Perplexity: 295.71\n",
      "Epoch 171, Minibatch loss: 4.10, Subsample Accuracy: 0.19, Train Perplexity: 224.76, Validation Perplexity: 296.56\n",
      "Epoch 172, Minibatch loss: 4.15, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 295.98\n",
      "Epoch 173, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 224.21, Validation Perplexity: 296.21\n",
      "Epoch 174, Minibatch loss: 4.17, Subsample Accuracy: 0.19, Train Perplexity: 224.11, Validation Perplexity: 296.44\n",
      "Epoch 175, Minibatch loss: 4.02, Subsample Accuracy: 0.18, Train Perplexity: 223.87, Validation Perplexity: 295.96\n",
      "Epoch 176, Minibatch loss: 4.43, Subsample Accuracy: 0.18, Train Perplexity: 224.22, Validation Perplexity: 296.70\n",
      "Epoch 177, Minibatch loss: 4.45, Subsample Accuracy: 0.19, Train Perplexity: 222.85, Validation Perplexity: 294.78\n",
      "Epoch 178, Minibatch loss: 4.18, Subsample Accuracy: 0.18, Train Perplexity: 223.25, Validation Perplexity: 294.85\n",
      "Epoch 179, Minibatch loss: 4.50, Subsample Accuracy: 0.19, Train Perplexity: 222.41, Validation Perplexity: 293.88\n",
      "Epoch 180, Minibatch loss: 3.95, Subsample Accuracy: 0.19, Train Perplexity: 222.83, Validation Perplexity: 294.24\n",
      "Epoch 181, Minibatch loss: 4.01, Subsample Accuracy: 0.18, Train Perplexity: 223.08, Validation Perplexity: 294.20\n",
      "Epoch 182, Minibatch loss: 4.13, Subsample Accuracy: 0.18, Train Perplexity: 223.52, Validation Perplexity: 295.34\n",
      "Epoch 183, Minibatch loss: 4.08, Subsample Accuracy: 0.18, Train Perplexity: 220.96, Validation Perplexity: 292.61\n",
      "Epoch 184, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 223.21, Validation Perplexity: 294.52\n",
      "Epoch 185, Minibatch loss: 4.24, Subsample Accuracy: 0.19, Train Perplexity: 222.59, Validation Perplexity: 293.70\n",
      "Epoch 186, Minibatch loss: 4.33, Subsample Accuracy: 0.19, Train Perplexity: 223.89, Validation Perplexity: 295.60\n",
      "Epoch 187, Minibatch loss: 4.35, Subsample Accuracy: 0.18, Train Perplexity: 224.00, Validation Perplexity: 295.06\n",
      "Epoch 188, Minibatch loss: 4.25, Subsample Accuracy: 0.19, Train Perplexity: 222.02, Validation Perplexity: 294.40\n",
      "Epoch 189, Minibatch loss: 3.95, Subsample Accuracy: 0.18, Train Perplexity: 221.55, Validation Perplexity: 293.86\n",
      "Epoch 190, Minibatch loss: 4.52, Subsample Accuracy: 0.19, Train Perplexity: 222.05, Validation Perplexity: 292.84\n",
      "Epoch 191, Minibatch loss: 4.07, Subsample Accuracy: 0.18, Train Perplexity: 223.65, Validation Perplexity: 294.15\n",
      "Epoch 192, Minibatch loss: 4.20, Subsample Accuracy: 0.19, Train Perplexity: 221.89, Validation Perplexity: 292.28\n",
      "Epoch 193, Minibatch loss: 4.56, Subsample Accuracy: 0.19, Train Perplexity: 221.92, Validation Perplexity: 293.61\n",
      "Epoch 194, Minibatch loss: 4.32, Subsample Accuracy: 0.18, Train Perplexity: 221.51, Validation Perplexity: 292.25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-619d9a32a5fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mminibatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-43f7dfafae68>\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(model, train_X, train_y, epochs, learn_rate, weight_decay, minibatch_size, print_results)\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    244\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 245\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    246\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\torch\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[0;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = train_model(model, train_X, train_y, epochs=200, learn_rate=0.001, minibatch_size=256, weight_decay=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96a5e98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"LSTMBaseline.model\") # save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2fff3960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load model, only run if the model hasn't already been trained\n",
    "model.load_state_dict(torch.load(\"LSTMBaseline.model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ab2088d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2e2dd",
   "metadata": {},
   "source": [
    "### Hyper-Parameter tuning findings\n",
    "- glove 300 dimension vectors are essential to not have a bias of 1000 perplexity on both train and validation\n",
    "- 2 layers of LSTM also gives high bias, perhaps there is not enough train data\n",
    "- Weight decay is essential in preventing Validation perplexity from skyrocketing\n",
    "- Dropout of 0.1 combined with weight decay 0.00001 works (around 250 validation perplexity)\n",
    "- Decreasing learning rate and increasing epochs has a minor benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db063f69",
   "metadata": {},
   "source": [
    "### Examine Performance of the model\n",
    "- Using both perplexity and qualitative evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "228f648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view overall performance\n",
    "model.eval()\n",
    "data = numpy_to_tensor(train_X[:1000])\n",
    "preds = model.predict(data)\n",
    "#preds = torch.nn.Softmax(dim=-1)(preds).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "3ad8a2d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['probably',\n",
       " 'help',\n",
       " 'begin',\n",
       " 'also',\n",
       " 'take',\n",
       " 'make',\n",
       " 'continue',\n",
       " 'the',\n",
       " 'have',\n",
       " 'be']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(map(lambda x: index_vocab[x], np.argsort(preds[1, 11, :])[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f38177fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.1733707e-07, 5.4392586e-07, 5.5573935e-07, ..., 1.3860598e-02,\n",
       "       3.9391726e-02, 7.3702824e-01], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(preds[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d20a349f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: ['<PAD>', '<PAD>', 'securities', \"'s\", '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '.', '.', '<PAD>', '<PAD>', '<PAD>', 'the', '<PAD>', '<PAD>', '<PAD>', \"'s\", '<PAD>']\n",
      "Input:     ['<PAD>', 'aer', 'banknote', 'berlitz', 'calloway', 'cluett', 'fromstein', 'gitano', 'guterman', 'ipo', 'kia', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'ssangyong', 'swapo', 'wachter']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'the', '.', '.', 'be', 'the', 'company', \"'s\", 'a', 'new', 'director', 'of', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'pierre', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov.', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'said', 'a', 'of', 'the', 'inc.', 'company', \"'s\", 'group', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'mr.', 'is', 'chairman', 'of', 'n.v.', 'the', 'dutch', 'publishing', 'group', '.']\n",
      "\n",
      "Predicted: ['<PAD>', \"'s\", 'ago', 'was', 'the', 'president', 'of', 'the', 'inc.', 'and', 'in', 'a', 'a', 'a', 'share', 'director', 'of', 'the', 'year', 'government']\n",
      "Input:     ['<PAD>', 'rudolph', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial']\n",
      "\n",
      "Predicted: ['<PAD>', 'new', 'of', 'the', 'and', 'the', 'to', 'the', 'the', 'the', 'and', 'to', 'been', 'by', 'share', 'court', 'of', 'the', 'institute', '.']\n",
      "Input:     ['<PAD>', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among']\n",
      "\n",
      "Predicted: ['<PAD>', 'company', 'and', 'and', 'a', 'high', 'the', 'is', 'the', 'company', 'the', 'if', 'and', 'and', 'the', \"'s\", 'the', '.', 'the', '.']\n",
      "Input:     ['<PAD>', 'the', 'asbestos', 'fiber', 'is', 'unusually', 'once', 'it', 'enters', 'the', 'with', 'even', 'brief', 'exposures', 'to', 'it', 'causing', 'symptoms', 'that', 'show', 'up']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', 'said', 'company', 'of', 'the', 'york', 'company', 'said', 'the', 'the', \"'s\", 'and', 'the', 'the', 'the', 'own', 'and', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', 'inc.', 'the', 'unit', 'of', 'new', 'york-based', 'corp.', 'that', 'makes', 'kent', 'cigarettes', 'stopped', 'using', 'in', 'its', 'cigarette', 'filters', 'in', '.']\n",
      "\n",
      "Predicted: ['<PAD>', 'the', 'agreement', 'that', \"n't\", 'a', 'than', 'the', 'new', 'earlier', 'the', 'company', 'quarter', 'of', 'to', 'the', \"'s\", '.', 'york', '.']\n",
      "Input:     ['<PAD>', 'although', 'preliminary', 'findings', 'were', 'reported', 'more', 'than', 'a', 'year', 'ago', 'the', 'latest', 'results', 'appear', 'in', 'today', \"'s\", 'new', 'england', 'journal']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'share', 'the', 'year', 'a', 'ounce', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'a', 'said', 'this', 'is', 'an', 'old', 'story', '.']\n",
      "\n",
      "Predicted: ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', \"'re\", 'going', 'about', 'the', 'ago', 'the', 'the', 'who', 'in', 'the', 'and', 'a', 'of', '.', '.']\n",
      "Input:     ['<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', 'we', \"'re\", 'talking', 'about', 'years', 'ago', 'before', 'anyone', 'heard', 'of', 'asbestos', 'having', 'any', 'questionable', 'properties', '.']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds_int = np.argmax(preds, axis=-1)\n",
    "for i in range(10):\n",
    "    sentence = list(map(index_vocab.get, preds_int[i]))\n",
    "    truth = list(map(lambda x: index_vocab[int(x)], train_y[i]))\n",
    "    input_sentence = ['<PAD>'] + truth\n",
    "    print(\"Predicted:\", sentence)\n",
    "    print(\"Input:    \",input_sentence)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "75bd595a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' aer banknote berlitz calloway centrust cluett fromstein gitano guterman hydro-quebec ipo kia memotec mlx nahb punts rake regatta rubens sim snack-food ssangyong swapo wachter \\n',\n",
       " ' pierre <unk> N years old will join the board as a nonexecutive director nov. N \\n',\n",
       " ' mr. <unk> is chairman of <unk> n.v. the dutch publishing group \\n',\n",
       " ' rudolph <unk> N years old and former chairman of consolidated gold fields plc was named a nonexecutive director of this british industrial conglomerate \\n',\n",
       " ' a form of asbestos once used to make kent cigarette filters has caused a high percentage of cancer deaths among a group of workers exposed to it more than N years ago researchers reported \\n',\n",
       " ' the asbestos fiber <unk> is unusually <unk> once it enters the <unk> with even brief exposures to it causing symptoms that show up decades later researchers said \\n',\n",
       " ' <unk> inc. the unit of new york-based <unk> corp. that makes kent cigarettes stopped using <unk> in its <unk> cigarette filters in N \\n',\n",
       " \" although preliminary findings were reported more than a year ago the latest results appear in today 's new england journal of medicine a forum likely to bring new attention to the problem \\n\",\n",
       " ' a <unk> <unk> said this is an old story \\n',\n",
       " \" we 're talking about years ago before anyone heard of asbestos having any questionable properties \\n\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "71ab8782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80.610664"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred1 = model.predict(numpy_to_tensor(valid_X[0:1]))\n",
    "perplexity(pred1, valid_y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "92debfa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.79262"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b8efd14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "245.91684"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity(model, valid_X, valid_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb4b94e-9fb6-4b4d-8a66-1b87cd9f41ec",
   "metadata": {},
   "source": [
    "# GPT2 Baseline\n",
    "Implement GPT2 as a language modelling baseline. GPT-3 is not publicly available and too large for practical purposes. BERT needs modification to work for language modelling, due to the fact that it is trained for bidirectional masked language modelling instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a61aafc-e915-4ff8-b371-3fc32331fd83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load　GPT, BERT and support materials from huggingface\n",
    "# requires pip install transformers\n",
    "# if in jupyter notebook see here and you get an error mention ipython widgets see here: \n",
    "# https://stackoverflow.com/questions/53247985/tqdm-4-28-1-in-jupyter-notebook-intprogress-not-found-please-update-jupyter-an\n",
    "from transformers import BertTokenizer, BertModel, GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d485f60-6997-409f-80f2-f4a7a6a6987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test'))\n",
    "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
    "valid = list(valid)\n",
    "test  = list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42140794-4ad1-4999-b6ab-1bd13ba648d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the models\n",
    "#bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "#bert_model = BertModel.from_pretrained('bert-base-uncased', return_dict=True, is_decoder = True) # TODO: Check if bert model give us the [CLS] output?\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa755a4d-7ad0-4e3c-b00c-669513e9c930",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hugging Face is based in DUMBO, New York City, and features art by the likes of The Simpsons' John F. Kennedy, Jumanji's Doki\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GPT2 example generation\n",
    "text = f\"Hugging Face is based in DUMBO, New York City, and\"\n",
    "tokens_tensor = gpt_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "\n",
    "# Put everything on cuda\n",
    "gpt_model.eval()\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "gpt_model.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "generated = tokens_tensor\n",
    "for i in range(20):\n",
    "    next_token_logits = gpt_model(generated).logits[:, -1, :]\n",
    "    # filter\n",
    "    filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0)\n",
    "    # sample\n",
    "    probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "    next_token = torch.multinomial(probs, num_samples=1)\n",
    "    generated = torch.cat([generated, next_token], dim=-1)\n",
    "\n",
    "resulting_string = gpt_tokenizer.decode(generated.tolist()[0])\n",
    "resulting_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b462ea-4c8d-41f2-9076-35f2180744b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Who was Jim Henson? Jim Henson was a man'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Their example is broken, can be fixed by requesting the logits output\n",
    "text = \"Who was Jim Henson? Jim Henson was a\"\n",
    "tokens_tensor = gpt_tokenizer.encode(text, return_tensors=\"pt\")\n",
    "tokens_tensor = tokens_tensor.to('cuda')\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = gpt_model(tokens_tensor).logits[:, -1, :]\n",
    "\n",
    "# get the predicted next sub-word (in our case, the word 'man')\n",
    "predicted_index = torch.argmax(predictions).item()\n",
    "predicted_text = text + gpt_tokenizer.decode([predicted_index])\n",
    "predicted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6795d64b-f959-450d-b9d2-2dce113b96a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' rud', 'olph', ' <', 'unk', '>', ' N', ' years', ' old', ' and', ' former', ' chairman', ' of', ' consolidated', ' gold', ' fields', ' pl', 'c', ' was', ' named', ' a', ' nonex', 'ec', 'utive', ' director', ' of', ' this', ' b', 'rit', 'ish', ' industrial', ' conglomerate', ' ', '\\n']\n"
     ]
    }
   ],
   "source": [
    "tokens = gpt_tokenizer.encode(train[3])\n",
    "print([gpt_tokenizer.decode([x]) for x in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a79ac-e65d-4082-bc08-22cfeee91848",
   "metadata": {},
   "source": [
    "We can see that the gpt_tokenizer works differently to ours, splitting up names such as 'rudolph' into 'rud' and 'olph' and words such as nonexecutive and british. Hence our perplexity evaluation will have to be slightly different, using gpt_tokenizer to get the ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3923d864-8141-4525-a915-930e03c08ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model wrapper for gpt2 that uses the \"past\" variable and for language modelling\n",
    "# TODO: Fine tuning - need to add padding for backprop in torch to work (just add '.' at the end for padding)\n",
    "# TODO: add the options for beam search\n",
    "class GPTModel(torch.nn.Module):\n",
    "    def __init__(self, sequence_length=20):\n",
    "        super().__init__()\n",
    "        self.gpt = gpt_model.to('cuda')\n",
    "        self.tokenizer = gpt_tokenizer\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = self.tokenizer.vocab_size\n",
    "    \n",
    "    # output the logits for the most likely next word at each position in the sentence\n",
    "    # note x must be a raw sentence and this only works for training as it assumes x contains all the tokens in the sentence\n",
    "    def forward(self, x):\n",
    "        # initialize variables\n",
    "        if type(x) == str or type(x) == np.str_:\n",
    "            x = [x] # now we can use batch method for the entire function \n",
    "        sentences = self.tokenizer.batch_encode_plus(x)['input_ids']\n",
    "        batch_size = len(sentences)\n",
    "        preds = torch.zeros((batch_size, self.sequence_length, self.vocab_size), device='cuda')\n",
    "        past = None\n",
    "\n",
    "        # for each sentence: predict next word for every token, `past` remembers results for previous tokens so `past`=`past` saves us computation\n",
    "        for i in range(batch_size):\n",
    "            for j in range(min(len(sentences[i]), self.sequence_length)):\n",
    "                context = torch.tensor([[sentences[i][j]]]).to('cuda')\n",
    "                raw_output = self.gpt(context, past_key_values=past)\n",
    "                output, past = raw_output['logits'], raw_output['past_key_values']\n",
    "                preds[i, j, :] = output[:, -1, :]\n",
    "        return preds\n",
    "    \n",
    "    # take in a sentence and output the predictions as in forward, but as the most likely sentence not logits\n",
    "    def forward_sentence(self, x):\n",
    "        preds = self.forward(x)\n",
    "        tokens = torch.argmax(preds, dim=-1)\n",
    "        return self.tokenizer.batch_decode(tokens)\n",
    "    \n",
    "    # sample the next word from the probability distribution\n",
    "    # set limit to an integer to generate `limit` words instead of ending at a full stop\n",
    "    def random_gen(self, x, limit=None):\n",
    "        # initialize variables\n",
    "        generated = self.tokenizer.encode_plus(x, return_tensors=\"pt\")['input_ids'].to('cuda')\n",
    "        next_token = [generated[0][-1]]\n",
    "        past = None\n",
    "        raw_output= None\n",
    "        \n",
    "        # generate until a \".\" is generated\n",
    "        while (limit is None and self.tokenizer.decode(next_token[0]) not in [\".\", \"?\", \"!\"]) or (limit is not None and len(generated[0]) < limit):\n",
    "            # get output of model, using past if available\n",
    "            if past is None:\n",
    "                raw_output = self.gpt(generated, past_key_values=past)\n",
    "            else:\n",
    "                raw_output = self.gpt(next_token, past_key_values=past)\n",
    "            output, past = raw_output['logits'], raw_output['past_key_values']\n",
    "            next_token_logits = output[:, -1, :]\n",
    "            \n",
    "            # sample a token from the top 50 most likely words\n",
    "            filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0) # filter to the top 50 tokens\n",
    "            probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "            generated = torch.cat([generated, next_token], dim=-1)\n",
    "            \n",
    "        return self.tokenizer.decode(generated[0])\n",
    "    \n",
    "    # do beam_search to find the most likely sentence\n",
    "    def beam_search(self, x, beam=5): \n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54c8b0b-40c7-479c-8676-151172ff6acd",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a88b3953-f775-4979-940e-7733c5240072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -28.0444,  -27.6692,  -30.8974,  ...,  -35.3630,  -34.5905,\n",
       "           -28.4036],\n",
       "         [ -90.0433,  -90.2299,  -94.4137,  ...,  -98.9912,  -97.1018,\n",
       "           -91.1713],\n",
       "         [ -95.0371,  -95.0307,  -99.2749,  ..., -103.6046, -104.1607,\n",
       "           -95.8909],\n",
       "         ...,\n",
       "         [ -75.0353,  -75.2578,  -76.3793,  ...,  -81.3778,  -81.5146,\n",
       "           -73.5530],\n",
       "         [ -65.5799,  -66.1714,  -66.9965,  ...,  -71.7699,  -71.6774,\n",
       "           -63.9302],\n",
       "         [ -75.3228,  -75.8465,  -76.2187,  ...,  -81.6134,  -80.9064,\n",
       "           -74.0212]]], device='cuda:0', grad_fn=<CopySlices>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_sequence_length = 40 # gpt splits up words into smaller tokens, so the sequence length should be longer\n",
    "modelgpt = GPTModel(sequence_length=gpt_sequence_length)\n",
    "modelgpt.eval()\n",
    "modelgpt.forward(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b038206b-229e-48b7-b734-c07d72126934",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 40, 50257])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelgpt.forward(\"it follows the same pattern as his tax returns\").shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1906f7cc-4a5f-4ed1-b3dd-3b18c26a19c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\" the results from not in than a year ago, study data were to the'sThe Journal issuera.. the.. for to draw together insights to the topic.\\xa0\\n!!!!\",\n",
       " ' newanow\\nunk> <: morning a important one.\\n\\n!!!!!!!!!!!!!!!!!!!!!!!!!',\n",
       " ' arell not about here ago\\n the even of it in been effect health.\\n\\n!!!!!!!!!!!!!!!!!!!!!!',\n",
       " ' is no evidence in the country and\\n\\n\\n!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 7\n",
    "modelgpt.forward_sentence(train[i:i+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5d5fd0e6-ea4c-428d-900d-87e68583c3b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Did you ever hear the tragedy of darth plaguies the wise? Darth plaguies was a great king and a great strategist. His victories during the First Battle of Yavin gave him the trust of his people. The first Emperor of the Republic, he took a huge share of the empire, and now sits at the head of the Empire. The First Emperor died at his wedding to his beautiful wife, Darth Traya, at a celebration of the birth of her son. He took her, but before he had the chance to return, the Emperor was shot, killed and taken prisoner. The Emperor is still said to be the best ruler in history. A great power, the Emperor was able to maintain the empire. It did not come at the head of a great empire, but because he saw the value in the Emperor's ability.\\n\\nOne day when his son arrived, the Emperor and his wife were in his honor at their king wedding. He asked some people to let him\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    result = modelgpt.random_gen(\"Did you ever hear the tragedy of darth plaguies the wise? Darth plaguies was a\", limit=200)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66430c0-a4d8-4549-9861-7684003c3a78",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2996c849-0809-4fbe-8f16-211cdbbb1a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perplexity(preds, ground_truth, epsilon=1e-30):\n",
    "    probs = []\n",
    "    for i in range(preds.shape[1]):\n",
    "        probs.append(preds[0, i, int(ground_truth[i])])\n",
    "    probs = np.array(probs)\n",
    "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
    "    return np.prod(probs).detach().cpu().numpy()\n",
    "\n",
    "def average_perplexity_gpt(model, train, print_results=False):\n",
    "    perplexities = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(train)):\n",
    "            # Get new ground truth labels using gpt_tokenizer\n",
    "            labels = gpt_tokenizer.encode(train[i])[1:gpt_sequence_length+1]\n",
    "            preds = model.forward(train[i])[:, :len(labels), :] # remove the last prediction as there is no ground truth \n",
    "            preds = torch.nn.functional.softmax(preds, dim=-1)\n",
    "            perplexities.append(perplexity(preds, labels))\n",
    "\n",
    "            if i % 100 == 0 and print_results:\n",
    "                print(\"Sentences analysed: {} Average perplexity: {}\".format(i, np.mean(perplexities)))\n",
    "    return np.mean(perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f850a00-c2d8-4efd-827d-e026b698c38b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences analysed: 0 Average perplexity: 1562.5426025390625\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "365.88016"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_perplexity_gpt(modelgpt, train[:50], print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f1ea2b-d89b-4638-bcb2-cf63e7d71211",
   "metadata": {},
   "source": [
    "### Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e393a801-6e05-4348-b2cb-afa2d1bb3eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform fine tuning, using stochastic gradient descent to prevent OOM error\n",
    "# to compensate for this we use a smaller learning rate and large beta parameter (for adam optimization) to counter that\n",
    "def finetune_gpt(model, train, epochs=10, learn_rate=0.00001, betas=(0.99, 0.9995), weight_decay=0.000001, print_results=True):\n",
    "    # Ensure this runs on gpu\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Prepare data: get labels\n",
    "    train = model.tokenizer.batch_encode_plus(train)['input_ids']\n",
    "    labels = [s[1:] for s in train]\n",
    "    train = [model.tokenizer.decode(train[i][:-1]) for i in range(len(train))] # the model expects raw string input so decode it again\n",
    "    X = np.array(train)\n",
    "    y = np.array(labels)\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Define loss and optimizer\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, betas=betas, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        model.train() # set to train flag\n",
    "        start_ts = time()\n",
    "        \n",
    "        # shuffle the data\n",
    "        new_indices = torch.randperm(n_samples)\n",
    "        X = X[new_indices]\n",
    "        y = y[new_indices]\n",
    "        \n",
    "        # Stochastic gradient descent\n",
    "        for index in range(n_samples):\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(X[index]) \n",
    "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
    "            \n",
    "            # Perform optimization step\n",
    "            loss = criterion(outputs, labels_i)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # show progress by printing a # every 5% of training data completed\n",
    "            if print_results and index % int(n_samples/20) == 0 and index != 0:\n",
    "                print('#', end='')\n",
    "            \n",
    "        # evaluate performance on part of the data (for memory reasons we take a subsample)\n",
    "        if print_results:\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                t_perplexity = average_perplexity_gpt(model, train[:100], print_results=False)\n",
    "                v_perplexity = average_perplexity_gpt(model, valid[:100], print_results=False)\n",
    "                end_ts = time()\n",
    "                print(\"Epoch {}, Train Perplexity: {:.2f}, Validation Perplexity: {:.2f}, Epoch Time: {:.2f} seconds\".format(\n",
    "                    epoch, t_perplexity, v_perplexity, end_ts - start_ts))\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    if print_results:\n",
    "        print('Finished Training')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec0a16c1-c329-46b8-a792-cdbb1df0fd1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-32-3d2464c74b99>:13: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  y = np.array(labels)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-659358373716>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodelgpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfinetune_gpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodelgpt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.00001\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[0mbetas\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.9995\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.000001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_results\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-3d2464c74b99>\u001b[0m in \u001b[0;36mfinetune_gpt\u001b[1;34m(model, train, epochs, learn_rate, betas, weight_decay, print_results)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-4549762a9874>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cuda'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m                 \u001b[0mraw_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'logits'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'past_key_values'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "modelgpt = finetune_gpt(modelgpt, train, epochs=1, learn_rate=0.00001,  betas=(0.99, 0.9995), weight_decay=0.000001, print_results=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab03f37-c70f-4e60-a223-345f58118a46",
   "metadata": {},
   "source": [
    "GPT2 Results\n",
    "- No fine tuning: 706.16187 perplexity\n",
    "- Fine tuning: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88de890b",
   "metadata": {},
   "source": [
    "# Neural ODE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1c08b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys ; sys.path.append('../')\n",
    "from torchdyn.models import *\n",
    "from torchdyn.datasets import *\n",
    "from torchdyn import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b901011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# an Neural ODE that uses an LSTMCell as the derivative function\n",
    "class ODELSTM(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
    "        super().__init__()\n",
    "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False) # this encodes the sequence\n",
    "        #self.fnode = torch.nn.LSTMCell(input_size, layer_size, 1, bidirectional=False)\n",
    "        self.f = torch.nn.Sequential(\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(layer_size, layer_size),\n",
    "        )\n",
    "        self.node = NeuralDE(self.f, sensitivity='adjoint', solver='dopri5').to(device)\n",
    "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
    "        self.softmax = torch.nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # at the moment this feeds the entire sequence to LSTM and asks Neural ODE to reproduce it\n",
    "        # TODO: switch to feeding half the sequence and asking NeuralODE to extrapolate\n",
    "        sequence_outputs, hidden_state = self.LSTM(x)\n",
    "        final_hidden = sequence_outputs[:, -1, :]\n",
    "\n",
    "        # feed to neural ode\n",
    "        timesteps = torch.linspace(0, sequence_length-1, sequence_length).to(device)\n",
    "        sequence_outputs = self.node.trajectory(final_hidden, timesteps) # input is 128 final hidden states of dimension 300\n",
    "        sequence_outputs = torch.swapaxes(sequence_outputs, 0, 1)        # output is output across 20 timesteps giving as (20, 128, 300) output, so swap the sequence and batch dimension\n",
    "        \n",
    "        # Get final output\n",
    "        pred = self.linear(sequence_outputs)\n",
    "        return pred\n",
    "    \n",
    "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
    "    def predict(self, x):\n",
    "        preds = self.forward(x)\n",
    "        preds = self.softmax(preds).detach().cpu().numpy()\n",
    "        return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a17d802e-e447-4295-a3ed-e8f5d3fed787",
   "metadata": {},
   "outputs": [],
   "source": [
    "NODEmodel = ODELSTM(vocab_size, input_size=300, layer_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0b8361-ec7b-4561-a2f5-93d6b35960b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Minibatch loss: 5.62, Subsample Accuracy: 0.19, Train Perplexity: 992.81, Validation Perplexity: 949.61, Epoch Time: 334.68 seconds\n",
      "Epoch 1, Minibatch loss: 5.38, Subsample Accuracy: 0.18, Train Perplexity: 825.13, Validation Perplexity: 815.57, Epoch Time: 438.83 seconds\n",
      "Epoch 2, Minibatch loss: 5.46, Subsample Accuracy: 0.19, Train Perplexity: 718.59, Validation Perplexity: 734.18, Epoch Time: 517.79 seconds\n",
      "Epoch 3, Minibatch loss: 5.50, Subsample Accuracy: 0.19, Train Perplexity: 699.71, Validation Perplexity: 762.84, Epoch Time: 598.56 seconds\n",
      "Epoch 4, Minibatch loss: 5.32, Subsample Accuracy: 0.18, Train Perplexity: 602.00, Validation Perplexity: 666.84, Epoch Time: 600.94 seconds\n",
      "Epoch 5, Minibatch loss: 5.17, Subsample Accuracy: 0.18, Train Perplexity: 601.95, Validation Perplexity: 663.96, Epoch Time: 599.20 seconds\n",
      "Epoch 6, Minibatch loss: 5.11, Subsample Accuracy: 0.18, Train Perplexity: 1000.37, Validation Perplexity: 742.20, Epoch Time: 607.65 seconds\n",
      "Epoch 7, Minibatch loss: 5.13, Subsample Accuracy: 0.19, Train Perplexity: 650.42, Validation Perplexity: 778.69, Epoch Time: 626.19 seconds\n",
      "Epoch 8, Minibatch loss: 4.74, Subsample Accuracy: 0.18, Train Perplexity: 620.08, Validation Perplexity: 753.30, Epoch Time: 672.27 seconds\n",
      "Epoch 9, Minibatch loss: 4.98, Subsample Accuracy: 0.19, Train Perplexity: 586.12, Validation Perplexity: 651.04, Epoch Time: 722.94 seconds\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "NODEmodel = train_model(NODEmodel, train_X, train_y, epochs=10, learn_rate=0.001, weight_decay=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb801211-f4bf-47ad-83cc-a22996691a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "del NODEmodel\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
