{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "toc-showtags": false,
    "colab": {
      "name": "Language ODE(M)",
      "provenance": [],
      "collapsed_sections": [
        "PUQZjAwa6Icw"
      ],
      "toc_visible": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa0cd16cce0141a6864c352a394a6259": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_cef0e3c2c11a46c39a0a2345904947e1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d055c42db03a42eeb6c6116a81f8c4c5",
              "IPY_MODEL_e567cd15679a4a0ea7ff734a4710121a"
            ]
          }
        },
        "cef0e3c2c11a46c39a0a2345904947e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d055c42db03a42eeb6c6116a81f8c4c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_acd51a3c6d4146b1b0b6ea4d5a6fbf04",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1042301,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1042301,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0ee58b71a9b54beb8fbe7b08b159b4f7"
          }
        },
        "e567cd15679a4a0ea7ff734a4710121a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_212d581b0c3447969a6cb7277abc60ed",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.04M/1.04M [00:01&lt;00:00, 676kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_42e38ea3b41f487db77e21fbf839077d"
          }
        },
        "acd51a3c6d4146b1b0b6ea4d5a6fbf04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0ee58b71a9b54beb8fbe7b08b159b4f7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "212d581b0c3447969a6cb7277abc60ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "42e38ea3b41f487db77e21fbf839077d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e4e18c5f0fd54c1b9e6a75e97e04a77c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_38adc032ae494c178fd9f918c8c84701",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5c2d96014e8c4c30a09d259ab01f8b47",
              "IPY_MODEL_17b40c593df74858b309966d3f700382"
            ]
          }
        },
        "38adc032ae494c178fd9f918c8c84701": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5c2d96014e8c4c30a09d259ab01f8b47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a06dcdfcdb234a3aa84061d52fcd31b9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 456318,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 456318,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5760c2f469eb4839bb9ea31c2c887c3c"
          }
        },
        "17b40c593df74858b309966d3f700382": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_fbf2ffe6f338469882c6705da3b0f0ac",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 456k/456k [00:00&lt;00:00, 902kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7fdc7711b784666bc1dd3c74db793f0"
          }
        },
        "a06dcdfcdb234a3aa84061d52fcd31b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5760c2f469eb4839bb9ea31c2c887c3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "fbf2ffe6f338469882c6705da3b0f0ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7fdc7711b784666bc1dd3c74db793f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c569b23db24844e3a1127a21548f2772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1daf7b85fad04cbdbae81a6ef547b2dc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_09b0923fbb2e4a7cb18710027160d88f",
              "IPY_MODEL_f9e58c45a51f44189dc791df0f95066f"
            ]
          }
        },
        "1daf7b85fad04cbdbae81a6ef547b2dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "09b0923fbb2e4a7cb18710027160d88f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d54633c6b019425a83eeef0776a93e6d",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 1355256,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1355256,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_eb31f0768d9e4efab2c3154ccd9346f2"
          }
        },
        "f9e58c45a51f44189dc791df0f95066f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_490f2638725a4ba99cb779ed89b3806e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 3.39MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f3d8210edd174426870ac6eea3a88498"
          }
        },
        "d54633c6b019425a83eeef0776a93e6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "eb31f0768d9e4efab2c3154ccd9346f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "490f2638725a4ba99cb779ed89b3806e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f3d8210edd174426870ac6eea3a88498": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "270bf48bfc0d49b68b5bc8d30150fdde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0e933e284f70443b81eb5a9dfdee3992",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bb59ddee5efb41239146dd86b286bc99",
              "IPY_MODEL_af4cd932837144539b6f1db240c73858"
            ]
          }
        },
        "0e933e284f70443b81eb5a9dfdee3992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bb59ddee5efb41239146dd86b286bc99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f5204be9616c4923a888b9ee1c02da53",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 665,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 665,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8cce387785b4fe881a8660712b893d6"
          }
        },
        "af4cd932837144539b6f1db240c73858": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_34a039c6df4b4eafa02a24a811315a3c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 665/665 [00:00&lt;00:00, 1.40kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_20396401fe574635b5045c7849c61b09"
          }
        },
        "f5204be9616c4923a888b9ee1c02da53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8cce387785b4fe881a8660712b893d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "34a039c6df4b4eafa02a24a811315a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "20396401fe574635b5045c7849c61b09": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7beec4b473434c5cb086a104a060ca9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_60369f15ccd1419da0b9893fdb8f237c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6b46e8f9de974d2ba8715a5da39d38ba",
              "IPY_MODEL_3ff7a15320ab4a7ebbcd6cccb044a993"
            ]
          }
        },
        "60369f15ccd1419da0b9893fdb8f237c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6b46e8f9de974d2ba8715a5da39d38ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d66e0007a95e4c7a8f573653c13009cd",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 548118077,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 548118077,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8feb7fa4e48a4c1db1db49a2c761bb98"
          }
        },
        "3ff7a15320ab4a7ebbcd6cccb044a993": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4b4c47490f1748a4962d48d435e2077e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:13&lt;00:00, 41.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8882d09e0cbf43e8a67c7f71a195b2f6"
          }
        },
        "d66e0007a95e4c7a8f573653c13009cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8feb7fa4e48a4c1db1db49a2c761bb98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4b4c47490f1748a4962d48d435e2077e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8882d09e0cbf43e8a67c7f71a195b2f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1443277b136643f38f0c2316d1736483": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_15dd2f6a688f4fc8a04517e39d9ec25e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_68fb25334af34598a9c13b771c497ac3",
              "IPY_MODEL_f33ec0b6adee4056a0bd075e6651051b"
            ]
          }
        },
        "15dd2f6a688f4fc8a04517e39d9ec25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "68fb25334af34598a9c13b771c497ac3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_4c99cda001c642ec81d801f8fa292ad1",
            "_dom_classes": [],
            "description": "Epoch 0: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 42068,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 42068,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9493dd3eff8e4e90ada07e054f07401b"
          }
        },
        "f33ec0b6adee4056a0bd075e6651051b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e66acc3ea6624d50bf2aaff2cf7984f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42068/42068 [1:53:04&lt;00:00,  6.20it/s, loss=4.83, v_num=2]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5eb46b40b9ba42dd927c4bfb2b068a7f"
          }
        },
        "4c99cda001c642ec81d801f8fa292ad1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9493dd3eff8e4e90ada07e054f07401b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e66acc3ea6624d50bf2aaff2cf7984f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5eb46b40b9ba42dd927c4bfb2b068a7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9c466ee8506e428993dade045f30d998": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_88d5b3daa1b24fe392f3f3c7ef5023f5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c5f226c3e40d4da980ed54ab39bf0be1",
              "IPY_MODEL_7397cedab7264c15833bd44b20c7523d"
            ]
          }
        },
        "88d5b3daa1b24fe392f3f3c7ef5023f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": "row wrap",
            "width": "100%",
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": "inline-flex",
            "left": null
          }
        },
        "c5f226c3e40d4da980ed54ab39bf0be1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9ec48903f1014ce3ba85b6b7add0aad4",
            "_dom_classes": [],
            "description": "Epoch 1: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 42068,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 42068,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7966eec2481f4a7695231daac1f1c757"
          }
        },
        "7397cedab7264c15833bd44b20c7523d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_026852a148364e5f8489448b8b0f888f",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 42068/42068 [1:48:00&lt;00:00,  6.49it/s, loss=4.38, v_num=2]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2d226338ed844f92a9ad198445c20890"
          }
        },
        "9ec48903f1014ce3ba85b6b7add0aad4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7966eec2481f4a7695231daac1f1c757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": "2",
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "026852a148364e5f8489448b8b0f888f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2d226338ed844f92a9ad198445c20890": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60a92153-258b-4d12-b136-c1d4ff34b715"
      },
      "source": [
        "# Requirements\n",
        "Requires an Nvidia GPU to run\n",
        "\n",
        "Create a new anaconda environment and run the following commands to install the required libraries \n",
        "```\n",
        "conda install pytorch torchvision torchaudio cudatoolkit=11.1 -c pytorch -c conda-forge\n",
        "conda install gensim\n",
        "pip install torchdyn\n",
        "pip install git+https://github.com/google-research/torchsde.git\n",
        "```"
      ],
      "id": "60a92153-258b-4d12-b136-c1d4ff34b715"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0dfe412-887c-43b0-ae9e-68e68207a975"
      },
      "source": [
        "# Citations\n",
        "- Marcus, Mitchell P., Marcinkiewicz, Mary Ann & Santorini, Beatrice (1993). Building a Large Annotated Corpus of English: The Penn Treebank\n",
        "\n",
        "```\n",
        "@article{poli2020torchdyn,\n",
        "  title={TorchDyn: A Neural Differential Equations Library},\n",
        "  author={Poli, Michael and Massaroli, Stefano and Yamashita, Atsushi and Asama, Hajime and Park, Jinkyoo},\n",
        "  journal={arXiv preprint arXiv:2009.09346},\n",
        "  year={2020}\n",
        "}\n",
        "```\n",
        "\n",
        "- GloVe\n",
        "\n",
        "- GPT2 paper\n",
        "\n",
        "- Huggingface for their implementation of transformers? Not sure if this has a paper\n"
      ],
      "id": "c0dfe412-887c-43b0-ae9e-68e68207a975"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dea389f-a316-483d-94f9-3e80e54e517d"
      },
      "source": [
        "# Imports"
      ],
      "id": "3dea389f-a316-483d-94f9-3e80e54e517d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzeThJStJIQV",
        "outputId": "55d3ddd1-df02-457d-b7c2-daeba8b8725c"
      },
      "source": [
        "!pip install torchdyn\n",
        "!pip install torchinfo\n",
        "!pip install git+https://github.com/google-research/torchsde.git"
      ],
      "id": "KzeThJStJIQV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchdyn\n",
            "  Downloading https://files.pythonhosted.org/packages/31/9b/56bd9cc4cf9f726e347a418920ea249fe91caf666bbcd1b619143a9386d6/torchdyn-0.2.2.1-py3-none-any.whl\n",
            "Collecting torchdiffeq>=0.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/63/c2/daf5cc6c548f789d0f5222a6daecb8a76d72ad2fa96d958d46cb85f7ae3a/torchdiffeq-0.2.1-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from torchdyn) (3.2.2)\n",
            "Collecting dgl>=0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/c4/ce24841375cf4393787dbf9a645e271c19a03d2d9a0e5770b08ba76bcfde/dgl-0.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.4MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4MB 28.4MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning>=0.8.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/42/80/03dcc7241722bffd5b76b796d31dbb1bbdc34a90d653de6b47c8ad9ffd73/pytorch_lightning-1.3.3-py3-none-any.whl (806kB)\n",
            "\u001b[K     |████████████████████████████████| 808kB 37.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchdyn) (1.8.1+cu101)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from torchdyn) (0.9.1+cu101)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torchdyn) (0.22.2.post1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torchdyn) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torchdyn) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torchdyn) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torchdyn) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->torchdyn) (0.10.0)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.7/dist-packages (from dgl>=0.4.1->torchdyn) (2.5.1)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from dgl>=0.4.1->torchdyn) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from dgl>=0.4.1->torchdyn) (1.4.1)\n",
            "Collecting tensorboard!=2.5.0,>=2.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/21/eebd23060763fedeefb78bc2b286e00fa1d8abda6f70efa2ee08c28af0d4/tensorboard-2.4.1-py3-none-any.whl (10.6MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6MB 37.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=0.8.4->torchdyn) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=0.8.4->torchdyn) (20.9)\n",
            "Collecting fsspec[http]>=2021.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/52/816d1a3a599176057bf29dfacb1f8fadb61d35fbd96cb1bab4aaa7df83c0/fsspec-2021.5.0-py3-none-any.whl (111kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 58.6MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/14/52/aa227a0884df71ed1957649085adf2b8bc2a1816d037c2f18b3078854516/pyDeprecate-0.3.0-py3-none-any.whl\n",
            "Collecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 41.4MB/s \n",
            "\u001b[?25hCollecting PyYAML<=5.4.1,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7a/a5/393c087efdc78091afa2af9f1378762f9821c9c1d7a22c5753fb5ac5f97a/PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636kB)\n",
            "\u001b[K     |████████████████████████████████| 645kB 37.2MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3b/e8/513cd9d0b1c83dc14cd8f788d05cd6a34758d4fd7e4f9e5ecd5d7d599c95/torchmetrics-0.3.2-py3-none-any.whl (274kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 53.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->torchdyn) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->torchdyn) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torchdyn) (1.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->torchdyn) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.1->dgl>=0.4.1->torchdyn) (4.4.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl>=0.4.1->torchdyn) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl>=0.4.1->torchdyn) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl>=0.4.1->torchdyn) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->dgl>=0.4.1->torchdyn) (2.10)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (3.12.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (1.34.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (56.1.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (0.36.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (1.8.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (0.4.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (3.3.4)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (4.2.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (4.0.1)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 54.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=2021.4.0->pytorch-lightning>=0.8.4->torchdyn) (21.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 56.6MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning>=0.8.4->torchdyn) (3.4.1)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=1ebb086af32920c8a7c544893eb56175325234207ab3ba26ccbdd94468fc8b18\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built future\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement tensorboard~=2.5, but you'll have tensorboard 2.4.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: torchdiffeq, dgl, tensorboard, multidict, yarl, async-timeout, aiohttp, fsspec, pyDeprecate, future, PyYAML, torchmetrics, pytorch-lightning, torchdyn\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 dgl-0.6.1 fsspec-2021.5.0 future-0.18.2 multidict-5.1.0 pyDeprecate-0.3.0 pytorch-lightning-1.3.3 tensorboard-2.4.1 torchdiffeq-0.2.1 torchdyn-0.2.2.1 torchmetrics-0.3.2 yarl-1.6.3\n",
            "Collecting torchinfo\n",
            "  Downloading https://files.pythonhosted.org/packages/5f/a6/7339896cf79ee7fb244777375ee4500f881a9b4b951113aa7f6edbc8e1b2/torchinfo-0.1.2-py3-none-any.whl\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-0.1.2\n",
            "Collecting git+https://github.com/google-research/torchsde.git\n",
            "  Cloning https://github.com/google-research/torchsde.git to /tmp/pip-req-build-1mwfv470\n",
            "  Running command git clone -q https://github.com/google-research/torchsde.git /tmp/pip-req-build-1mwfv470\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy==1.19.* in /usr/local/lib/python3.7/dist-packages (from torchsde==0.2.5) (1.19.5)\n",
            "Collecting trampoline>=0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/73/54/d2805324fb746d8da86d3844bee4f55c0cfd6c136de61b713772d44c5bea/trampoline-0.1.2-py3-none-any.whl\n",
            "Collecting scipy==1.5.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/7e/8f6a79b102ca1ea928bae8998b05bf5dc24a90571db13cd119f275ba6252/scipy-1.5.4-cp37-cp37m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 1.5MB/s \n",
            "\u001b[?25hCollecting boltons>=20.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/a7/1a31561d10a089fcb46fe286766dd4e053a12f6e23b4fd1c26478aff2475/boltons-21.0.0-py2.py3-none-any.whl (193kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 41.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from torchsde==0.2.5) (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->torchsde==0.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: torchsde\n",
            "  Building wheel for torchsde (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torchsde: filename=torchsde-0.2.5-cp37-none-any.whl size=55586 sha256=8c81bdb5c6966b3f2abf989909f9b215acedb27a31b244c63b8c20df00d9781e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-mecclp3w/wheels/31/b5/4b/53c7d7c124c1bbfebd2c5f429ca86b5e59f6cd4718dc0f1229\n",
            "Successfully built torchsde\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: trampoline, scipy, boltons, torchsde\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "Successfully installed boltons-21.0.0 scipy-1.5.4 torchsde-0.2.5 trampoline-0.1.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d65ff0e6"
      },
      "source": [
        "import torch\n",
        "import torch.utils.data\n",
        "import torchtext\n",
        "import numpy as np\n",
        "import gensim.downloader as api\n",
        "from functools import reduce\n",
        "from sklearn.metrics import *\n",
        "from time import time\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from torchdyn.models import *\n",
        "from torchdyn import *\n",
        "import pytorch_lightning as pl\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "from torchinfo import summary"
      ],
      "id": "d65ff0e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y67aiTtCtYmw"
      },
      "source": [
        "def numpy_to_tensor(array):\n",
        "    return torch.from_numpy(array).to(device).float()"
      ],
      "id": "Y67aiTtCtYmw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b469c71b"
      },
      "source": [
        "# LSTM Baseline\n",
        "Create a baseline RNN and evaluate it's perplexity\n",
        "\n",
        "To do\n",
        "- Use LSTM as baseline\n",
        "    - Examine perplexity of model on validation set\n",
        "- Implement Neural ODE"
      ],
      "id": "b469c71b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AgdaZst5dWY9"
      },
      "source": [
        "## Data Processing"
      ],
      "id": "AgdaZst5dWY9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c916364a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e956dec3-0e93-4488-cc8e-55cb29f6dde6"
      },
      "source": [
        "%%time\n",
        "# load word embeddings\n",
        "glove = api.load(\"glove-wiki-gigaword-300\")"
      ],
      "id": "c916364a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 376.1/376.1MB downloaded\n",
            "CPU times: user 3min, sys: 9.86 s, total: 3min 10s\n",
            "Wall time: 3min 35s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45e5e765"
      },
      "source": [
        "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test')) # len(train) = 4.2w\n",
        "# train, valid, test = torchtext.datasets.WikiText2(split=('train', 'valid', 'test')) # len(train) = 3.6w\n",
        "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
        "valid = list(valid)\n",
        "test  = list(test)"
      ],
      "id": "45e5e765",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd8d4c81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df54b71-d6bf-4831-862c-8d79f4da9b45"
      },
      "source": [
        "# build the vocab\n",
        "corpus = train + valid\n",
        "vocab = {\"<PAD>\": 0}\n",
        "index_vocab = {0 : \"<PAD>\"}\n",
        "for sentence in corpus:\n",
        "    for token in sentence.split(\" \")[1:]:\n",
        "        if token not in vocab:\n",
        "            index = len(vocab)\n",
        "            vocab[token] = index\n",
        "            index_vocab[index] = token\n",
        "\n",
        "# replace penn treebank end sentence token \"\\n\" with glove's end sentence token \".\"\n",
        "index = vocab[\"\\n\"]\n",
        "vocab.pop(\"\\n\")         \n",
        "vocab[\".\"] = index\n",
        "index_vocab[index] = \".\"\n",
        "\n",
        "# view size\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocab size: \", vocab_size)"
      ],
      "id": "dd8d4c81",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab size:  10001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cb19c15"
      },
      "source": [
        "# pad sentences and convert words to their glove vector to get input features\n",
        "# convert to 1 hot vocab and shift 1 to the left to get output labels (converting to 1 hot takes too much memory, so just store indices and convert later)\n",
        "# use left padding, as we want the hidden state at the end (right) to ignore the padding\n",
        "# returns word_vector_dataset, labels\n",
        "def preprocess(dataset, sequence_length, wv):\n",
        "    embedding_size = wv[\"hello\"].shape[0]\n",
        "    processed = np.zeros((len(dataset), sequence_length, embedding_size))\n",
        "    labels = np.zeros((len(dataset), sequence_length, 1))\n",
        "    \n",
        "    for i in range(len(dataset)):\n",
        "        tokens = dataset[i].split(\" \")[1:]\n",
        "        \n",
        "        # get the word vectors for all of the tokens, removing out of vocabulary (OOV) tokens\n",
        "        tokens_np = np.zeros((len(tokens), embedding_size))\n",
        "        labels_np = np.zeros((len(tokens), 1))\n",
        "        j = 0\n",
        "        for word in tokens:\n",
        "            if word == \"\\n\": word = \".\" # replace PennTreebank end sentence token '\\n' with glove end sentence token \".\"\n",
        "            if word not in wv: continue # ignore OOV tokens\n",
        "            if j < sequence_length - 1: # only add sequence_length - 1 tokens at max\n",
        "                # so that there is always a 0 vector at the start so the model learns most common starting words\n",
        "                tokens_np[j, :] = wv[word]\n",
        "            # we can look ahead to find the next word to set as the label for the last word\n",
        "            if j < sequence_length:\n",
        "                labels_np[j, :] = vocab[word]\n",
        "            else: break\n",
        "            j += 1\n",
        "            \n",
        "        tokens_np = tokens_np[:j-1, :]\n",
        "        labels_np = labels_np[:j, :]\n",
        "        \n",
        "        # add this sentence to the overall dataset, with left padding of 0 vectors\n",
        "        processed[i, sequence_length - tokens_np.shape[0]:, :] = tokens_np\n",
        "        labels[i, sequence_length - labels_np.shape[0]:, :] = labels_np\n",
        "    return processed, labels"
      ],
      "id": "1cb19c15",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "162bc724"
      },
      "source": [
        "sequence_length = 20\n",
        "train_X, train_y = preprocess(train, sequence_length, glove)\n",
        "valid_X, valid_y = preprocess(valid, sequence_length, glove)\n",
        "test_X , test_y  = preprocess(test,  sequence_length, glove)"
      ],
      "id": "162bc724",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6DGrlw_d8cg"
      },
      "source": [
        "## Model Training"
      ],
      "id": "A6DGrlw_d8cg"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c40b5ab7"
      },
      "source": [
        "class LSTMModel(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
        "        super().__init__()\n",
        "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False)\n",
        "        self.dropout = torch.nn.Dropout(p=dropout)\n",
        "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # convert words to their vectors here\n",
        "        sequence_outputs, hidden_state = self.LSTM(x)\n",
        "        sequence_outputs = self.dropout(sequence_outputs)\n",
        "        pred = self.linear(sequence_outputs)\n",
        "        return pred\n",
        "    \n",
        "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
        "    def predict(self, x):\n",
        "        preds = self.forward(x)\n",
        "        preds = self.softmax(preds).detach().cpu().numpy()\n",
        "        return preds"
      ],
      "id": "c40b5ab7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46abd825",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60a4628-965d-40da-b991-fec4541d25db"
      },
      "source": [
        "model = LSTMModel(vocab_size, input_size=300, layer_size=300, dropout=0.1)\n",
        "model.to(device)\n",
        "summary(model)"
      ],
      "id": "46abd825",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTMModel(\n",
              "  (LSTM): LSTM(300, 300)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (linear): Linear(in_features=300, out_features=10001, bias=True)\n",
              "  (softmax): Softmax(dim=-1)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "025faae9"
      },
      "source": [
        "# clear memory\n",
        "del data\n",
        "torch.cuda.empty_cache()"
      ],
      "id": "025faae9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBx7DTJPprbH"
      },
      "source": [
        "### Evaluation Methods"
      ],
      "id": "eBx7DTJPprbH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "441a5611"
      },
      "source": [
        "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
        "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
        "def perplexity(preds, ground_truth, epsilon=1e-30):\n",
        "    probs = []\n",
        "    for i in range(preds.shape[1]):\n",
        "        probs.append(preds[0, i, int(ground_truth[i])])\n",
        "    probs = np.array(probs)\n",
        "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
        "    return np.prod(probs)\n",
        "\n",
        "# Calculate overall perplexity for a dataset\n",
        "def average_perplexity(model, X, y):\n",
        "    perplexities = [perplexity(model.predict(numpy_to_tensor(X[i:i+1])), y[i]) for i in range(X.shape[0])]\n",
        "    return np.mean(perplexities)\n",
        "\n",
        "\n",
        "# Define the functions to compute the bleu score, \n",
        "# in our particular case, reference should be multiple sentences - all sentences as label, against the candidate - predicted sentence\n",
        "# here I use the 10001 vocab to represent each word (num also works in bleu)\n",
        "# smoothie is will help when sentence is too short\n",
        "# https://stackoverflow.com/questions/46444656/bleu-scores-could-i-use-nltk-translate-bleu-score-sentence-bleu-for-calculating\n",
        "def bleu(preds, reference, smoothie):\n",
        "    candidate = np.argmax(preds, axis=2)[0]\n",
        "    return sentence_bleu(reference, candidate, smoothing_function=smoothie)\n",
        "\n",
        "\n",
        "def average_bleu(model, X, y):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    for i in range(X.shape[0]):\n",
        "        preds = model.predict(numpy_to_tensor(X[i:i+1]))\n",
        "        reference = [list(train_y[i].flatten())]\n",
        "        bleus.append(bleu(preds, reference, smoothie))\n",
        "    return np.mean(bleus)\n"
      ],
      "id": "441a5611",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It_KWm5ppwPz"
      },
      "source": [
        "### Actual Training"
      ],
      "id": "It_KWm5ppwPz"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7fc95fb"
      },
      "source": [
        "# training the model\n",
        "def train_model(model, train_X, train_y, epochs=10, learn_rate=0.01, weight_decay=0.001, minibatch_size=128, print_results=True):\n",
        "    # Prepare data\n",
        "    X = numpy_to_tensor(train_X)\n",
        "    y = numpy_to_tensor(train_y).long()[:, :, 0]\n",
        "    n_samples = X.shape[0]\n",
        "    \n",
        "    # Define loss and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Ensure this runs on gpu\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):      \n",
        "        model.train() # set to train flag\n",
        "        start_ts = time()\n",
        "        \n",
        "        # shuffle the data\n",
        "        new_indices = torch.randperm(n_samples)\n",
        "        X = X[new_indices, :, :] \n",
        "        y = y[new_indices, :]\n",
        "        \n",
        "        for batch_n in range(int(np.ceil(n_samples/minibatch_size))):\n",
        "            # get the minibatch\n",
        "            start_index = batch_n * minibatch_size\n",
        "            end_index = min(start_index + minibatch_size, n_samples)\n",
        "            batch_X = X[start_index: end_index, :, :]\n",
        "            batch_y = y[start_index: end_index, :]\n",
        "            \n",
        "            # forward + backward + optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(batch_X) \n",
        "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            \n",
        "        # evaluate performance on part of the data (for runtime reason we only use perplexity of validation dataset)\n",
        "        if print_results:\n",
        "            with torch.no_grad():\n",
        "                model.eval()\n",
        "                v_perplexity = average_perplexity(model, valid_X, valid_y)\n",
        "                end_ts = time()\n",
        "                print(\"Epoch {}, Minibatch loss: {:.2f}, Val Perplexity: {:.2f}, Epoch Time: {:.2f} seconds\"\n",
        "                .format(epoch, loss.item(), v_perplexity, end_ts - start_ts))\n",
        "    \n",
        "    del X\n",
        "    del y\n",
        "    torch.cuda.empty_cache()\n",
        "    if print_results:\n",
        "        print('Finished Training')\n",
        "    return model"
      ],
      "id": "a7fc95fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "tags": [],
        "id": "d51665cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16e18c9c-e66a-43f1-fe1f-1f710a060afe"
      },
      "source": [
        "%%time\n",
        "model = train_model(model, train_X, train_y, epochs=50, learn_rate=1e-3, minibatch_size=256, weight_decay=1e-5)"
      ],
      "id": "d51665cc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0, Minibatch loss: 5.61, Val Perplexity: 4260.62, Epoch Time: 14.00 seconds\n",
            "Epoch 1, Minibatch loss: 5.64, Val Perplexity: 2802.05, Epoch Time: 14.04 seconds\n",
            "Epoch 2, Minibatch loss: 5.35, Val Perplexity: 2045.14, Epoch Time: 14.04 seconds\n",
            "Epoch 3, Minibatch loss: 5.00, Val Perplexity: 1624.39, Epoch Time: 14.09 seconds\n",
            "Epoch 4, Minibatch loss: 4.90, Val Perplexity: 1352.87, Epoch Time: 14.11 seconds\n",
            "Epoch 5, Minibatch loss: 4.69, Val Perplexity: 1116.65, Epoch Time: 14.06 seconds\n",
            "Epoch 6, Minibatch loss: 4.70, Val Perplexity: 972.72, Epoch Time: 14.07 seconds\n",
            "Epoch 7, Minibatch loss: 4.74, Val Perplexity: 839.83, Epoch Time: 14.01 seconds\n",
            "Epoch 8, Minibatch loss: 4.74, Val Perplexity: 731.71, Epoch Time: 13.94 seconds\n",
            "Epoch 9, Minibatch loss: 4.62, Val Perplexity: 648.30, Epoch Time: 14.08 seconds\n",
            "Epoch 10, Minibatch loss: 4.64, Val Perplexity: 581.80, Epoch Time: 14.09 seconds\n",
            "Epoch 11, Minibatch loss: 4.33, Val Perplexity: 528.58, Epoch Time: 14.09 seconds\n",
            "Epoch 12, Minibatch loss: 4.38, Val Perplexity: 484.68, Epoch Time: 14.07 seconds\n",
            "Epoch 13, Minibatch loss: 4.52, Val Perplexity: 446.85, Epoch Time: 14.09 seconds\n",
            "Epoch 14, Minibatch loss: 4.55, Val Perplexity: 419.61, Epoch Time: 14.02 seconds\n",
            "Epoch 15, Minibatch loss: 4.61, Val Perplexity: 399.05, Epoch Time: 14.04 seconds\n",
            "Epoch 16, Minibatch loss: 4.51, Val Perplexity: 376.26, Epoch Time: 14.12 seconds\n",
            "Epoch 17, Minibatch loss: 4.73, Val Perplexity: 360.39, Epoch Time: 14.12 seconds\n",
            "Epoch 18, Minibatch loss: 4.52, Val Perplexity: 345.88, Epoch Time: 14.09 seconds\n",
            "Epoch 19, Minibatch loss: 4.44, Val Perplexity: 332.13, Epoch Time: 14.03 seconds\n",
            "Epoch 20, Minibatch loss: 4.78, Val Perplexity: 324.62, Epoch Time: 14.09 seconds\n",
            "Epoch 21, Minibatch loss: 4.21, Val Perplexity: 318.29, Epoch Time: 14.03 seconds\n",
            "Epoch 22, Minibatch loss: 4.76, Val Perplexity: 312.86, Epoch Time: 14.10 seconds\n",
            "Epoch 23, Minibatch loss: 4.35, Val Perplexity: 309.11, Epoch Time: 14.04 seconds\n",
            "Epoch 24, Minibatch loss: 4.57, Val Perplexity: 307.18, Epoch Time: 14.05 seconds\n",
            "Epoch 25, Minibatch loss: 4.31, Val Perplexity: 304.38, Epoch Time: 14.09 seconds\n",
            "Epoch 26, Minibatch loss: 4.58, Val Perplexity: 303.94, Epoch Time: 14.06 seconds\n",
            "Epoch 27, Minibatch loss: 4.44, Val Perplexity: 301.61, Epoch Time: 14.02 seconds\n",
            "Epoch 28, Minibatch loss: 4.18, Val Perplexity: 300.35, Epoch Time: 14.06 seconds\n",
            "Epoch 29, Minibatch loss: 4.32, Val Perplexity: 297.33, Epoch Time: 14.05 seconds\n",
            "Epoch 30, Minibatch loss: 4.44, Val Perplexity: 295.73, Epoch Time: 14.02 seconds\n",
            "Epoch 31, Minibatch loss: 4.58, Val Perplexity: 294.51, Epoch Time: 14.02 seconds\n",
            "Epoch 32, Minibatch loss: 4.41, Val Perplexity: 292.92, Epoch Time: 14.04 seconds\n",
            "Epoch 33, Minibatch loss: 4.50, Val Perplexity: 292.54, Epoch Time: 14.08 seconds\n",
            "Epoch 34, Minibatch loss: 4.50, Val Perplexity: 291.40, Epoch Time: 14.04 seconds\n",
            "Epoch 35, Minibatch loss: 4.25, Val Perplexity: 290.61, Epoch Time: 14.01 seconds\n",
            "Epoch 36, Minibatch loss: 4.13, Val Perplexity: 289.43, Epoch Time: 14.00 seconds\n",
            "Epoch 37, Minibatch loss: 4.35, Val Perplexity: 286.78, Epoch Time: 14.08 seconds\n",
            "Epoch 38, Minibatch loss: 4.20, Val Perplexity: 287.18, Epoch Time: 14.06 seconds\n",
            "Epoch 39, Minibatch loss: 4.40, Val Perplexity: 286.86, Epoch Time: 14.03 seconds\n",
            "Epoch 40, Minibatch loss: 4.19, Val Perplexity: 288.52, Epoch Time: 14.05 seconds\n",
            "Epoch 41, Minibatch loss: 4.35, Val Perplexity: 285.53, Epoch Time: 14.06 seconds\n",
            "Epoch 42, Minibatch loss: 4.34, Val Perplexity: 286.93, Epoch Time: 14.06 seconds\n",
            "Epoch 43, Minibatch loss: 4.34, Val Perplexity: 285.55, Epoch Time: 14.06 seconds\n",
            "Epoch 44, Minibatch loss: 3.97, Val Perplexity: 285.42, Epoch Time: 14.08 seconds\n",
            "Epoch 45, Minibatch loss: 4.26, Val Perplexity: 284.80, Epoch Time: 14.05 seconds\n",
            "Epoch 46, Minibatch loss: 4.27, Val Perplexity: 282.85, Epoch Time: 14.06 seconds\n",
            "Epoch 47, Minibatch loss: 4.25, Val Perplexity: 280.63, Epoch Time: 14.05 seconds\n",
            "Epoch 48, Minibatch loss: 4.31, Val Perplexity: 282.08, Epoch Time: 14.04 seconds\n",
            "Epoch 49, Minibatch loss: 4.23, Val Perplexity: 280.88, Epoch Time: 14.03 seconds\n",
            "Finished Training\n",
            "CPU times: user 7min 20s, sys: 4min 23s, total: 11min 44s\n",
            "Wall time: 11min 43s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50c2e2dd"
      },
      "source": [
        "## Hyper-Parameter tuning findings\n",
        "- glove 300 dimension vectors are essential to not have a bias of 1000 perplexity on both train and validation\n",
        "- 2 layers of LSTM also gives high bias, perhaps there is not enough train data\n",
        "- Weight decay is essential in preventing Validation perplexity from skyrocketing\n",
        "- Dropout of 0.1 combined with weight decay 0.00001 works (around 250 validation perplexity)\n",
        "- Decreasing learning rate and increasing epochs has a minor benefit"
      ],
      "id": "50c2e2dd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db063f69"
      },
      "source": [
        "## Examine Performance of the model\n",
        "- Using both perplexity and qualitative evaluation"
      ],
      "id": "db063f69"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUQZjAwa6Icw"
      },
      "source": [
        "### Performance of Teacher Forcing"
      ],
      "id": "PUQZjAwa6Icw"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92debfa8"
      },
      "source": [
        "%%time\n",
        "model.eval()\n",
        "print('Train perplexity is ', average_perplexity(model, train_X, train_y))"
      ],
      "id": "92debfa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4b8efd14",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0fef55a-1cc9-437c-bcb0-150c7aa7e9b0"
      },
      "source": [
        "average_perplexity(model, valid_X, valid_y)"
      ],
      "id": "4b8efd14",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "288.61844"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hx9tEDiieyHA",
        "outputId": "ac57e962-2641-4a4a-a8d8-d23ce28bbf4e"
      },
      "source": [
        "average_perplexity(model, test_X, test_y)"
      ],
      "id": "Hx9tEDiieyHA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "263.7379"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3j8tH_tC7Gos",
        "outputId": "f8550305-0536-452e-e962-9d686fee3ebf"
      },
      "source": [
        "%%time\n",
        "average_bleu(model, train_X, train_y)"
      ],
      "id": "3j8tH_tC7Gos",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 59.3 s, sys: 1.67 s, total: 1min\n",
            "Wall time: 1min\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2952516745473606"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUQvJla0-yYp",
        "outputId": "16d08072-8289-497f-d386-8c646a18e24e"
      },
      "source": [
        "average_bleu(model, valid_X, valid_y)"
      ],
      "id": "pUQvJla0-yYp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.23072047692589107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUyN1qhE_BzF",
        "outputId": "26197e97-843a-4916-c4c5-3f8a1ecdcaec"
      },
      "source": [
        "average_bleu(model, test_X, test_y)"
      ],
      "id": "xUyN1qhE_BzF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2289468706162604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GefhdkC6QFb"
      },
      "source": [
        "### Performance of Extrapolation"
      ],
      "id": "6GefhdkC6QFb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lTLJNZto5-c"
      },
      "source": [
        "#### Demo + Methods"
      ],
      "id": "7lTLJNZto5-c"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16LMW-yf6YMO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9104866f-417f-4b66-91f8-16ad6bedc289"
      },
      "source": [
        "%%time\n",
        "idx = 20\n",
        "\n",
        "input = numpy_to_tensor(train_X[idx:idx+1])\n",
        "input_idx = []\n",
        "for each in input[0]:\n",
        "    if sum(each) == 0: word = '<PAD>'\n",
        "    else:\n",
        "        word = glove.most_similar(positive=[each.cpu().detach().numpy(),])[0][0]\n",
        "    index = vocab[word]\n",
        "    # print(word, \": \", index)\n",
        "    input_idx.append(index)\n",
        "\n",
        "print('The original Sentence is:\\n{}\\n'.format(train[idx]))\n",
        "\n",
        "pred = np.argmax(model.forward(input).cpu().detach().numpy(), axis=2)[0].astype(float)\n",
        "label = train_y[idx].flatten()\n",
        "\n",
        "print('input index is:\\n{}\\n->with shape {}'.format(input_idx, input.shape))\n",
        "print('predict is:\\n{}\\n->with shape {}'.format(pred, pred.shape))\n",
        "print('label is:\\n{}\\n->with shape {}'.format(label, label.shape))"
      ],
      "id": "16LMW-yf6YMO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The original Sentence is:\n",
            " the percentage of lung cancer deaths among the workers at the west <unk> mass. paper factory appears to be the highest for any asbestos workers studied in western industrialized countries he said \n",
            "\n",
            "\n",
            "input index is:\n",
            "[0, 33, 73, 43, 208, 74, 75, 76, 33, 77, 161, 33, 217, 218, 181, 219, 220, 65, 221, 33]\n",
            "->with shape torch.Size([1, 20, 300])\n",
            "predict is:\n",
            "[   0.  190.   43.   33.   74.   25.  109.   33.  190.   49.   33.  190.\n",
            " 3577.  120.   49.   49.   65.   33.   36.  190.]\n",
            "->with shape (20,)\n",
            "label is:\n",
            "[ 33.  73.  43. 208.  74.  75.  76.  33.  77. 161.  33. 217. 218. 181.\n",
            " 219. 220.  65. 221.  33. 222.]\n",
            "->with shape (20,)\n",
            "CPU times: user 2.2 s, sys: 239 ms, total: 2.44 s\n",
            "Wall time: 1.27 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfKpg9rhpWe-"
      },
      "source": [
        "Beam Search"
      ],
      "id": "bfKpg9rhpWe-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2hAdwNK_yMCc"
      },
      "source": [
        "# get the top k most predicted results\n",
        "def get_topK(predicted, k=1):\n",
        "    \n",
        "    # Get the index of the highest k index\n",
        "    # Since the input is just one sentence, we can use [0] to extract the prediction result\n",
        "    top_k = np.argsort(predicted)[-k:]\n",
        "\n",
        "    # return a list of tuple\n",
        "    # tuple[0]:word_id, tuple[1]:log(p)\n",
        "    return [(id, predicted[id]) for id in top_k]\n",
        "\n",
        "def generate_text(model, wv, dataset, next_words, k=1):\n",
        "    # generate sentence given indexed sentence input (train_X, valid_X, test_X)\n",
        "    # TODO: choose pred or just ground truth as the first 20 words?\n",
        "    print('generating texts...')\n",
        "    emb_size = wv[\"hello\"].shape[0]\n",
        "    generated = []\n",
        "    for i in range(len(dataset)):\n",
        "        if i % 10 == 0:\n",
        "            print('{} of {} generates.'.format(i, len(dataset)))\n",
        "        seed_text = dataset[i] # text will take form of (seq length, embedding size)\n",
        "        seed_candidates = [(seed_text, .0)]\n",
        "        for _ in range(next_words):\n",
        "            successives = []\n",
        "            # if k = 1, len(seed_candidates) will always be 1\n",
        "            for i in range(len(seed_candidates)):\n",
        "                seed_text, score = seed_candidates[i]\n",
        "\n",
        "                seed_input = numpy_to_tensor(np.array([seed_text[-sequence_length:]]))\n",
        "                predicted = model(seed_input).cpu().detach().numpy()[0][-1] # take the vocab prob of last word as the output\n",
        "\n",
        "                tuples = get_topK(predicted, k)\n",
        "                for id, val in tuples:\n",
        "                    # get the output word\n",
        "                    if id == 0: output_emb = np.zeros((emb_size,))\n",
        "                    else:\n",
        "                        output_emb = wv[index_vocab[id]]\n",
        "                    # put the word into the sentence input\n",
        "                    # calcualte the accumulated score by -log(p)\n",
        "                    successives.append((np.vstack((seed_text,output_emb)), score - val)) \n",
        "\n",
        "            # Get the lowest k accumulated scores (highest k accumulated probabilities)\n",
        "            # Then, make them as the seed_candidate for the next word to predict\n",
        "            ordered = sorted(successives, key=lambda tup: tup[1])\n",
        "            seed_candidates = ordered[:k]\n",
        "        generated.append(seed_candidates[0][0])\n",
        "\n",
        "    return generated\n",
        "\n",
        "def generation_bleu(generates, wv, generate_length, references):\n",
        "    # generates(n_samples, total sequence_length, emb_size)\n",
        "    # only calcuate the generated part, otherwise score will be the same\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    for i in range(len(generates)):\n",
        "        candidate = []\n",
        "        for each in generates[i]:\n",
        "            if sum(each) == 0: word = '<PAD>'\n",
        "            else:\n",
        "                word = wv.most_similar(positive=[each,])[0][0] # very slow, needs improvement\n",
        "            candidate.append(vocab[word])\n",
        "        reference = references[i].flatten()\n",
        "        bleu = sentence_bleu([reference], candidate[-generate_length:], smoothing_function=smoothie)\n",
        "        bleus.append(bleu)\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            print('{} of {}, current mean: {}'.format(i, len(generates), np.mean(bleus)))\n",
        "    return np.mean(bleus)\n",
        "\n",
        "# generated = generate_text(model, glove, train_X[idx:idx+1], 20, k=4)\n"
      ],
      "id": "2hAdwNK_yMCc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIjtsj86jmu1",
        "outputId": "96995ccf-d60a-4698-fd42-faf71d1633b4"
      },
      "source": [
        "%%time\n",
        "words = []\n",
        "for each in generated[0]:\n",
        "    if sum(each) == 0: word = '<PAD>'\n",
        "    else:\n",
        "        word = glove.most_similar(positive=[each,])[0][0]\n",
        "    words.append(word)\n",
        "\n",
        "print(' '.join(words))"
      ],
      "id": "JIjtsj86jmu1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PAD> the bank which previously said it was for sale said it has received no offers and that its board of $ million or $ million or $ million or $ million or $ million or $ million of the\n",
            "CPU times: user 4.31 s, sys: 356 ms, total: 4.66 s\n",
            "Wall time: 2.42 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzchiq6ypNrK"
      },
      "source": [
        "Pure Argmax (Repetitive words)"
      ],
      "id": "kzchiq6ypNrK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrgUdfjBHDGF",
        "outputId": "78e7a122-6a6f-4624-8b4c-663a12d32b52"
      },
      "source": [
        "%%time\n",
        "generate_length = 20\n",
        "generated = [index_vocab[i] for i in input_idx]\n",
        "for g_idx in range(generate_length):\n",
        "    output = np.argmax(model.forward(input).cpu().detach().numpy(), axis=2)[0]\n",
        "    gen_text = [index_vocab[each] for each in output]\n",
        "    generated.append(gen_text[-1])\n",
        "    sentence = ' '.join(gen_text)\n",
        "    data_sent, __ = preprocess([sentence], sequence_length, glove)\n",
        "    input = numpy_to_tensor(data_sent[0:1])\n",
        "\n",
        "print(' '.join(generated))"
      ],
      "id": "BrgUdfjBHDGF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<PAD> the percentage of lung cancer deaths among the workers at the west mass. paper factory appears to be the <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            "CPU times: user 23 ms, sys: 2.96 ms, total: 26 ms\n",
            "Wall time: 26.3 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WTxm2qTrpHvq"
      },
      "source": [
        "#### Metric Testing"
      ],
      "id": "WTxm2qTrpHvq"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DvsWJfSqs1z",
        "outputId": "ebc42162-1d7c-45d9-e720-68323b2bde52"
      },
      "source": [
        "%%time \n",
        "gen_num = 200\n",
        "generate_length = 20\n",
        "generates = generate_text(model, glove, train_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, train_y))"
      ],
      "id": "9DvsWJfSqs1z",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 1min 6s, sys: 1.31 s, total: 1min 7s\n",
            "Wall time: 1min 7s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUMc9T1S6Iww",
        "outputId": "0bec9de1-9d8c-4593-a46c-18ad4405c7e7"
      },
      "source": [
        "%%time \n",
        "generates = generate_text(model, glove, valid_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, train_y))"
      ],
      "id": "tUMc9T1S6Iww",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 300, current mean: 0.0\n",
            "10 of 300, current mean: 0.14789264018854723\n",
            "20 of 300, current mean: 0.15120775962893512\n",
            "30 of 300, current mean: 0.14330097103655004\n",
            "40 of 300, current mean: 0.1410526006390764\n",
            "50 of 300, current mean: 0.14896077352347178\n",
            "60 of 300, current mean: 0.1384521413169753\n",
            "70 of 300, current mean: 0.13929613490233908\n",
            "80 of 300, current mean: 0.1384781248583928\n",
            "90 of 300, current mean: 0.13753324989701837\n",
            "100 of 300, current mean: 0.1400572111092312\n",
            "110 of 300, current mean: 0.1392895892897581\n",
            "120 of 300, current mean: 0.13739621476244523\n",
            "130 of 300, current mean: 0.1326058703500126\n",
            "140 of 300, current mean: 0.13364301669130613\n",
            "150 of 300, current mean: 0.13746308850729247\n",
            "160 of 300, current mean: 0.13620562854418256\n",
            "170 of 300, current mean: 0.13833006638909742\n",
            "180 of 300, current mean: 0.13676069313333056\n",
            "190 of 300, current mean: 0.1379898741210657\n",
            "200 of 300, current mean: 0.13710617443185913\n",
            "210 of 300, current mean: 0.13570865395884718\n",
            "220 of 300, current mean: 0.13541548225165345\n",
            "230 of 300, current mean: 0.13532482316680294\n",
            "240 of 300, current mean: 0.13412371230631712\n",
            "250 of 300, current mean: 0.13498732996639953\n",
            "260 of 300, current mean: 0.1337486814192327\n",
            "270 of 300, current mean: 0.13423043302046062\n",
            "280 of 300, current mean: 0.13461421384511602\n",
            "290 of 300, current mean: 0.13610186991073314\n",
            "final bleu is:  0.13689175894347175\n",
            "CPU times: user 20min 16s, sys: 1min 28s, total: 21min 45s\n",
            "Wall time: 11min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qr8Fj15T7lkp",
        "outputId": "4a79ec0a-4caf-4f44-b41a-9c75a678cf71"
      },
      "source": [
        "%%time \n",
        "generates = generate_text(model, glove, test_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, train_y))"
      ],
      "id": "qr8Fj15T7lkp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 300, current mean: 0.0\n",
            "10 of 300, current mean: 0.11751849961784878\n",
            "20 of 300, current mean: 0.12538663665626343\n",
            "30 of 300, current mean: 0.13291820485759656\n",
            "40 of 300, current mean: 0.13801671875477103\n",
            "50 of 300, current mean: 0.1364898668581318\n",
            "60 of 300, current mean: 0.13783356948121334\n",
            "70 of 300, current mean: 0.1429065723530485\n",
            "80 of 300, current mean: 0.14659209554975075\n",
            "90 of 300, current mean: 0.15096345387588156\n",
            "100 of 300, current mean: 0.15266055209327156\n",
            "110 of 300, current mean: 0.14727311055997241\n",
            "120 of 300, current mean: 0.14668877464796012\n",
            "130 of 300, current mean: 0.14313333068298362\n",
            "140 of 300, current mean: 0.14436660420378525\n",
            "150 of 300, current mean: 0.1462144523915425\n",
            "160 of 300, current mean: 0.14436104740095076\n",
            "170 of 300, current mean: 0.14602176452641197\n",
            "180 of 300, current mean: 0.14640573856705205\n",
            "190 of 300, current mean: 0.1483496462912163\n",
            "200 of 300, current mean: 0.14591980627934392\n",
            "210 of 300, current mean: 0.1443720620640844\n",
            "220 of 300, current mean: 0.1427722610519441\n",
            "230 of 300, current mean: 0.1413973714443185\n",
            "240 of 300, current mean: 0.14049419894231743\n",
            "250 of 300, current mean: 0.1385156115588656\n",
            "260 of 300, current mean: 0.136688640066866\n",
            "270 of 300, current mean: 0.1389864447725446\n",
            "280 of 300, current mean: 0.13933916128284424\n",
            "290 of 300, current mean: 0.13972719490037389\n",
            "final bleu is:  0.13954577400131726\n",
            "CPU times: user 20min 18s, sys: 1min 28s, total: 21min 46s\n",
            "Wall time: 11min 26s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeb4b94e-9fb6-4b4d-8a66-1b87cd9f41ec"
      },
      "source": [
        "# GPT2 Baseline\n",
        "Implement GPT2 as a language modelling baseline. GPT-3 is not publicly available and too large for practical purposes. BERT needs modification to work for language modelling, due to the fact that it is trained for bidirectional masked language modelling instead.\n",
        "\n",
        "This section makes use of several tutorials for fine tuning, including:\n",
        "- https://reyfarhan.com/posts/easy-gpt2-finetuning-huggingface/\n",
        "- https://mccormickml.com/2019/07/22/BERT-fine-tuning/#4-train-our-classification-model\n",
        "- https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel (the documentation)\n",
        "- https://huggingface.co/transformers/custom_datasets.html"
      ],
      "id": "aeb4b94e-9fb6-4b4d-8a66-1b87cd9f41ec"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSaFdSit1Kvd",
        "outputId": "1f4d2c83-8c1e-4211-a5be-49900d846ad0"
      },
      "source": [
        "# colab does not have transformers by default\n",
        "!pip install transformers"
      ],
      "id": "uSaFdSit1Kvd",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
            "\r\u001b[K     |▏                               | 10kB 22.5MB/s eta 0:00:01\r\u001b[K     |▎                               | 20kB 29.5MB/s eta 0:00:01\r\u001b[K     |▍                               | 30kB 23.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 40kB 18.1MB/s eta 0:00:01\r\u001b[K     |▊                               | 51kB 16.2MB/s eta 0:00:01\r\u001b[K     |▉                               | 61kB 18.5MB/s eta 0:00:01\r\u001b[K     |█                               | 71kB 14.2MB/s eta 0:00:01\r\u001b[K     |█▏                              | 81kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▎                              | 92kB 13.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 102kB 14.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 112kB 14.1MB/s eta 0:00:01\r\u001b[K     |█▊                              | 122kB 14.1MB/s eta 0:00:01\r\u001b[K     |██                              | 133kB 14.1MB/s eta 0:00:01\r\u001b[K     |██                              | 143kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 153kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▎                             | 163kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 174kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 184kB 14.1MB/s eta 0:00:01\r\u001b[K     |██▊                             | 194kB 14.1MB/s eta 0:00:01\r\u001b[K     |███                             | 204kB 14.1MB/s eta 0:00:01\r\u001b[K     |███                             | 215kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▏                            | 225kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▍                            | 235kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▌                            | 245kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 256kB 14.1MB/s eta 0:00:01\r\u001b[K     |███▉                            | 266kB 14.1MB/s eta 0:00:01\r\u001b[K     |████                            | 276kB 14.1MB/s eta 0:00:01\r\u001b[K     |████                            | 286kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▎                           | 296kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▍                           | 307kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▌                           | 317kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▋                           | 327kB 14.1MB/s eta 0:00:01\r\u001b[K     |████▉                           | 337kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 348kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 358kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 368kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 378kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 389kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 399kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 409kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████                          | 419kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 430kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 440kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 450kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 460kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 471kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 481kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████                         | 491kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 501kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 512kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 522kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 532kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 542kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 552kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████                        | 563kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 573kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 583kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 593kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 604kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 614kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 624kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████                       | 634kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 645kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 655kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 665kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 675kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 686kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 696kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████                      | 706kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 716kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 727kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 737kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 747kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 757kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 768kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████                     | 778kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 788kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 798kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 808kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 819kB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 829kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 839kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████                    | 849kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 860kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 870kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 880kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 890kB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 901kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 911kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 921kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 931kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 942kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 952kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 962kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 972kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 983kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 993kB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 1.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████                | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 1.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 1.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 1.3MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.4MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.5MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.6MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.7MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.8MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.9MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 2.0MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 2.1MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.2MB 14.1MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 2.3MB 14.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 45.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (4.0.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 51.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting huggingface-hub==0.0.8\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.0.8 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.6.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a61aafc-e915-4ff8-b371-3fc32331fd83"
      },
      "source": [
        "# load　GPT, BERT and support materials from huggingface\n",
        "# requires pip install transformers\n",
        "# if in jupyter notebook see here and you get an error mention ipython widgets see here: \n",
        "# https://stackoverflow.com/questions/53247985/tqdm-4-28-1-in-jupyter-notebook-intprogress-not-found-please-update-jupyter-an\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, top_k_top_p_filtering, Trainer, TrainingArguments\n",
        "import torchtext"
      ],
      "id": "9a61aafc-e915-4ff8-b371-3fc32331fd83",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "612a2463-2077-4fb4-867f-f2b928af469d"
      },
      "source": [
        "### Data Processing"
      ],
      "id": "612a2463-2077-4fb4-867f-f2b928af469d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7d485f60-6997-409f-80f2-f4a7a6a6987e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df274afa-c1de-4a7e-96d6-00d81a905e92"
      },
      "source": [
        "train, valid, test = torchtext.datasets.PennTreebank(split=('train', 'valid', 'test')) # len(train) = 4.2w\n",
        "# train, valid, test = torchtext.datasets.WikiText2(split=('train', 'valid', 'test')) # len(train) = 3.6w\n",
        "train = list(train) # these are originally iterators, the data is so small we can just retrieve all of it at once\n",
        "valid = list(valid)\n",
        "test  = list(test)"
      ],
      "id": "7d485f60-6997-409f-80f2-f4a7a6a6987e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ptb.train.txt: 5.10MB [00:00, 102MB/s]                    \n",
            "ptb.valid.txt: 400kB [00:00, 33.1MB/s]                   \n",
            "ptb.test.txt: 450kB [00:00, 39.6MB/s]                   \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42140794-4ad1-4999-b6ab-1bd13ba648d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262,
          "referenced_widgets": [
            "fa0cd16cce0141a6864c352a394a6259",
            "cef0e3c2c11a46c39a0a2345904947e1",
            "d055c42db03a42eeb6c6116a81f8c4c5",
            "e567cd15679a4a0ea7ff734a4710121a",
            "acd51a3c6d4146b1b0b6ea4d5a6fbf04",
            "0ee58b71a9b54beb8fbe7b08b159b4f7",
            "212d581b0c3447969a6cb7277abc60ed",
            "42e38ea3b41f487db77e21fbf839077d",
            "e4e18c5f0fd54c1b9e6a75e97e04a77c",
            "38adc032ae494c178fd9f918c8c84701",
            "5c2d96014e8c4c30a09d259ab01f8b47",
            "17b40c593df74858b309966d3f700382",
            "a06dcdfcdb234a3aa84061d52fcd31b9",
            "5760c2f469eb4839bb9ea31c2c887c3c",
            "fbf2ffe6f338469882c6705da3b0f0ac",
            "b7fdc7711b784666bc1dd3c74db793f0",
            "c569b23db24844e3a1127a21548f2772",
            "1daf7b85fad04cbdbae81a6ef547b2dc",
            "09b0923fbb2e4a7cb18710027160d88f",
            "f9e58c45a51f44189dc791df0f95066f",
            "d54633c6b019425a83eeef0776a93e6d",
            "eb31f0768d9e4efab2c3154ccd9346f2",
            "490f2638725a4ba99cb779ed89b3806e",
            "f3d8210edd174426870ac6eea3a88498",
            "270bf48bfc0d49b68b5bc8d30150fdde",
            "0e933e284f70443b81eb5a9dfdee3992",
            "bb59ddee5efb41239146dd86b286bc99",
            "af4cd932837144539b6f1db240c73858",
            "f5204be9616c4923a888b9ee1c02da53",
            "f8cce387785b4fe881a8660712b893d6",
            "34a039c6df4b4eafa02a24a811315a3c",
            "20396401fe574635b5045c7849c61b09",
            "7beec4b473434c5cb086a104a060ca9f",
            "60369f15ccd1419da0b9893fdb8f237c",
            "6b46e8f9de974d2ba8715a5da39d38ba",
            "3ff7a15320ab4a7ebbcd6cccb044a993",
            "d66e0007a95e4c7a8f573653c13009cd",
            "8feb7fa4e48a4c1db1db49a2c761bb98",
            "4b4c47490f1748a4962d48d435e2077e",
            "8882d09e0cbf43e8a67c7f71a195b2f6"
          ]
        },
        "outputId": "f46c0f30-65ec-488a-95f2-eb5751271ff1"
      },
      "source": [
        "# Download the models\n",
        "# Documentation for GPT: https://huggingface.co/transformers/model_doc/gpt2.html#gpt2lmheadmodel\n",
        "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "gpt_model = GPT2LMHeadModel.from_pretrained('gpt2')"
      ],
      "id": "42140794-4ad1-4999-b6ab-1bd13ba648d4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa0cd16cce0141a6864c352a394a6259",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1042301.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e4e18c5f0fd54c1b9e6a75e97e04a77c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=456318.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c569b23db24844e3a1127a21548f2772",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1355256.0, style=ProgressStyle(descript…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "270bf48bfc0d49b68b5bc8d30150fdde",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=665.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7beec4b473434c5cb086a104a060ca9f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=548118077.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb4a79ac-e65d-4082-bc08-22cfeee91848"
      },
      "source": [
        "We can see that the gpt_tokenizer works differently to ours, splitting up names such as 'rudolph' into 'rud' and 'olph' and words such as nonexecutive and british. Hence our perplexity evaluation will have to be slightly different, using gpt_tokenizer to get the ground truth labels"
      ],
      "id": "eb4a79ac-e65d-4082-bc08-22cfeee91848"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ad00057-0e44-4206-b291-971fed2150a3"
      },
      "source": [
        "# Define a dataset class for fine-tuning, it's a generator so we don't have to store the entire dataset in memory\n",
        "class GPT2Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, txt_list, tokenizer, gpt2_type=\"gpt2\", max_length=40):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.input_ids = []\n",
        "        self.attn_masks = []\n",
        "\n",
        "        # Encode all the text, padding and truncuating it along with adding attention masks to get the sequence length the same across all samples\n",
        "        for txt in txt_list:\n",
        "            encodings_dict = tokenizer.encode_plus('<|startoftext|>'+ txt + '<|endoftext|>', truncation=True, max_length=max_length, padding=\"max_length\")\n",
        "            self.input_ids.append(encodings_dict['input_ids'])\n",
        "            self.attn_masks.append(encodings_dict['attention_mask'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # The tutorial use a dictionary format that also stores labels \n",
        "        return_dict = {\"input_ids\": torch.tensor(self.input_ids[idx]),\n",
        "                       \"attention_mask\": torch.tensor(self.attn_masks[idx]), \n",
        "                       \"labels\": torch.tensor(self.input_ids[idx])} \n",
        "        return return_dict"
      ],
      "id": "3ad00057-0e44-4206-b291-971fed2150a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "943bca9e-d14d-4f87-9b50-d62e6c5efe90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85fe220d-7bea-42ba-e2f3-23488324a305"
      },
      "source": [
        "gpt_tokenizer.pad_token = gpt_tokenizer.eos_token # set the pad token\n",
        "gpt_sequence_length = 40 # gpt splits up words into smaller tokens, so the sequence length should be longer\n",
        "train_dataset = GPT2Dataset(train, gpt_tokenizer, max_length=gpt_sequence_length)\n",
        "val_dataset = GPT2Dataset(valid, gpt_tokenizer, max_length=gpt_sequence_length)\n",
        "test_dataset = GPT2Dataset(test, gpt_tokenizer, max_length=gpt_sequence_length)\n",
        "train_dataset[1]"
      ],
      "id": "943bca9e-d14d-4f87-9b50-d62e6c5efe90",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'input_ids': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
              "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
              "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
              "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]),\n",
              " 'labels': tensor([   27,    91,  9688,  1659,  5239,    91,    29, 17748,   260,  1279,\n",
              "          2954,    29,   399,   812,  1468,   481,  4654,   262,  3096,   355,\n",
              "           257, 36196,   721,  8827,  3437,   645,    85,    13,   399,   220,\n",
              "           198, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16f1ea2b-d89b-4638-bcb2-cf63e7d71211"
      },
      "source": [
        "### Fine Tuning\n",
        "\n",
        "Do fine tuning of the gpt_model using the hugging face out of the box trainer https://huggingface.co/transformers/custom_datasets.html#fine-tuning-with-trainerfrom "
      ],
      "id": "16f1ea2b-d89b-4638-bcb2-cf63e7d71211"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51091355-c64c-4efb-ac21-68232c8e9d29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 918
        },
        "outputId": "caa7dd55-3580-4fe2-e98e-a95f3d56fef5"
      },
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir='gpt_finetuning',     # output directory\n",
        "    num_train_epochs=1,              # total number of training epochs (1 is enough to get very low perplexity and perplexity increases at 2)\n",
        "    per_device_train_batch_size=16,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.001,               # strength of weight decay\n",
        "    logging_dir='gpt_finetuning_logs',            # directory for storing logs\n",
        "    logging_steps=100,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=gpt_model,                         # the instantiated 🤗 Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=val_dataset             # evaluation dataset\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "id": "51091355-c64c-4efb-ac21-68232c8e9d29",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2630' max='2630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2630/2630 07:12, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>5.082100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>2.623600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>2.443200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>2.398500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>2.316800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>2.260200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>2.206100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>2.201100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>2.204600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>2.156500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>2.134400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>2.135500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>2.120400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>2.131800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>2.141600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>2.104100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>2.098900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>2.076400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>2.119600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>2.062400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>2.113100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>2.070700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>2.104700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>2.066200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>2.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>2.107900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2630, training_loss=2.288104583102034, metrics={'train_runtime': 433.1046, 'train_samples_per_second': 6.072, 'total_flos': 61761965506560.0, 'epoch': 1.0, 'init_mem_cpu_alloc_delta': 2092535808, 'init_mem_gpu_alloc_delta': 511148032, 'init_mem_cpu_peaked_delta': 0, 'init_mem_gpu_peaked_delta': 0, 'train_mem_cpu_alloc_delta': -219136000, 'train_mem_gpu_alloc_delta': 1505119232, 'train_mem_cpu_peaked_delta': 239112192, 'train_mem_gpu_peaked_delta': 1470224384})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d00e5ae3-672d-4c81-9695-8e01ad6adad3"
      },
      "source": [
        "### Model Definition\n",
        "Define a wrapper model that can use GPT2 both for standard next word prediction and language generation"
      ],
      "id": "d00e5ae3-672d-4c81-9695-8e01ad6adad3"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3923d864-8141-4525-a915-930e03c08ef2"
      },
      "source": [
        "# Build a model wrapper for gpt2 that uses the \"past\" variable and for language modelling\n",
        "# TODO: add the options for beam search\n",
        "class GPTModel(torch.nn.Module):\n",
        "    def __init__(self, model=None, sequence_length=20):\n",
        "        super().__init__()\n",
        "        self.gpt = model.to(device)\n",
        "        self.tokenizer = gpt_tokenizer\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "    \n",
        "    # output the logits for the most likely next word at each position in the sentence and optionally the hidden states (used for the Neural ODE) \n",
        "    # note input_dataset must be an element taken from a GPT2Dataset class (e.g. train_dataset[0])\n",
        "    def forward(self, input_dataset, output_hidden_states=False):\n",
        "        output = self.gpt.forward(input_ids = input_dataset['input_ids'].to(device), \n",
        "                                  attention_mask=input_dataset['attention_mask'].to(device),\n",
        "                                  use_cache=False,\n",
        "                                  output_hidden_states = output_hidden_states)\n",
        "        if output_hidden_states:\n",
        "            return output[\"hidden_states\"]\n",
        "        return output[\"logits\"]\n",
        "    \n",
        "    # take in a sentence and output the predictions as in forward, but as the most likely sentence not logits\n",
        "    def forward_sentence(self, input_dataset):\n",
        "        preds = self.forward(input_dataset) # TODO: Extrapolation\n",
        "        tokens = torch.argmax(preds, dim=-1)\n",
        "        return self.tokenizer.decode(tokens)\n",
        "    \n",
        "    # generate a sentence by sampling the next word from the probability distribution\n",
        "    # set limit to an integer to generate `limit` number of words instead of ending at a full stop\n",
        "    def random_gen(self, x, limit=None):\n",
        "        # initialize variables\n",
        "        generated = self.tokenizer.encode_plus(x, return_tensors=\"pt\")['input_ids'].to('cuda')\n",
        "        x_len = len(generated[0])\n",
        "        next_token = [generated[0][-1]]\n",
        "        past = None\n",
        "        raw_output= None\n",
        "        stop_list = ['.', '?', '!', '<|endoftext|>']\n",
        "        \n",
        "        # generate until a \".\" is generated\n",
        "        while ((limit is None) or (limit is not None and len(generated[0]) < limit)) and \\\n",
        "        self.tokenizer.decode(next_token[0]) not in stop_list:\n",
        "            # get output of model, using past if available\n",
        "            if past is None:\n",
        "                raw_output = self.gpt(generated, past_key_values=past)\n",
        "            else:\n",
        "                raw_output = self.gpt(next_token, past_key_values=past)\n",
        "            output, past = raw_output['logits'], raw_output['past_key_values']\n",
        "            next_token_logits = output[:, -1, :]\n",
        "            \n",
        "            # sample a token from the top 50 most likely words\n",
        "            filtered_next_token_logits = top_k_top_p_filtering(next_token_logits, top_k=50, top_p=1.0) # filter to the top 50 tokens\n",
        "            probs = torch.nn.functional.softmax(filtered_next_token_logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "            # break loop when the end indicator (stop_list) is reached\n",
        "            if (self.tokenizer.decode(next_token[0]) in stop_list):\n",
        "                break\n",
        "            generated = torch.cat([generated, next_token], dim=-1)\n",
        "\n",
        "        full_output = self.tokenizer.decode(generated[0])\n",
        "        gen_output = self.tokenizer.decode(generated[0][x_len:])\n",
        "      \n",
        "        return full_output, gen_output\n",
        "    \n",
        "    # do beam_search to find the most likely sentence\n",
        "    def beam_search(self, x, beam=5): \n",
        "        input_ids = self.tokenizer.encode_plus(x, return_tensors=\"pt\")['input_ids'].to('cuda')\n",
        "        generated = self.gpt.generate(input_ids=input_ids, num_beams=beam)\n",
        "        full_output = self.tokenizer.decode(generated[0], skip_special_tokens=True).strip()\n",
        "        gen_output = self.tokenizer.decode(generated[0][len(input_ids[0]):], skip_special_tokens=True).strip()\n",
        "        return full_output, gen_output"
      ],
      "id": "3923d864-8141-4525-a915-930e03c08ef2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a88b3953-f775-4979-940e-7733c5240072",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4bdad9-5980-4f81-eb29-99e8009e6135"
      },
      "source": [
        "modelgpt = GPTModel(model=gpt_model, sequence_length=gpt_sequence_length)\n",
        "modelgpt.eval()\n",
        "modelgpt.forward(train_dataset[0])"
      ],
      "id": "a88b3953-f775-4979-940e-7733c5240072",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-31.7897, -30.7168, -32.1723,  ..., -39.5315, -39.7644, -31.6574],\n",
              "        [-63.3438, -60.9143, -61.4364,  ..., -72.3313, -72.0760, -62.4938],\n",
              "        [-53.8140, -53.4152, -53.4254,  ..., -63.6133, -62.0699, -54.8247],\n",
              "        ...,\n",
              "        [-73.2818, -73.2793, -74.6490,  ..., -78.8852, -78.8792, -71.2035],\n",
              "        [-85.5154, -84.4664, -85.6136,  ..., -92.3424, -91.5167, -83.2108],\n",
              "        [-67.5895, -67.4203, -67.9004,  ..., -74.4833, -74.5151, -65.2906]],\n",
              "       device='cuda:0', grad_fn=<MmBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AWsf5miVIN0",
        "outputId": "810ec78f-8be5-4eef-ca04-8ec6a236364b"
      },
      "source": [
        "summary(modelgpt)"
      ],
      "id": "9AWsf5miVIN0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "======================================================================\n",
              "Layer (type:depth-idx)                        Param #\n",
              "======================================================================\n",
              "GPTModel                                      --\n",
              "├─GPT2LMHeadModel: 1-1                        --\n",
              "│    └─GPT2Model: 2-1                         --\n",
              "│    │    └─Embedding: 3-1                    38,597,376\n",
              "│    │    └─Embedding: 3-2                    786,432\n",
              "│    │    └─Dropout: 3-3                      --\n",
              "│    │    └─ModuleList: 3-4                   85,054,464\n",
              "│    │    └─LayerNorm: 3-5                    1,536\n",
              "│    └─Linear: 2-2                            38,597,376\n",
              "======================================================================\n",
              "Total params: 163,037,184\n",
              "Trainable params: 163,037,184\n",
              "Non-trainable params: 0\n",
              "======================================================================"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "164f4366-5d71-4ad9-85b7-d9b6b5dd9964"
      },
      "source": [
        "View the next word output for a single example"
      ],
      "id": "164f4366-5d71-4ad9-85b7-d9b6b5dd9964"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e66430c0-a4d8-4549-9861-7684003c3a78"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "e66430c0-a4d8-4549-9861-7684003c3a78"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d9b5a30-3012-4bcf-b3a9-768fdbaac8f7"
      },
      "source": [
        "View the ability to generate without teacher forcing using the random_gen() function"
      ],
      "id": "5d9b5a30-3012-4bcf-b3a9-768fdbaac8f7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3xmJv7lL1RF"
      },
      "source": [
        "#### Force Teaching"
      ],
      "id": "l3xmJv7lL1RF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2996c849-0809-4fbe-8f16-211cdbbb1a48"
      },
      "source": [
        "# The formula for calculating perplexity in language models can be found here: https://web.stanford.edu/~jurafsky/slp3/3.pdf (page 8)\n",
        "# An interesting detail is that the geometric mean of perplexity from each word is used\n",
        "# if the mask is 0 at index i don't use the value at index i to calculate perplexity\n",
        "def perplexity_gpt(preds, ground_truth, mask, epsilon=1e-30):\n",
        "    probs = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        if mask[i] != 0:\n",
        "            probs.append(preds[i, int(ground_truth[i])])\n",
        "    probs = np.array(probs)\n",
        "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
        "    return np.prod(probs).detach().cpu().numpy()\n",
        "\n",
        "# Can optionally define n_samples=int to limit the number of samples used for perplexity evaluation\n",
        "def average_perplexity_gpt(model, train, n_samples=None, print_results=False):\n",
        "    perplexities = []\n",
        "    n_samples = len(train) if n_samples is None else n_samples\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_samples):\n",
        "            # Compute perplexity for a single sample\n",
        "            labels = train[i]['input_ids'][1:]\n",
        "            mask = train[i]['attention_mask'][:-1]\n",
        "            preds = model.forward(train[i])[:-1] # remove the last prediction as there is no ground truth \n",
        "            preds = torch.nn.functional.softmax(preds, dim=-1)\n",
        "            perplexities.append(perplexity_gpt(preds[6:], labels[6:], mask[6:])) # remove the first 7 tokens that represent \"<|startoftext|>\"\n",
        "\n",
        "            if i % 100 == 0 and print_results:\n",
        "                print(\"Sentences analysed: {} Average perplexity: {}\".format(i, np.mean(perplexities)))\n",
        "    return np.mean(perplexities)\n",
        "\n",
        "\n",
        "# straight calculation of BLEU\n",
        "def average_bleu_gpt(model, train):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    n_samples = len(train)\n",
        "    with torch.no_grad():\n",
        "        for i in range(n_samples):\n",
        "            mask = train[i]['attention_mask'][:-1][6:].tolist()\n",
        "\n",
        "            reference_withmask = train[i]['input_ids'][1:][6:].tolist()\n",
        "            preds = torch.nn.functional.softmax(modelgpt.forward(train[i])[:-1], dim=-1)\n",
        "            candidate_withmask = np.argmax(preds[6:].cpu(), axis=1).tolist()\n",
        "\n",
        "            reference = [reference_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "            candidate = [candidate_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "\n",
        "            bleus.append(sentence_bleu([reference], candidate, smoothing_function=smoothie))\n",
        "    return np.mean(bleus)"
      ],
      "id": "2996c849-0809-4fbe-8f16-211cdbbb1a48",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "7f850a00-c2d8-4efd-827d-e026b698c38b"
      },
      "source": [
        "%%time\n",
        "average_perplexity_gpt(modelgpt, train_dataset, n_samples=None, print_results=False)"
      ],
      "id": "7f850a00-c2d8-4efd-827d-e026b698c38b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "236a0ec9-a3bc-4704-865b-4cfba976c022",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b759ac9c-3401-42b9-e64e-33e67ba4a6e5"
      },
      "source": [
        "average_perplexity_gpt(modelgpt, val_dataset, n_samples=None, print_results=False)"
      ],
      "id": "236a0ec9-a3bc-4704-865b-4cfba976c022",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1219.9313"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14803271-64e3-4580-b468-65f8605cb0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58ff69c5-bce9-428f-b207-9a8880236f0b"
      },
      "source": [
        "average_perplexity_gpt(modelgpt, test_dataset, n_samples=None, print_results=False)"
      ],
      "id": "14803271-64e3-4580-b468-65f8605cb0d6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "23.9871"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LdZVPZ6sWUP7"
      },
      "source": [
        "%%time\n",
        "average_bleu_gpt(modelgpt, train_dataset)"
      ],
      "id": "LdZVPZ6sWUP7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AM489djuWZ8-",
        "outputId": "550f8623-fd91-42c0-b05a-56d8d1831af0"
      },
      "source": [
        "average_bleu_gpt(modelgpt, val_dataset)"
      ],
      "id": "AM489djuWZ8-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.225191197779219"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXjpkbk0OqQG",
        "outputId": "a852aad4-5650-40d6-8936-127970367591"
      },
      "source": [
        "average_bleu_gpt(modelgpt, test_dataset)"
      ],
      "id": "VXjpkbk0OqQG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22589598064165836"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5JhbqcQ4L6sA"
      },
      "source": [
        "#### Generate Text"
      ],
      "id": "5JhbqcQ4L6sA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSY5aUeqcw_M"
      },
      "source": [
        "##### Demo+Methods"
      ],
      "id": "CSY5aUeqcw_M"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-On240YvMBj1",
        "outputId": "80dc70f1-aea4-4a29-c025-b0a1ff8607bb"
      },
      "source": [
        "%%time\n",
        "# Still use the dataset for LSTM\n",
        "idx = 57\n",
        "o_sent = train[idx].strip()\n",
        "print('original sentence:\\n\\t', o_sent)\n",
        "\n",
        "gen_len = len(o_sent.split(' '))\n",
        "preceed_len = 5\n",
        "\n",
        "if gen_len == preceed_len:\n",
        "    print('should skip')\n",
        "\n",
        "input = ' '.join(o_sent.split(' ')[:preceed_len])\n",
        "\n",
        "print('input sentence:\\n\\t', input)\n",
        "\n",
        "full_sent, gen_sent = modelgpt.beam_search(input, beam=5)\n",
        "\n",
        "print('full sentence:\\n\\t', full_sent)\n",
        "print('generated part:\\n\\t', gen_sent)\n",
        "\n",
        "smoothie = SmoothingFunction().method4\n",
        "if len(gen_sent) == 0: pass\n",
        "else:\n",
        "    print('The BLEU score is: ', sentence_bleu([o_sent], gen_sent, smoothing_function=smoothie))"
      ],
      "id": "-On240YvMBj1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original sentence:\n",
            "\t <unk> is an italian state-owned holding company with interests in the mechanical engineering industry\n",
            "input sentence:\n",
            "\t <unk> is an italian state-owned\n",
            "full sentence:\n",
            "\t <unk> is an italian state-owned <unk> company based in <unk> <\n",
            "generated part:\n",
            "\t <unk> company based in <unk> <\n",
            "The BLEU score is:  0.052178861809722275\n",
            "CPU times: user 133 ms, sys: 0 ns, total: 133 ms\n",
            "Wall time: 132 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgX5ex53W2IZ"
      },
      "source": [
        "def gpt_gen_bleu(modelgpt, dataset, beam=5, preceed_len = 5, max_instances = 1000):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    for idx in range(len(dataset)):\n",
        "        if idx + 1 > max_instances: break\n",
        "        o_sent = dataset[idx] # original sent\n",
        "        gen_len = len(train[idx].split(' '))\n",
        "        if gen_len == preceed_len: continue\n",
        "        \n",
        "        i_sent = ' '.join(train[idx].split(' ')[:preceed_len]) # input sent\n",
        "        if beam <= 1:\n",
        "            __, gen_sent = modelgpt.random_gen(i_sent, gen_len)\n",
        "        else:\n",
        "            __, gen_sent = modelgpt.beam_search(i_sent, beam=beam)\n",
        "        if len(gen_sent) == 0: continue\n",
        "\n",
        "        bleus.append(sentence_bleu([o_sent], gen_sent, smoothing_function=smoothie))\n",
        "\n",
        "        if idx % 50 == 0:\n",
        "            print('{} of {}, current mean BLEU: {}'.format(idx, min(len(dataset), max_instances), np.mean(bleus)))\n",
        "    return np.mean(bleus)"
      ],
      "id": "sgX5ex53W2IZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uNjzaD1hfU-Q"
      },
      "source": [
        "##### Metric Testing"
      ],
      "id": "uNjzaD1hfU-Q"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aR4cxQE1ttCl"
      },
      "source": [
        "import logging\n",
        "from logging import DEBUG, INFO, WARNING, ERROR, CRITICAL, NOTSET\n",
        "\n",
        "logging.disable(level=WARNING)"
      ],
      "id": "aR4cxQE1ttCl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qq8BVntFqsyp"
      },
      "source": [
        "%%time \n",
        "# score of beam search is relatively low\n",
        "train_bleu = gpt_gen_bleu(modelgpt, train, beam=5)\n",
        "\n",
        "print('finished ! Final mean BLEU is: ', train_bleu) "
      ],
      "id": "qq8BVntFqsyp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "hNKWhvzzdWJb",
        "outputId": "e0a2ac63-54d8-4774-d0c8-ca6366c0f7b5"
      },
      "source": [
        "%%time\n",
        "train_bleu = gpt_gen_bleu(modelgpt, train,  beam=0)\n",
        "\n",
        "print('finished ! Final mean BLEU is: ', train_bleu)"
      ],
      "id": "hNKWhvzzdWJb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean BLEU: 0.010071651377979756\n",
            "50 of 2000, current mean BLEU: 0.11911115400946945\n",
            "100 of 2000, current mean BLEU: 0.12949902722881348\n",
            "150 of 2000, current mean BLEU: 0.1361311873990362\n",
            "200 of 2000, current mean BLEU: 0.13587786469857652\n",
            "250 of 2000, current mean BLEU: 0.13458132013703467\n",
            "300 of 2000, current mean BLEU: 0.1331856057372505\n",
            "350 of 2000, current mean BLEU: 0.13197857953433884\n",
            "400 of 2000, current mean BLEU: 0.13303948249875258\n",
            "450 of 2000, current mean BLEU: 0.13423432690039477\n",
            "500 of 2000, current mean BLEU: 0.1353561917592508\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-237-c332be7dedb5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"train_bleu = gpt_gen_bleu(modelgpt, train,  beam=0)\\n\\nprint('finished ! Final mean BLEU is: ', train_bleu)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-236-b90a19d6371c>\u001b[0m in \u001b[0;36mgpt_gen_bleu\u001b[0;34m(modelgpt, dataset, beam, preceed_len, max_instances)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mi_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreceed_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbeam\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-231-7ddf384ae777>\u001b[0m in \u001b[0;36mrandom_gen\u001b[0;34m(self, x, limit)\u001b[0m\n\u001b[1;32m     43\u001b[0m                 \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                 \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logits'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'past_key_values'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mnext_token_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m         )\n\u001b[1;32m    956\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 )\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 698
        },
        "id": "rwAfy4iJwS7v",
        "outputId": "ee882be6-5fcb-4f9e-dad6-90ab2a4036a2"
      },
      "source": [
        "valid_bleu = gpt_gen_bleu(modelgpt, valid, beam=0)\n",
        "\n",
        "print('finished ! Final mean BLEU is: ', valid_bleu)"
      ],
      "id": "rwAfy4iJwS7v",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean BLEU: 0.08904865917278683\n",
            "50 of 2000, current mean BLEU: 0.09975866581086114\n",
            "100 of 2000, current mean BLEU: 0.09040492859127659\n",
            "150 of 2000, current mean BLEU: 0.09374448971734457\n",
            "200 of 2000, current mean BLEU: 0.09725366946714924\n",
            "250 of 2000, current mean BLEU: 0.09974675554953126\n",
            "300 of 2000, current mean BLEU: 0.09902339383331599\n",
            "350 of 2000, current mean BLEU: 0.10453152342912325\n",
            "400 of 2000, current mean BLEU: 0.10542553255724137\n",
            "450 of 2000, current mean BLEU: 0.10555352520599964\n",
            "500 of 2000, current mean BLEU: 0.10545958585727615\n",
            "550 of 2000, current mean BLEU: 0.10507760399240752\n",
            "600 of 2000, current mean BLEU: 0.10664569754859836\n",
            "650 of 2000, current mean BLEU: 0.1054687718329927\n",
            "700 of 2000, current mean BLEU: 0.10516710104715085\n",
            "750 of 2000, current mean BLEU: 0.10440869068560814\n",
            "800 of 2000, current mean BLEU: 0.10312904842898596\n",
            "850 of 2000, current mean BLEU: 0.10337492575789471\n",
            "900 of 2000, current mean BLEU: 0.10447467954922028\n",
            "950 of 2000, current mean BLEU: 0.10586946145894173\n",
            "1000 of 2000, current mean BLEU: 0.10608288587842478\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-269-ef476fb1c1dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalid_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_gen_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished ! Final mean BLEU is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_bleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-264-3ae602378713>\u001b[0m in \u001b[0;36mgpt_gen_bleu\u001b[0;34m(modelgpt, dataset, beam, preceed_len, max_instances)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mi_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpreceed_len\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# input sent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbeam\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0m__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_sent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelgpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeam_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-258-e4d7a411995b>\u001b[0m in \u001b[0;36mrandom_gen\u001b[0;34m(self, x, limit)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# get output of model, using past if available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpast\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mraw_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    953\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 954\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    955\u001b[0m         )\n\u001b[1;32m    956\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    795\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m                     \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 797\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    798\u001b[0m                 )\n\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         )\n\u001b[1;32m    325\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# output_attn: a, present, (attentions)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, layer_past, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mpresent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_heads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36m_attn\u001b[0;34m(self, query, key, value, attention_mask, head_mask)\u001b[0m\n\u001b[1;32m    198\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattn_weights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4MiroZBNwfnL",
        "outputId": "5b720707-96bf-44dd-f11e-e043437bdcb8"
      },
      "source": [
        "test_bleu = gpt_gen_bleu(modelgpt, test, beam=0)\n",
        "\n",
        "print('finished ! Final mean BLEU is: ', test_bleu)"
      ],
      "id": "4MiroZBNwfnL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 1000, current mean BLEU: 0.056823540398864544\n",
            "50 of 1000, current mean BLEU: 0.11283436320085767\n",
            "100 of 1000, current mean BLEU: 0.11015643863772863\n",
            "150 of 1000, current mean BLEU: 0.11309885491713392\n",
            "200 of 1000, current mean BLEU: 0.1099456656544335\n",
            "250 of 1000, current mean BLEU: 0.10457915745048403\n",
            "300 of 1000, current mean BLEU: 0.10244082494421641\n",
            "350 of 1000, current mean BLEU: 0.10281257821714644\n",
            "400 of 1000, current mean BLEU: 0.10129830353050887\n",
            "450 of 1000, current mean BLEU: 0.10214188575597626\n",
            "500 of 1000, current mean BLEU: 0.10464878356218418\n",
            "550 of 1000, current mean BLEU: 0.10308959588325638\n",
            "600 of 1000, current mean BLEU: 0.10389376244884468\n",
            "650 of 1000, current mean BLEU: 0.10298833875547865\n",
            "700 of 1000, current mean BLEU: 0.10214513158340849\n",
            "750 of 1000, current mean BLEU: 0.10174657798372658\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-272-994569d14f57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_bleu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgpt_gen_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelgpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'finished ! Final mean BLEU is: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_bleu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-271-fb47950e61c2>\u001b[0m in \u001b[0;36mgpt_gen_bleu\u001b[0;34m(modelgpt, dataset, beam, preceed_len, max_instances)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_sent\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbleus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_bleu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mo_sent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_sent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msmoothie\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36msentence_bleu\u001b[0;34m(references, hypothesis, weights, smoothing_function, auto_reweigh, emulate_multibleu)\u001b[0m\n\u001b[1;32m     87\u001b[0m     return corpus_bleu([references], [hypothesis],\n\u001b[1;32m     88\u001b[0m                         \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msmoothing_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauto_reweigh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                         emulate_multibleu)\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mcorpus_bleu\u001b[0;34m(list_of_references, hypotheses, weights, smoothing_function, auto_reweigh, emulate_multibleu)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;31m#       smoothing method allows.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     p_n = smoothing_function(p_n, references=references, hypothesis=hypothesis,\n\u001b[0;32m--> 199\u001b[0;31m                              hyp_len=hyp_len, emulate_multibleu=emulate_multibleu)\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mbp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/nltk/translate/bleu_score.py\u001b[0m in \u001b[0;36mmethod4\u001b[0;34m(self, p_n, references, hypothesis, hyp_len, *args, **kwargs)\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mp_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumerator\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhyp_len\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 544\u001b[0;31m                 \u001b[0mincvnt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyp_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Note that this K is different from the K from NIST.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    545\u001b[0m                 \u001b[0mp_n\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mincvnt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mp_n\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMwRUOSYaHVB"
      },
      "source": [
        "# NeuralDE-LSTM"
      ],
      "id": "SMwRUOSYaHVB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jb9p5ogzaLqd"
      },
      "source": [
        "class ODELSTM(torch.nn.Module):\n",
        "    def __init__(self, vocab_size, input_size=100, layer_size=100, dropout=0):\n",
        "        super().__init__()\n",
        "        self.LSTM = torch.nn.LSTM(input_size, layer_size, 1, bidirectional=False, dropout=dropout)\n",
        "        self.linear = torch.nn.Linear(layer_size, vocab_size)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "        # Define the derivative function\n",
        "        f = torch.nn.Sequential(\n",
        "            torch.nn.Linear(layer_size, layer_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(layer_size, layer_size),\n",
        "        )\n",
        "\n",
        "        self.node = NeuralDE(f, sensitivity='adjoint', solver='dopri5')\n",
        "        self.timesteps = torch.arange(0, 20, 1, device=device).float() # define the number of output items of the Neural ODE\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # convert words to their vectors here\n",
        "        senquence_output , hidden_state = self.LSTM(x)\n",
        "        output = self.node.trajectory(senquence_output, self.timesteps)\n",
        "        \n",
        "        pred = self.linear(senquence_output)\n",
        "        return pred\n",
        "    \n",
        "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
        "    def predict(self, x):\n",
        "        preds = self.forward(x)\n",
        "        preds = self.softmax(preds).detach().cpu().numpy()\n",
        "        return preds"
      ],
      "id": "jb9p5ogzaLqd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lviMpGuh-Q3",
        "outputId": "9f74a947-69f1-4c14-874d-37208d059e47"
      },
      "source": [
        "%%time\n",
        "\n",
        "odeLSTM = ODELSTM(vocab_size, input_size=300, layer_size=300)\n",
        "odeLSTM.to(device)\n",
        "odeLSTM.eval()\n",
        "\n",
        "# unit test to check that forward propagation works\n",
        "data = numpy_to_tensor(train_X[1:2])\n",
        "print('shape of final output is: ', odeLSTM.forward(data).shape)"
      ],
      "id": "7lviMpGuh-Q3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shape of final output is:  torch.Size([1, 20, 10001])\n",
            "CPU times: user 93.3 ms, sys: 0 ns, total: 93.3 ms\n",
            "Wall time: 144 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VnT63XJhx67",
        "outputId": "b77e63cc-ecde-4a54-8308-e99b00afdb6b"
      },
      "source": [
        "del data\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "summary(odeLSTM)"
      ],
      "id": "7VnT63XJhx67",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "ODELSTM                                  --\n",
              "├─LSTM: 1-1                              722,400\n",
              "├─Linear: 1-2                            3,010,301\n",
              "├─Softmax: 1-3                           --\n",
              "├─NeuralDE: 1-4                          --\n",
              "│    └─DEFunc: 2-1                       --\n",
              "│    │    └─Sequential: 3-1              180,600\n",
              "│    └─Adjoint: 2-2                      --\n",
              "=================================================================\n",
              "Total params: 3,913,301\n",
              "Trainable params: 3,913,301\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 276
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBNYT1Et-nAF"
      },
      "source": [
        "## Training"
      ],
      "id": "xBNYT1Et-nAF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir32KVS3-mLO"
      },
      "source": [
        "# training the model\n",
        "def train_odelstm(model, train_X, train_y, epochs=10, learn_rate=0.01, weight_decay=0.001):\n",
        "    # Prepare data\n",
        "    X = numpy_to_tensor(train_X)\n",
        "    y = numpy_to_tensor(train_y).long()[:, :, 0]\n",
        "    n_samples = X.shape[0]\n",
        "    \n",
        "    # Define loss and optimizer\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learn_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Ensure this runs on gpu\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        start_t2 = time()\n",
        "        \n",
        "        # shuffle the data\n",
        "        new_indices = torch.randperm(n_samples)\n",
        "        X = X[new_indices, :, :] \n",
        "        y = y[new_indices, :]\n",
        "        \n",
        "        for idx in range(n_samples):\n",
        "            # forward + backward + optimize\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(X[idx:idx+1]) \n",
        "            outputs = torch.swapaxes(outputs, 1, 2) # cross entropy expects a tensor of (n_samples, n_outputs, sequence_length)\n",
        "            loss = criterion(outputs, y[idx:idx+1])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if idx % 500 == 0:\n",
        "            # print loss every 500 steps (perplexity takes too much time)\n",
        "                print(\"Epoch {}: index {} of {} - Current loss: {:.2f}, Time Taken: {:.2f} secs\"\n",
        "                .format(epoch, idx, n_samples, loss.item(), time() - start_t2))\n",
        "    \n",
        "    del X\n",
        "    del y\n",
        "    torch.cuda.empty_cache()\n",
        "    print('Finished Training!')\n",
        "    return model"
      ],
      "id": "ir32KVS3-mLO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biPOOjZUAfv8",
        "outputId": "768a26bb-0b8a-4e24-a326-b00f2d435ed4"
      },
      "source": [
        "%%time\n",
        "odeLSTM = train_odelstm(odeLSTM, train_X, train_y, epochs=1, learn_rate=1e-3, weight_decay=1e-5)"
      ],
      "id": "biPOOjZUAfv8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: index 0 of 42068 - Current loss: 9.23, Time Taken: 0.07 secs\n",
            "Epoch 0: index 500 of 42068 - Current loss: 6.62, Time Taken: 24.09 secs\n",
            "Epoch 0: index 1000 of 42068 - Current loss: 6.45, Time Taken: 47.97 secs\n",
            "Epoch 0: index 1500 of 42068 - Current loss: 6.89, Time Taken: 71.75 secs\n",
            "Epoch 0: index 2000 of 42068 - Current loss: 7.48, Time Taken: 95.50 secs\n",
            "Epoch 0: index 2500 of 42068 - Current loss: 5.52, Time Taken: 119.37 secs\n",
            "Epoch 0: index 3000 of 42068 - Current loss: 2.62, Time Taken: 143.16 secs\n",
            "Epoch 0: index 3500 of 42068 - Current loss: 5.36, Time Taken: 166.92 secs\n",
            "Epoch 0: index 4000 of 42068 - Current loss: 5.69, Time Taken: 190.74 secs\n",
            "Epoch 0: index 4500 of 42068 - Current loss: 6.42, Time Taken: 214.61 secs\n",
            "Epoch 0: index 5000 of 42068 - Current loss: 6.83, Time Taken: 238.38 secs\n",
            "Epoch 0: index 5500 of 42068 - Current loss: 6.12, Time Taken: 262.04 secs\n",
            "Epoch 0: index 6000 of 42068 - Current loss: 6.13, Time Taken: 285.64 secs\n",
            "Epoch 0: index 6500 of 42068 - Current loss: 5.87, Time Taken: 309.37 secs\n",
            "Epoch 0: index 7000 of 42068 - Current loss: 6.78, Time Taken: 332.88 secs\n",
            "Epoch 0: index 7500 of 42068 - Current loss: 5.96, Time Taken: 356.39 secs\n",
            "Epoch 0: index 8000 of 42068 - Current loss: 6.68, Time Taken: 380.06 secs\n",
            "Epoch 0: index 8500 of 42068 - Current loss: 1.91, Time Taken: 403.51 secs\n",
            "Epoch 0: index 9000 of 42068 - Current loss: 3.94, Time Taken: 427.03 secs\n",
            "Epoch 0: index 9500 of 42068 - Current loss: 7.38, Time Taken: 450.61 secs\n",
            "Epoch 0: index 10000 of 42068 - Current loss: 6.03, Time Taken: 473.91 secs\n",
            "Epoch 0: index 10500 of 42068 - Current loss: 3.55, Time Taken: 497.32 secs\n",
            "Epoch 0: index 11000 of 42068 - Current loss: 2.97, Time Taken: 520.65 secs\n",
            "Epoch 0: index 11500 of 42068 - Current loss: 5.25, Time Taken: 544.01 secs\n",
            "Epoch 0: index 12000 of 42068 - Current loss: 5.17, Time Taken: 567.32 secs\n",
            "Epoch 0: index 12500 of 42068 - Current loss: 6.08, Time Taken: 590.69 secs\n",
            "Epoch 0: index 13000 of 42068 - Current loss: 5.72, Time Taken: 614.09 secs\n",
            "Epoch 0: index 13500 of 42068 - Current loss: 7.18, Time Taken: 637.54 secs\n",
            "Epoch 0: index 14000 of 42068 - Current loss: 2.53, Time Taken: 660.89 secs\n",
            "Epoch 0: index 14500 of 42068 - Current loss: 4.32, Time Taken: 684.34 secs\n",
            "Epoch 0: index 15000 of 42068 - Current loss: 6.16, Time Taken: 707.76 secs\n",
            "Epoch 0: index 15500 of 42068 - Current loss: 6.11, Time Taken: 731.16 secs\n",
            "Epoch 0: index 16000 of 42068 - Current loss: 7.60, Time Taken: 754.54 secs\n",
            "Epoch 0: index 16500 of 42068 - Current loss: 4.58, Time Taken: 778.54 secs\n",
            "Epoch 0: index 17000 of 42068 - Current loss: 4.68, Time Taken: 803.40 secs\n",
            "Epoch 0: index 17500 of 42068 - Current loss: 5.86, Time Taken: 826.88 secs\n",
            "Epoch 0: index 18000 of 42068 - Current loss: 1.17, Time Taken: 850.40 secs\n",
            "Epoch 0: index 18500 of 42068 - Current loss: 4.01, Time Taken: 874.09 secs\n",
            "Epoch 0: index 19000 of 42068 - Current loss: 5.83, Time Taken: 897.53 secs\n",
            "Epoch 0: index 19500 of 42068 - Current loss: 4.97, Time Taken: 920.87 secs\n",
            "Epoch 0: index 20000 of 42068 - Current loss: 2.79, Time Taken: 944.35 secs\n",
            "Epoch 0: index 20500 of 42068 - Current loss: 3.78, Time Taken: 967.66 secs\n",
            "Epoch 0: index 21000 of 42068 - Current loss: 3.14, Time Taken: 991.04 secs\n",
            "Epoch 0: index 21500 of 42068 - Current loss: 5.40, Time Taken: 1014.60 secs\n",
            "Epoch 0: index 22000 of 42068 - Current loss: 3.48, Time Taken: 1038.05 secs\n",
            "Epoch 0: index 22500 of 42068 - Current loss: 5.54, Time Taken: 1062.17 secs\n",
            "Epoch 0: index 23000 of 42068 - Current loss: 4.25, Time Taken: 1086.61 secs\n",
            "Epoch 0: index 23500 of 42068 - Current loss: 5.38, Time Taken: 1111.24 secs\n",
            "Epoch 0: index 24000 of 42068 - Current loss: 5.67, Time Taken: 1135.51 secs\n",
            "Epoch 0: index 24500 of 42068 - Current loss: 1.68, Time Taken: 1159.72 secs\n",
            "Epoch 0: index 25000 of 42068 - Current loss: 4.09, Time Taken: 1183.80 secs\n",
            "Epoch 0: index 25500 of 42068 - Current loss: 2.68, Time Taken: 1207.64 secs\n",
            "Epoch 0: index 26000 of 42068 - Current loss: 5.62, Time Taken: 1230.88 secs\n",
            "Epoch 0: index 26500 of 42068 - Current loss: 4.43, Time Taken: 1254.19 secs\n",
            "Epoch 0: index 27000 of 42068 - Current loss: 4.49, Time Taken: 1277.69 secs\n",
            "Epoch 0: index 27500 of 42068 - Current loss: 1.06, Time Taken: 1300.97 secs\n",
            "Epoch 0: index 28000 of 42068 - Current loss: 5.72, Time Taken: 1324.37 secs\n",
            "Epoch 0: index 28500 of 42068 - Current loss: 1.79, Time Taken: 1347.90 secs\n",
            "Epoch 0: index 29000 of 42068 - Current loss: 7.02, Time Taken: 1371.38 secs\n",
            "Epoch 0: index 29500 of 42068 - Current loss: 5.43, Time Taken: 1394.67 secs\n",
            "Epoch 0: index 30000 of 42068 - Current loss: 5.54, Time Taken: 1418.29 secs\n",
            "Epoch 0: index 30500 of 42068 - Current loss: 1.48, Time Taken: 1441.70 secs\n",
            "Epoch 0: index 31000 of 42068 - Current loss: 6.71, Time Taken: 1465.17 secs\n",
            "Epoch 0: index 31500 of 42068 - Current loss: 6.19, Time Taken: 1488.61 secs\n",
            "Epoch 0: index 32000 of 42068 - Current loss: 2.58, Time Taken: 1512.04 secs\n",
            "Epoch 0: index 32500 of 42068 - Current loss: 4.63, Time Taken: 1535.51 secs\n",
            "Epoch 0: index 33000 of 42068 - Current loss: 4.22, Time Taken: 1558.83 secs\n",
            "Epoch 0: index 33500 of 42068 - Current loss: 6.84, Time Taken: 1582.34 secs\n",
            "Epoch 0: index 34000 of 42068 - Current loss: 5.54, Time Taken: 1605.87 secs\n",
            "Epoch 0: index 34500 of 42068 - Current loss: 2.99, Time Taken: 1629.37 secs\n",
            "Epoch 0: index 35000 of 42068 - Current loss: 1.25, Time Taken: 1652.75 secs\n",
            "Epoch 0: index 35500 of 42068 - Current loss: 5.35, Time Taken: 1676.13 secs\n",
            "Epoch 0: index 36000 of 42068 - Current loss: 5.20, Time Taken: 1699.56 secs\n",
            "Epoch 0: index 36500 of 42068 - Current loss: 3.96, Time Taken: 1722.86 secs\n",
            "Epoch 0: index 37000 of 42068 - Current loss: 5.15, Time Taken: 1746.44 secs\n",
            "Epoch 0: index 37500 of 42068 - Current loss: 1.91, Time Taken: 1770.18 secs\n",
            "Epoch 0: index 38000 of 42068 - Current loss: 3.94, Time Taken: 1793.51 secs\n",
            "Epoch 0: index 38500 of 42068 - Current loss: 6.42, Time Taken: 1817.03 secs\n",
            "Epoch 0: index 39000 of 42068 - Current loss: 2.30, Time Taken: 1840.54 secs\n",
            "Epoch 0: index 39500 of 42068 - Current loss: 5.45, Time Taken: 1863.91 secs\n",
            "Epoch 0: index 40000 of 42068 - Current loss: 1.43, Time Taken: 1887.36 secs\n",
            "Epoch 0: index 40500 of 42068 - Current loss: 3.18, Time Taken: 1910.75 secs\n",
            "Epoch 0: index 41000 of 42068 - Current loss: 5.80, Time Taken: 1933.99 secs\n",
            "Epoch 0: index 41500 of 42068 - Current loss: 6.40, Time Taken: 1957.32 secs\n",
            "Epoch 0: index 42000 of 42068 - Current loss: 2.50, Time Taken: 1980.58 secs\n",
            "Finished Training!\n",
            "CPU times: user 33min 3s, sys: 3.13 s, total: 33min 7s\n",
            "Wall time: 33min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvakHU_7F8MV"
      },
      "source": [
        "## Evaluation"
      ],
      "id": "cvakHU_7F8MV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxX5sgDkyc4J"
      },
      "source": [
        "### Force Teaching"
      ],
      "id": "HxX5sgDkyc4J"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaLh8uEGF3Mj",
        "outputId": "fa036c93-081c-4eec-c52c-59db96511a77"
      },
      "source": [
        "%%time\n",
        "odeLSTM.eval()\n",
        "print('Train perplexity is ', average_perplexity(odeLSTM, train_X, train_y))"
      ],
      "id": "vaLh8uEGF3Mj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train perplexity is  232.5228\n",
            "CPU times: user 30min 4s, sys: 5.62 s, total: 30min 10s\n",
            "Wall time: 30min 6s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQvfy7FZYibF",
        "outputId": "a0cf742d-7c3f-4ffc-a226-40e024b443c2"
      },
      "source": [
        "average_perplexity(odeLSTM, valid_X, valid_y)"
      ],
      "id": "HQvfy7FZYibF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276.98474"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAq6QSt7Yi7a",
        "outputId": "bb0c668f-c5cd-450a-8485-70b35b808545"
      },
      "source": [
        "average_perplexity(odeLSTM, test_X, test_y)"
      ],
      "id": "hAq6QSt7Yi7a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "253.82364"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIZlbyfMGAEF",
        "outputId": "a25cf138-2344-4c0c-a0b5-98c012f6440c"
      },
      "source": [
        "%%time\n",
        "average_bleu(odeLSTM, train_X, train_y)"
      ],
      "id": "vIZlbyfMGAEF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 30min 27s, sys: 4.86 s, total: 30min 32s\n",
            "Wall time: 30min 29s\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2962042315776427"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxzgIRAYYhGk",
        "outputId": "d11ef272-fbcf-4544-8626-3b25d158e887"
      },
      "source": [
        "average_bleu(odeLSTM, valid_X, valid_y)"
      ],
      "id": "vxzgIRAYYhGk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22759942516244355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rp3XHWzpY1U9",
        "outputId": "10a47322-7146-4046-95df-dee8a0923153"
      },
      "source": [
        "average_bleu(odeLSTM, test_X, test_y)"
      ],
      "id": "Rp3XHWzpY1U9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.22526977732762332"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx7RWc9hygFo"
      },
      "source": [
        "### Generation"
      ],
      "id": "Vx7RWc9hygFo"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "voWokwNiyiu9",
        "outputId": "6b195fae-00ab-4859-c705-e665fef5117a"
      },
      "source": [
        "%%time \n",
        "gen_num = 100 \n",
        "generate_length = 20\n",
        "generates = generate_text(odeLSTM, glove, train_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, train_y))"
      ],
      "id": "voWokwNiyiu9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating texts...\n",
            "0 of 100 generates.\n",
            "10 of 100 generates.\n",
            "20 of 100 generates.\n",
            "30 of 100 generates.\n",
            "40 of 100 generates.\n",
            "50 of 100 generates.\n",
            "60 of 100 generates.\n",
            "70 of 100 generates.\n",
            "80 of 100 generates.\n",
            "90 of 100 generates.\n",
            "0 of 100, current mean: 0.0\n",
            "10 of 100, current mean: 0.08578610427913255\n",
            "20 of 100, current mean: 0.12926636671074815\n",
            "30 of 100, current mean: 0.12609363790669073\n",
            "40 of 100, current mean: 0.12362938795424462\n",
            "50 of 100, current mean: 0.11152848683633648\n",
            "60 of 100, current mean: 0.11669757477666057\n",
            "70 of 100, current mean: 0.11780536079504665\n",
            "80 of 100, current mean: 0.11724413601320274\n",
            "90 of 100, current mean: 0.11979319907791623\n",
            "final bleu is:  0.12317864915106946\n",
            "CPU times: user 10min 34s, sys: 25.3 s, total: 10min 59s\n",
            "Wall time: 8min 25s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5YFihl99TVs",
        "outputId": "1ccb3caf-5c41-48fe-fa76-bfbce28ddf3f"
      },
      "source": [
        "generates = generate_text(odeLSTM, glove, valid_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, valid_y))"
      ],
      "id": "X5YFihl99TVs",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating texts...\n",
            "0 of 100 generates.\n",
            "10 of 100 generates.\n",
            "20 of 100 generates.\n",
            "30 of 100 generates.\n",
            "40 of 100 generates.\n",
            "50 of 100 generates.\n",
            "60 of 100 generates.\n",
            "70 of 100 generates.\n",
            "80 of 100 generates.\n",
            "90 of 100 generates.\n",
            "0 of 100, current mean: 0.0\n",
            "10 of 100, current mean: 0.16549101212311698\n",
            "20 of 100, current mean: 0.13599679350414534\n",
            "30 of 100, current mean: 0.12256709066637131\n",
            "40 of 100, current mean: 0.12455909851927117\n",
            "50 of 100, current mean: 0.1235535708523886\n",
            "60 of 100, current mean: 0.11933256071651413\n",
            "70 of 100, current mean: 0.12080518572858785\n",
            "80 of 100, current mean: 0.1152958925064089\n",
            "90 of 100, current mean: 0.11562571341613051\n",
            "final bleu is:  0.11939014017448064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5rHRtaq9bkb",
        "outputId": "f71844f1-dcb1-46e5-eb83-be04bff40616"
      },
      "source": [
        "generates = generate_text(odeLSTM, glove, test_X[:gen_num], generate_length, k=4)\n",
        "\n",
        "print('final bleu is: ', generation_bleu(generates, glove, generate_length, test_y))"
      ],
      "id": "u5rHRtaq9bkb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "generating texts...\n",
            "0 of 100 generates.\n",
            "10 of 100 generates.\n",
            "20 of 100 generates.\n",
            "30 of 100 generates.\n",
            "40 of 100 generates.\n",
            "50 of 100 generates.\n",
            "60 of 100 generates.\n",
            "70 of 100 generates.\n",
            "80 of 100 generates.\n",
            "90 of 100 generates.\n",
            "0 of 100, current mean: 0.0\n",
            "10 of 100, current mean: 0.0920419121273297\n",
            "20 of 100, current mean: 0.09752345541111389\n",
            "30 of 100, current mean: 0.09761440818932049\n",
            "40 of 100, current mean: 0.08267665208368496\n",
            "50 of 100, current mean: 0.08207742715461866\n",
            "60 of 100, current mean: 0.08707285201208335\n",
            "70 of 100, current mean: 0.08858447976017784\n",
            "80 of 100, current mean: 0.08715209913284197\n",
            "90 of 100, current mean: 0.08832279252742575\n",
            "final bleu is:  0.09145840249831404\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88de890b"
      },
      "source": [
        "# NeuralDE-GPT2"
      ],
      "id": "88de890b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91be6636-c221-4672-8a4f-a64f919f41dd"
      },
      "source": [
        "# Build a wrapper for gpt that takes a torch.util.data.TensorDataset as input, needed for pytorch lightning\n",
        "class GPTModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model=None, sequence_length=40):\n",
        "        super().__init__()\n",
        "        self.gpt = model.to(device)\n",
        "        self.tokenizer = gpt_tokenizer\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "    \n",
        "    # output the hidden states for the entire sequence used for the Neural ODE\n",
        "    def forward(self, input_dataset):\n",
        "        output = self.gpt.forward(input_ids = input_dataset[0].to(device), \n",
        "                                  attention_mask=input_dataset[1].to(device),\n",
        "                                  use_cache=False,\n",
        "                                  output_hidden_states=True)\n",
        "        return output[\"hidden_states\"]\n",
        "\n",
        "\n",
        "# Defines an ODE that uses a GPT to get a representation for the sentence\n",
        "class ODEGPT(pl.LightningModule):\n",
        "    def __init__(self, modelgpt, sequence_length=40):\n",
        "        super().__init__()\n",
        "        layer_size = 768 # the size of gpt's hidden state\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        # Freeze the GPT model's parameters to save training time\n",
        "        self.modelgpt = modelgpt\n",
        "        for param in self.modelgpt.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Define the derivative function\n",
        "        self.f = torch.nn.Sequential(\n",
        "            torch.nn.Linear(layer_size, layer_size),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(layer_size, layer_size),\n",
        "        )\n",
        "        \n",
        "        # Define the model itself\n",
        "        self.node = NeuralDE(self.f, sensitivity='adjoint', solver='dopri5').to(device)\n",
        "        self.linear = torch.nn.Linear(layer_size, self.modelgpt.vocab_size).to(device)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.timesteps = torch.arange(0, 40, 1, device=device).float() # define the number of output items of the Neural ODE\n",
        "    \n",
        "    # take in a single sample and feed forward, giving the logits as output\n",
        "    # note x must be an element of a GPT2Dataset class so that it can be fed to the GPT model\n",
        "    def forward(self, x):\n",
        "        # at the moment this feeds the entire sequence to LSTM and asks Neural ODE to reproduce it\n",
        "        # TODO: switch to feeding half the sequence and asking NeuralODE to extrapolate\n",
        "        x = [x[0][:, :20], x[1][:, :20], x[2][:, :20]] \n",
        "        hidden_states = self.modelgpt(x) \n",
        "        # hidden_states = self.modelgpt(x[:20]) \n",
        "        attention_mask = x[1].to(device)[0, :] # batching makes x[1] have a shape of (batch_size, features), we use batches of 1 so take the first\n",
        "        \n",
        "        # use the output of GPT2's 12th decoder, \"BERT Rediscovers the Classical NLP Pipeline\" has shown transformers' later layers represent high level meaning, which is \n",
        "        # what we want to input to the Neural ODE\n",
        "        # TODO: Perhaps consider the above paper's method of having a weighted sum of layers representations, with trainable weights\n",
        "        final_hidden = hidden_states[12] \n",
        "        final_hidden = final_hidden[0, attention_mask, :][-1, :] # Take the output of the last sequence item that isn't a pad token\n",
        "        # feed to neural ode\n",
        "        sequence_outputs = self.node.trajectory(final_hidden, self.timesteps) # output is of shape (sequence_length, gpt_hidden_layer_size)\n",
        "        \n",
        "        # Get final output\n",
        "        pred = self.linear(sequence_outputs)\n",
        "        return pred\n",
        "    \n",
        "    # compute the loss on a batch, required by pytorch lightning\n",
        "    # note the batch must be an element of a tf.utils.data.TensorDataset, this function is only meant to be used with pytorch_lightning's training loop\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        labels = batch[2][0, 1:].to(device) # shift the input 1 step ahead to get the next word labels\n",
        "        preds = self.forward(batch)[:-1, :] # remove the prediction for the last token as there is no label\n",
        "        loss = self.loss(preds, labels) # crossentropy loss expects preds to be of size (batch, n_classes) so it handles our sequence model use case\n",
        "        return loss\n",
        "    \n",
        "    # configure the optimizer for pytorch lightning\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.00001, betas=(0.95, 0.999)) # low learning rate and momentum since this is stochastic optimisation\n",
        "    \n",
        "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
        "    def predict(self, x):\n",
        "        preds = self.forward(x)\n",
        "        preds = self.softmax(preds).detach().cpu().numpy()\n",
        "        return preds"
      ],
      "id": "91be6636-c221-4672-8a4f-a64f919f41dd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf6b0d2b-690f-4b3a-903f-ce4edac5fddd"
      },
      "source": [
        "gptmodel_wrapper = GPTModelWrapper(gpt_model)\n",
        "odemodel = ODEGPT(gptmodel_wrapper)"
      ],
      "id": "bf6b0d2b-690f-4b3a-903f-ce4edac5fddd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d94d25e-6ab7-4c72-ace3-8791507fd035"
      },
      "source": [
        "Errors: setting num_workers = 1 causes DataLoader to hang. Setting num_workers = 0 causes a random CUDA error that doesn't happen in the above functions"
      ],
      "id": "1d94d25e-6ab7-4c72-ace3-8791507fd035"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6c99ee5c-9ec8-4405-ba8c-24a6dfa1f8b6"
      },
      "source": [
        "### Training\n",
        "Use the pytorch lightning's training loop to speed up training. \n",
        "\n",
        "Important documentation\n",
        "- https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html#training\n",
        "- https://pytorch-lightning.readthedocs.io/en/latest/common/trainer.html#trainer-class-api"
      ],
      "id": "6c99ee5c-9ec8-4405-ba8c-24a6dfa1f8b6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bc138d0-fc59-4e87-9a36-0027436aa333"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'"
      ],
      "id": "3bc138d0-fc59-4e87-9a36-0027436aa333",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cb8d8a53-7a32-4c05-a9a3-1b80285a3b2e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5244f351-9aec-4441-ceaa-9fee812d7f08"
      },
      "source": [
        "# Load the data into a new dataset, pytorch_lightning doesn't like our custom dataset\n",
        "full_dataset = train_dataset[:]\n",
        "train_tensor_dataset = torch.utils.data.TensorDataset(full_dataset['input_ids'], full_dataset['attention_mask'], full_dataset['labels'])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=1, shuffle=True,\n",
        "                             num_workers=2, pin_memory=True)\n",
        "\n",
        "# Test run to check for errors\n",
        "for batch in train_dataloader:\n",
        "    results = odemodel.forward(batch)\n",
        "    print(results[:768], torch.sum(results[:768]))\n",
        "    print(results[768:], torch.sum(results[768:]))\n",
        "    #print(odemodel.forward(batch))\n",
        "    print(odemodel.training_step(batch, 0))\n",
        "    break"
      ],
      "id": "cb8d8a53-7a32-4c05-a9a3-1b80285a3b2e",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-5.5348e+00,  5.9912e+00, -3.9831e+00,  ...,  1.8607e-01,\n",
            "         -1.4220e+00, -2.9475e+00],\n",
            "        [-7.1594e+00,  6.9915e+00, -5.7088e+00,  ..., -4.7523e-01,\n",
            "         -5.5087e-02, -1.2406e+00],\n",
            "        [-8.5823e+00,  8.1473e+00, -7.3720e+00,  ..., -1.2025e+00,\n",
            "          1.1492e+00,  2.8073e-01],\n",
            "        ...,\n",
            "        [-1.1781e+04, -2.3058e+03, -2.3049e+04,  ...,  7.8360e+03,\n",
            "          3.8049e+03, -2.6058e+03],\n",
            "        [-1.5199e+04, -3.5479e+03, -2.9103e+04,  ...,  9.9815e+03,\n",
            "          4.3686e+03, -3.7223e+03],\n",
            "        [-1.9538e+04, -5.1800e+03, -3.6753e+04,  ...,  1.2736e+04,\n",
            "          4.9002e+03, -5.0784e+03]], device='cuda:0', grad_fn=<SliceBackward>) tensor(8521420., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor([], device='cuda:0', size=(0, 50257), grad_fn=<SliceBackward>) tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(6626.0190, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dd793cab-8f30-4d6e-b922-a099b2ec906c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "1443277b136643f38f0c2316d1736483",
            "15dd2f6a688f4fc8a04517e39d9ec25e",
            "68fb25334af34598a9c13b771c497ac3",
            "f33ec0b6adee4056a0bd075e6651051b",
            "4c99cda001c642ec81d801f8fa292ad1",
            "9493dd3eff8e4e90ada07e054f07401b",
            "e66acc3ea6624d50bf2aaff2cf7984f2",
            "5eb46b40b9ba42dd927c4bfb2b068a7f"
          ]
        },
        "outputId": "86f7255d-597e-4aee-9fa8-8f195b085e79"
      },
      "source": [
        "# Train the model\n",
        "trainer = pl.Trainer(max_epochs=1, gpus=1, progress_bar_refresh_rate=10)\n",
        "trainer.fit(odemodel, train_dataloader)"
      ],
      "id": "dd793cab-8f30-4d6e-b922-a099b2ec906c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type             | Params\n",
            "----------------------------------------------\n",
            "0 | loss     | CrossEntropyLoss | 0     \n",
            "1 | modelgpt | GPTModelWrapper  | 124 M \n",
            "2 | f        | Sequential       | 1.2 M \n",
            "3 | node     | NeuralDE         | 1.2 M \n",
            "4 | linear   | Linear           | 38.6 M\n",
            "5 | softmax  | Softmax          | 0     \n",
            "----------------------------------------------\n",
            "39.8 M    Trainable params\n",
            "124 M     Non-trainable params\n",
            "164 M     Total params\n",
            "657.074   Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1443277b136643f38f0c2316d1736483",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnprXT_smFx2"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "PnprXT_smFx2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11315364-9ca9-49ba-b5b2-409e2cd58052"
      },
      "source": [
        "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
        "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
        "def perplexity_ode(preds, ground_truth, mask, epsilon=1e-30):\n",
        "    probs = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        if mask[i] != 0:\n",
        "            probs.append(preds[i, int(ground_truth[i])])\n",
        "    probs = np.array(probs)\n",
        "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
        "    return np.prod(probs)\n",
        "\n",
        "# Calculate overall perplexity for a dataset\n",
        "def average_perplexity_ode(model, train_dataloader, print_results=False, max_items=5000):\n",
        "    perplexities = []\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        preds = model.predict(batch)[:-1, :]\n",
        "        mask = batch[1][0][:-1]\n",
        "        labels = batch[2][0, 1:].numpy() # shift the input 1 step ahead to get the next word labels\n",
        "        perplexities.append(perplexity_ode(preds, labels, mask))\n",
        "        if print_results and i % 100 == 0:\n",
        "            print('{} of {}, current mean perplex: {:.2f}'.format(i, min(max_items, len(train_dataloader)), (np.mean(perplexities))))\n",
        "        if i == max_items:\n",
        "            break\n",
        "    return np.mean(perplexities)\n",
        "\n",
        "\n",
        "# straight calculation of BLEU\n",
        "def average_bleu_ode(model, train_dataloader, print_results=False, max_items=2000):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        mask = batch[1][0][:-1].tolist()\n",
        "\n",
        "        reference_withmask = batch[2][0, 1:].numpy().tolist()\n",
        "        preds = model.predict(batch)[:-1, :]\n",
        "        candidate_withmask = np.argmax(preds, axis=1).tolist()\n",
        "\n",
        "        reference = [reference_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "        candidate = [candidate_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "\n",
        "        bleus.append(sentence_bleu([reference], candidate, smoothing_function=smoothie))\n",
        "\n",
        "        if print_results and i % 100 == 0:\n",
        "            print('{} of {}, current mean bleu: {:.4f}'.format(i, min(max_items, len(train_dataloader)), (np.mean(bleus))))\n",
        "        if i == max_items:\n",
        "            break\n",
        "    return np.mean(bleus)"
      ],
      "id": "11315364-9ca9-49ba-b5b2-409e2cd58052",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xF7oXQx8b9h"
      },
      "source": [
        "#### Perplexity"
      ],
      "id": "7xF7oXQx8b9h"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Rebwxii1T-6",
        "outputId": "b7580120-6adb-4a50-be23-039a7cda2610"
      },
      "source": [
        "# train perplexity\n",
        "odemodel.eval()\n",
        "odemodel.to(device)\n",
        "eval_dataloader = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, eval_dataloader, print_results=True)"
      ],
      "id": "4Rebwxii1T-6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 5000, current mean perplex: 84.00\n",
            "100 of 5000, current mean perplex: 182.91\n",
            "200 of 5000, current mean perplex: 179.69\n",
            "300 of 5000, current mean perplex: 180.56\n",
            "400 of 5000, current mean perplex: 187.96\n",
            "500 of 5000, current mean perplex: 189.69\n",
            "600 of 5000, current mean perplex: 198.57\n",
            "700 of 5000, current mean perplex: 193.64\n",
            "800 of 5000, current mean perplex: 193.01\n",
            "900 of 5000, current mean perplex: 193.82\n",
            "1000 of 5000, current mean perplex: 194.00\n",
            "1100 of 5000, current mean perplex: 235.33\n",
            "1200 of 5000, current mean perplex: 233.27\n",
            "1300 of 5000, current mean perplex: 230.57\n",
            "1400 of 5000, current mean perplex: 227.89\n",
            "1500 of 5000, current mean perplex: 224.78\n",
            "1600 of 5000, current mean perplex: 223.38\n",
            "1700 of 5000, current mean perplex: 223.52\n",
            "1800 of 5000, current mean perplex: 220.96\n",
            "1900 of 5000, current mean perplex: 218.96\n",
            "2000 of 5000, current mean perplex: 217.23\n",
            "2100 of 5000, current mean perplex: 216.61\n",
            "2200 of 5000, current mean perplex: 216.13\n",
            "2300 of 5000, current mean perplex: 215.24\n",
            "2400 of 5000, current mean perplex: 213.94\n",
            "2500 of 5000, current mean perplex: 213.31\n",
            "2600 of 5000, current mean perplex: 212.28\n",
            "2700 of 5000, current mean perplex: 211.14\n",
            "2800 of 5000, current mean perplex: 210.66\n",
            "2900 of 5000, current mean perplex: 210.70\n",
            "3000 of 5000, current mean perplex: 210.09\n",
            "3100 of 5000, current mean perplex: 209.97\n",
            "3200 of 5000, current mean perplex: 209.55\n",
            "3300 of 5000, current mean perplex: 209.18\n",
            "3400 of 5000, current mean perplex: 207.86\n",
            "3500 of 5000, current mean perplex: 207.80\n",
            "3600 of 5000, current mean perplex: 207.95\n",
            "3700 of 5000, current mean perplex: 206.71\n",
            "3800 of 5000, current mean perplex: 206.29\n",
            "3900 of 5000, current mean perplex: 205.49\n",
            "4000 of 5000, current mean perplex: 205.07\n",
            "4100 of 5000, current mean perplex: 205.19\n",
            "4200 of 5000, current mean perplex: 205.10\n",
            "4300 of 5000, current mean perplex: 204.46\n",
            "4400 of 5000, current mean perplex: 204.16\n",
            "4500 of 5000, current mean perplex: 203.74\n",
            "4600 of 5000, current mean perplex: 204.20\n",
            "4700 of 5000, current mean perplex: 203.76\n",
            "4800 of 5000, current mean perplex: 203.49\n",
            "4900 of 5000, current mean perplex: 203.45\n",
            "5000 of 5000, current mean perplex: 203.30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "203.30031"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xSWerLUXirFL",
        "outputId": "ccc355d9-ae09-4d86-efdc-66e4bfcc3612"
      },
      "source": [
        "# validate perplexity\n",
        "full_val_dataset = val_dataset[:]\n",
        "val_tensor_dataset = torch.utils.data.TensorDataset(full_val_dataset['input_ids'], full_val_dataset['attention_mask'], full_val_dataset['labels'])\n",
        "val_dataloader = torch.utils.data.DataLoader(val_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, val_dataloader, print_results=True)"
      ],
      "id": "xSWerLUXirFL",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 3370, current mean perplex: 390.71\n",
            "100 of 3370, current mean perplex: 208.44\n",
            "200 of 3370, current mean perplex: 191.30\n",
            "300 of 3370, current mean perplex: 199.88\n",
            "400 of 3370, current mean perplex: 210.70\n",
            "500 of 3370, current mean perplex: 210.94\n",
            "600 of 3370, current mean perplex: 210.13\n",
            "700 of 3370, current mean perplex: 208.18\n",
            "800 of 3370, current mean perplex: 203.43\n",
            "900 of 3370, current mean perplex: 205.93\n",
            "1000 of 3370, current mean perplex: 207.75\n",
            "1100 of 3370, current mean perplex: 207.27\n",
            "1200 of 3370, current mean perplex: 207.75\n",
            "1300 of 3370, current mean perplex: 206.42\n",
            "1400 of 3370, current mean perplex: 210.75\n",
            "1500 of 3370, current mean perplex: 212.15\n",
            "1600 of 3370, current mean perplex: 213.81\n",
            "1700 of 3370, current mean perplex: 212.41\n",
            "1800 of 3370, current mean perplex: 210.89\n",
            "1900 of 3370, current mean perplex: 209.78\n",
            "2000 of 3370, current mean perplex: 209.16\n",
            "2100 of 3370, current mean perplex: 208.56\n",
            "2200 of 3370, current mean perplex: 207.54\n",
            "2300 of 3370, current mean perplex: 209.60\n",
            "2400 of 3370, current mean perplex: 209.15\n",
            "2500 of 3370, current mean perplex: 208.68\n",
            "2600 of 3370, current mean perplex: 207.69\n",
            "2700 of 3370, current mean perplex: 207.50\n",
            "2800 of 3370, current mean perplex: 207.73\n",
            "2900 of 3370, current mean perplex: 208.66\n",
            "3000 of 3370, current mean perplex: 208.52\n",
            "3100 of 3370, current mean perplex: 207.95\n",
            "3200 of 3370, current mean perplex: 208.41\n",
            "3300 of 3370, current mean perplex: 209.54\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "209.62929"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQf2PeUBs-FX",
        "outputId": "e64ab01a-8756-4aa6-9e16-0afe8504af8d"
      },
      "source": [
        "# test perplexity\n",
        "full_test_dataset = test_dataset[:]\n",
        "test_tensor_dataset = torch.utils.data.TensorDataset(full_test_dataset['input_ids'], full_test_dataset['attention_mask'], full_test_dataset['labels'])\n",
        "test_dataloader = torch.utils.data.DataLoader(test_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, test_dataloader, print_results=True)"
      ],
      "id": "aQf2PeUBs-FX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 3761, current mean perplex: 135.79\n",
            "100 of 3761, current mean perplex: 184.10\n",
            "200 of 3761, current mean perplex: 189.74\n",
            "300 of 3761, current mean perplex: 189.17\n",
            "400 of 3761, current mean perplex: 192.95\n",
            "500 of 3761, current mean perplex: 192.78\n",
            "600 of 3761, current mean perplex: 190.50\n",
            "700 of 3761, current mean perplex: 188.73\n",
            "800 of 3761, current mean perplex: 185.62\n",
            "900 of 3761, current mean perplex: 184.59\n",
            "1000 of 3761, current mean perplex: 185.26\n",
            "1100 of 3761, current mean perplex: 188.54\n",
            "1200 of 3761, current mean perplex: 188.44\n",
            "1300 of 3761, current mean perplex: 190.02\n",
            "1400 of 3761, current mean perplex: 189.19\n",
            "1500 of 3761, current mean perplex: 191.13\n",
            "1600 of 3761, current mean perplex: 190.14\n",
            "1700 of 3761, current mean perplex: 189.32\n",
            "1800 of 3761, current mean perplex: 189.83\n",
            "1900 of 3761, current mean perplex: 189.20\n",
            "2000 of 3761, current mean perplex: 188.24\n",
            "2100 of 3761, current mean perplex: 188.65\n",
            "2200 of 3761, current mean perplex: 187.70\n",
            "2300 of 3761, current mean perplex: 187.17\n",
            "2400 of 3761, current mean perplex: 186.07\n",
            "2500 of 3761, current mean perplex: 185.80\n",
            "2600 of 3761, current mean perplex: 185.59\n",
            "2700 of 3761, current mean perplex: 184.41\n",
            "2800 of 3761, current mean perplex: 184.10\n",
            "2900 of 3761, current mean perplex: 183.31\n",
            "3000 of 3761, current mean perplex: 183.55\n",
            "3100 of 3761, current mean perplex: 184.00\n",
            "3200 of 3761, current mean perplex: 183.57\n",
            "3300 of 3761, current mean perplex: 183.64\n",
            "3400 of 3761, current mean perplex: 183.37\n",
            "3500 of 3761, current mean perplex: 182.61\n",
            "3600 of 3761, current mean perplex: 182.53\n",
            "3700 of 3761, current mean perplex: 182.50\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "182.5745"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q70rk9Mm8gXW"
      },
      "source": [
        "#### BlEU"
      ],
      "id": "Q70rk9Mm8gXW"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AbesUKowsV6",
        "outputId": "4a56ad2a-4375-44a2-b0c0-cd0c925030a3"
      },
      "source": [
        "average_bleu_ode(odemodel, eval_dataloader, print_results=True)"
      ],
      "id": "5AbesUKowsV6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1906\n",
            "100 of 2000, current mean bleu: 0.1742\n",
            "200 of 2000, current mean bleu: 0.1761\n",
            "300 of 2000, current mean bleu: 0.1796\n",
            "400 of 2000, current mean bleu: 0.1801\n",
            "500 of 2000, current mean bleu: 0.1814\n",
            "600 of 2000, current mean bleu: 0.1812\n",
            "700 of 2000, current mean bleu: 0.1794\n",
            "800 of 2000, current mean bleu: 0.1795\n",
            "900 of 2000, current mean bleu: 0.1797\n",
            "1000 of 2000, current mean bleu: 0.1795\n",
            "1100 of 2000, current mean bleu: 0.1808\n",
            "1200 of 2000, current mean bleu: 0.1804\n",
            "1300 of 2000, current mean bleu: 0.1802\n",
            "1400 of 2000, current mean bleu: 0.1796\n",
            "1500 of 2000, current mean bleu: 0.1794\n",
            "1600 of 2000, current mean bleu: 0.1790\n",
            "1700 of 2000, current mean bleu: 0.1785\n",
            "1800 of 2000, current mean bleu: 0.1786\n",
            "1900 of 2000, current mean bleu: 0.1784\n",
            "2000 of 2000, current mean bleu: 0.1783\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17831192756554887"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jX_XRH6w59qZ",
        "outputId": "8d49f81a-3f6f-4b48-aa7b-a409c942fc92"
      },
      "source": [
        "average_bleu_ode(odemodel, val_dataloader, print_results=True)"
      ],
      "id": "jX_XRH6w59qZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1471\n",
            "100 of 2000, current mean bleu: 0.1774\n",
            "200 of 2000, current mean bleu: 0.1784\n",
            "300 of 2000, current mean bleu: 0.1750\n",
            "400 of 2000, current mean bleu: 0.1754\n",
            "500 of 2000, current mean bleu: 0.1751\n",
            "600 of 2000, current mean bleu: 0.1753\n",
            "700 of 2000, current mean bleu: 0.1764\n",
            "800 of 2000, current mean bleu: 0.1782\n",
            "900 of 2000, current mean bleu: 0.1787\n",
            "1000 of 2000, current mean bleu: 0.1783\n",
            "1100 of 2000, current mean bleu: 0.1783\n",
            "1200 of 2000, current mean bleu: 0.1787\n",
            "1300 of 2000, current mean bleu: 0.1787\n",
            "1400 of 2000, current mean bleu: 0.1786\n",
            "1500 of 2000, current mean bleu: 0.1777\n",
            "1600 of 2000, current mean bleu: 0.1780\n",
            "1700 of 2000, current mean bleu: 0.1779\n",
            "1800 of 2000, current mean bleu: 0.1782\n",
            "1900 of 2000, current mean bleu: 0.1777\n",
            "2000 of 2000, current mean bleu: 0.1776\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17756214980956334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zWVwWhWAJU-",
        "outputId": "0541e6f0-9458-4d87-e903-443d8f4519fe"
      },
      "source": [
        "average_bleu_ode(odemodel, test_dataloader, print_results=True)"
      ],
      "id": "9zWVwWhWAJU-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1595\n",
            "100 of 2000, current mean bleu: 0.1726\n",
            "200 of 2000, current mean bleu: 0.1733\n",
            "300 of 2000, current mean bleu: 0.1733\n",
            "400 of 2000, current mean bleu: 0.1750\n",
            "500 of 2000, current mean bleu: 0.1760\n",
            "600 of 2000, current mean bleu: 0.1782\n",
            "700 of 2000, current mean bleu: 0.1800\n",
            "800 of 2000, current mean bleu: 0.1795\n",
            "900 of 2000, current mean bleu: 0.1793\n",
            "1000 of 2000, current mean bleu: 0.1791\n",
            "1100 of 2000, current mean bleu: 0.1791\n",
            "1200 of 2000, current mean bleu: 0.1793\n",
            "1300 of 2000, current mean bleu: 0.1797\n",
            "1400 of 2000, current mean bleu: 0.1804\n",
            "1500 of 2000, current mean bleu: 0.1805\n",
            "1600 of 2000, current mean bleu: 0.1801\n",
            "1700 of 2000, current mean bleu: 0.1799\n",
            "1800 of 2000, current mean bleu: 0.1799\n",
            "1900 of 2000, current mean bleu: 0.1799\n",
            "2000 of 2000, current mean bleu: 0.1800\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18002267113973042"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iogUI9wnNCZ0"
      },
      "source": [
        ""
      ],
      "id": "iogUI9wnNCZ0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3u357CriuPt"
      },
      "source": [
        "# Augmented-NeuralDE-GPT2"
      ],
      "id": "S3u357CriuPt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8IUuLO7izl7"
      },
      "source": [
        "# Build a wrapper for gpt that takes a torch.util.data.TensorDataset as input, needed for pytorch lightning\n",
        "class GPTModelWrapper(torch.nn.Module):\n",
        "    def __init__(self, model=None, sequence_length=40):\n",
        "        super().__init__()\n",
        "        self.gpt = model.to(device)\n",
        "        self.tokenizer = gpt_tokenizer\n",
        "        self.sequence_length = sequence_length\n",
        "        self.vocab_size = self.tokenizer.vocab_size\n",
        "    \n",
        "    # output the hidden states for the entire sequence used for the Neural ODE\n",
        "    def forward(self, input_dataset):\n",
        "        output = self.gpt.forward(input_ids = input_dataset[0].to(device), \n",
        "                                  attention_mask=input_dataset[1].to(device),\n",
        "                                  use_cache=False,\n",
        "                                  output_hidden_states=True)\n",
        "        return output[\"hidden_states\"]\n",
        "\n",
        "# Defines an ODE that uses a GPT to get a representation for the sentence\n",
        "class ODEGPT_AugmentedVersion(pl.LightningModule):\n",
        "    def __init__(self, modelgpt, sequence_length=40, extrapolation=False, zero_initialisation=False):\n",
        "        super().__init__()\n",
        "        self.layer_size = 768 # the size of gpt's hidden state\n",
        "        self.loss = torch.nn.CrossEntropyLoss()\n",
        "        self.extrapolation= extrapolation\n",
        "        self.zero_initialisation = zero_initialisation\n",
        "        \n",
        "        # Freeze the GPT model's parameters to save training time\n",
        "        self.modelgpt = modelgpt\n",
        "        for param in self.modelgpt.parameters():\n",
        "            param.requires_grad = False\n",
        "        \n",
        "        # Define the derivative function\n",
        "        # self.f = ODEMemory()\n",
        "        self.f = torch.nn.Sequential(\n",
        "            torch.nn.Linear(self.layer_size*2, self.layer_size*2),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(self.layer_size*2, self.layer_size*2),\n",
        "        )\n",
        "        \n",
        "        # Define the model itself\n",
        "        self.node = NeuralDE(self.f, sensitivity='adjoint', solver='dopri5').to(device)\n",
        "        self.linear = torch.nn.Linear(self.layer_size*2, self.modelgpt.vocab_size).to(device)\n",
        "        self.softmax = torch.nn.Softmax(dim=-1)\n",
        "        self.sequence_length = sequence_length\n",
        "        self.timesteps = torch.arange(0, 40, 1, device=device).float() # define the number of output items of the Neural ODE\n",
        "    \n",
        "    # take in a single sample and feed forward, giving the logits as output\n",
        "    # note x must be an element of a GPT2Dataset class so that it can be fed to the GPT model\n",
        "    def forward(self, x):\n",
        "        if self.extrapolation:\n",
        "            # Feed GPT-2 half the sequence to get a hidden state representing it then train NeuralODE to reconstruct and extrapolate\n",
        "            x = [x[0][:, :20], x[1][:, :20], x[2][:, :20]]\n",
        "        hidden_states = self.modelgpt(x) \n",
        "        attention_mask = x[1].to(device)[0, :] # batching makes x[1] have a shape of (batch_size, features), we use batches of 1 so take the first\n",
        "        \n",
        "        # use the output of GPT2's 12th decoder, \"BERT Rediscovers the Classical NLP Pipeline\" has shown transformers' later layers represent high level meaning, which is \n",
        "        # what we want to input to the Neural ODE\n",
        "        final_hidden = hidden_states[12] \n",
        "        hidden_state = final_hidden[0, attention_mask, :][-1, :] # Take the output of the last sequence item that isn't a pad token as an overall sentence representation\n",
        "        input_state  = final_hidden[0, attention_mask, :][0,  :] # Take the output for the first word as the input to the neural ODE that should help it predict the next (2nd) word\n",
        "\n",
        "        if self.zero_initialisation: # implement augmented ODE as in the original paper, with the added dimensions being 0\n",
        "            input_state = torch.zeros(input_state.shape).to(device)\n",
        "            \n",
        "        gru_input = torch.cat([hidden_state, input_state], dim=0).reshape(2*self.layer_size)\n",
        "\n",
        "        # feed to neural ode\n",
        "        sequence_outputs = self.node.trajectory(gru_input, self.timesteps) # output is of shape (sequence_length, gpt_hidden_layer_size)\n",
        "        \n",
        "        # Get final output\n",
        "        pred = self.linear(sequence_outputs)\n",
        "        return pred\n",
        "    \n",
        "    # compute the loss on a batch, required by pytorch lightning\n",
        "    # note the batch must be an element of a tf.utils.data.TensorDataset, this function is only meant to be used with pytorch_lightning's training loop\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        labels = batch[2][0, 1:].to(device) # shift the input 1 step ahead to get the next word labels\n",
        "        preds = self.forward(batch)[:-1, :] # remove the prediction for the last token as there is no label\n",
        "        loss = self.loss(preds, labels) # crossentropy loss expects preds to be of size (batch, n_classes) so it handles our sequence model use case\n",
        "        return loss\n",
        "    \n",
        "    # configure the optimizer for pytorch lightning\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(), lr=0.00001, betas=(0.95, 0.999)) # low learning rate and momentum since this is stochastic optimisation\n",
        "    \n",
        "    # wrapper function that forward propagates, applies softmax and converts to numpy \n",
        "    def predict(self, x):\n",
        "        preds = self.forward(x)\n",
        "        preds = self.softmax(preds).detach().cpu().numpy()\n",
        "        return preds"
      ],
      "id": "y8IUuLO7izl7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-yWwUB6lnId"
      },
      "source": [
        "# gptmodel_wrapper = GPTModelWrapper(gpt_model)\n",
        "# odemodel = ODEGPT_AugmentedVersion(gptmodel_wrapper)\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/COMP5329/Ass/Assignment 2/languageODE-augf-0initialisation'\n",
        "\n",
        "odemodel = ODEGPT_AugmentedVersion(gptmodel_wrapper, zero_initialisation=True)\n",
        "odemodel.load_state_dict(torch.load(filepath))\n",
        "odemodel.eval()"
      ],
      "id": "U-yWwUB6lnId",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLTGBGGUl2lO",
        "outputId": "93ecfb5c-eb0c-4f02-da75-e4ca9e3efa8c"
      },
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "# Load the data into a new dataset, pytorch_lightning doesn't like our custom dataset\n",
        "full_dataset = train_dataset[:]\n",
        "train_tensor_dataset = torch.utils.data.TensorDataset(full_dataset['input_ids'], full_dataset['attention_mask'], full_dataset['labels'])\n",
        "train_dataloader = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=1, shuffle=True,\n",
        "                             num_workers=2, pin_memory=True)\n",
        "\n",
        "# Test run to check for errors\n",
        "for batch in train_dataloader:\n",
        "    results = odemodel.forward(batch)\n",
        "    print(results[:768], torch.sum(results[:768]))\n",
        "    print(results[768:], torch.sum(results[768:]))\n",
        "    #print(odemodel.forward(batch))\n",
        "    print(odemodel.training_step(batch, 0))\n",
        "    break"
      ],
      "id": "OLTGBGGUl2lO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[-44.7862, -43.6993, -46.0333,  ..., -45.7293, -44.6557,   2.4222],\n",
            "        [-35.0560, -34.3139, -34.3964,  ..., -35.6592, -35.0025,   3.2305],\n",
            "        [-30.4650, -29.8420, -29.2571,  ..., -31.0252, -30.5095,   2.7842],\n",
            "        ...,\n",
            "        [-13.7895, -13.8039, -13.5388,  ..., -13.8990, -13.3496,  10.0173],\n",
            "        [-14.1596, -14.2361, -13.9108,  ..., -14.4314, -13.6911,  10.8687],\n",
            "        [-14.8116, -14.9353, -14.5242,  ..., -15.2675, -14.2874,  11.8214]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward>) tensor(-29542392., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor([], device='cuda:0', size=(0, 50257), grad_fn=<SliceBackward>) tensor(0., device='cuda:0', grad_fn=<SumBackward0>)\n",
            "tensor(2.7449, device='cuda:0', grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_OkB2RRnNzn"
      },
      "source": [
        "### Training"
      ],
      "id": "V_OkB2RRnNzn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "9c466ee8506e428993dade045f30d998",
            "88d5b3daa1b24fe392f3f3c7ef5023f5",
            "c5f226c3e40d4da980ed54ab39bf0be1",
            "7397cedab7264c15833bd44b20c7523d",
            "9ec48903f1014ce3ba85b6b7add0aad4",
            "7966eec2481f4a7695231daac1f1c757",
            "026852a148364e5f8489448b8b0f888f",
            "2d226338ed844f92a9ad198445c20890"
          ]
        },
        "id": "gi3w3t3nmA9F",
        "outputId": "11b3d3f9-6d03-4c9b-f7b7-cf842748d6c4"
      },
      "source": [
        "# Train the model\n",
        "trainer = pl.Trainer(max_epochs=2, gpus=1, progress_bar_refresh_rate=10)\n",
        "trainer.fit(odemodel, train_dataloader)"
      ],
      "id": "gi3w3t3nmA9F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name     | Type             | Params\n",
            "----------------------------------------------\n",
            "0 | loss     | CrossEntropyLoss | 0     \n",
            "1 | modelgpt | GPTModelWrapper  | 124 M \n",
            "2 | f        | Sequential       | 4.7 M \n",
            "3 | node     | NeuralDE         | 4.7 M \n",
            "4 | linear   | Linear           | 77.2 M\n",
            "5 | softmax  | Softmax          | 0     \n",
            "----------------------------------------------\n",
            "82.0 M    Trainable params\n",
            "124 M     Non-trainable params\n",
            "206 M     Total params\n",
            "825.626   Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9c466ee8506e428993dade045f30d998",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOpBqrUeSkJU"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "wOpBqrUeSkJU"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohmL7oBSnSYl"
      },
      "source": [
        "# Define functions to calculate perplexity for a single sentence: see the metric definition here https://web.stanford.edu/~jurafsky/slp3/3.pdf \n",
        "# We use teacher forcing (feeding the ground_truth label for sequence i to get pred for sequence i+1) to get the predictions\n",
        "def perplexity_ode(preds, ground_truth, mask, epsilon=1e-30):\n",
        "    probs = []\n",
        "    for i in range(preds.shape[0]):\n",
        "        if mask[i] != 0:\n",
        "            probs.append(preds[i, int(ground_truth[i])])\n",
        "    probs = np.array(probs)\n",
        "    probs = np.power(1/(probs+epsilon), 1/probs.shape[0]) # normalise before taking the product, to prevent underflowing to 0\n",
        "    return np.prod(probs)\n",
        "\n",
        "# Calculate overall perplexity for a dataset\n",
        "def average_perplexity_ode(model, train_dataloader, print_results=False, max_items=3000):\n",
        "    perplexities = []\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        preds = model.predict(batch)[:-1, :]\n",
        "        mask = batch[1][0][:-1]\n",
        "        labels = batch[2][0, 1:].numpy() # shift the input 1 step ahead to get the next word labels\n",
        "        perplexities.append(perplexity_ode(preds, labels, mask))\n",
        "        if print_results and i % 100 == 0:\n",
        "            print('{} of {}, current mean perplex: {:.2f}'.format(i, min(max_items, len(train_dataloader)), (np.mean(perplexities))))\n",
        "        if i == max_items:\n",
        "            break\n",
        "    return np.mean(perplexities)\n",
        "\n",
        "\n",
        "# straight calculation of BLEU\n",
        "def average_bleu_ode(model, train_dataloader, print_results=False, max_items=2000):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleus = []\n",
        "    for i, batch in enumerate(train_dataloader):\n",
        "        mask = batch[1][0][:-1].tolist()\n",
        "\n",
        "        reference_withmask = batch[2][0, 1:].numpy().tolist()\n",
        "        preds = model.predict(batch)[:-1, :]\n",
        "        candidate_withmask = np.argmax(preds, axis=1).tolist()\n",
        "\n",
        "        reference = [reference_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "        candidate = [candidate_withmask[i] for i in range(len(mask)) if mask[i] != 0]\n",
        "\n",
        "        bleus.append(sentence_bleu([reference], candidate, smoothing_function=smoothie))\n",
        "\n",
        "        if print_results and i % 100 == 0:\n",
        "            print('{} of {}, current mean bleu: {:.4f}'.format(i, min(max_items, len(train_dataloader)), (np.mean(bleus))))\n",
        "        if i == max_items:\n",
        "            break\n",
        "    return np.mean(bleus)"
      ],
      "id": "ohmL7oBSnSYl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwkwfcuOStj5"
      },
      "source": [
        "#### Perplexity"
      ],
      "id": "BwkwfcuOStj5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_TdN78mS_xF",
        "outputId": "74b67c09-c6b8-4cba-92c1-aa7fd009a06a"
      },
      "source": [
        "# train perplexity\n",
        "odemodel.eval()\n",
        "odemodel.to(device)\n",
        "eval_dataloader = torch.utils.data.DataLoader(train_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, eval_dataloader, print_results=True)"
      ],
      "id": "-_TdN78mS_xF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 3000, current mean perplex: 156.13\n",
            "100 of 3000, current mean perplex: 183.27\n",
            "200 of 3000, current mean perplex: 173.80\n",
            "300 of 3000, current mean perplex: 175.93\n",
            "400 of 3000, current mean perplex: 176.42\n",
            "500 of 3000, current mean perplex: 179.43\n",
            "600 of 3000, current mean perplex: 179.49\n",
            "700 of 3000, current mean perplex: 179.51\n",
            "800 of 3000, current mean perplex: 179.59\n",
            "900 of 3000, current mean perplex: 177.54\n",
            "1000 of 3000, current mean perplex: 178.22\n",
            "1100 of 3000, current mean perplex: 176.31\n",
            "1200 of 3000, current mean perplex: 174.16\n",
            "1300 of 3000, current mean perplex: 174.02\n",
            "1400 of 3000, current mean perplex: 174.14\n",
            "1500 of 3000, current mean perplex: 173.74\n",
            "1600 of 3000, current mean perplex: 173.80\n",
            "1700 of 3000, current mean perplex: 173.92\n",
            "1800 of 3000, current mean perplex: 173.87\n",
            "1900 of 3000, current mean perplex: 174.19\n",
            "2000 of 3000, current mean perplex: 173.62\n",
            "2100 of 3000, current mean perplex: 175.13\n",
            "2200 of 3000, current mean perplex: 175.46\n",
            "2300 of 3000, current mean perplex: 175.52\n",
            "2400 of 3000, current mean perplex: 175.55\n",
            "2500 of 3000, current mean perplex: 176.10\n",
            "2600 of 3000, current mean perplex: 176.37\n",
            "2700 of 3000, current mean perplex: 176.38\n",
            "2800 of 3000, current mean perplex: 175.93\n",
            "2900 of 3000, current mean perplex: 176.16\n",
            "3000 of 3000, current mean perplex: 175.62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "175.61554"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz7ULX43TB5s",
        "outputId": "11620560-75a7-433b-fae8-9c62e2f028bf"
      },
      "source": [
        "# validate perplexity\n",
        "full_val_dataset = val_dataset[:]\n",
        "val_tensor_dataset = torch.utils.data.TensorDataset(full_val_dataset['input_ids'], full_val_dataset['attention_mask'], full_val_dataset['labels'])\n",
        "val_dataloader = torch.utils.data.DataLoader(val_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, val_dataloader, print_results=True)"
      ],
      "id": "Qz7ULX43TB5s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 3000, current mean perplex: 92.80\n",
            "100 of 3000, current mean perplex: 217.33\n",
            "200 of 3000, current mean perplex: 193.79\n",
            "300 of 3000, current mean perplex: 194.18\n",
            "400 of 3000, current mean perplex: 190.35\n",
            "500 of 3000, current mean perplex: 189.01\n",
            "600 of 3000, current mean perplex: 194.65\n",
            "700 of 3000, current mean perplex: 191.85\n",
            "800 of 3000, current mean perplex: 193.95\n",
            "900 of 3000, current mean perplex: 191.53\n",
            "1000 of 3000, current mean perplex: 191.34\n",
            "1100 of 3000, current mean perplex: 191.72\n",
            "1200 of 3000, current mean perplex: 192.48\n",
            "1300 of 3000, current mean perplex: 191.01\n",
            "1400 of 3000, current mean perplex: 190.82\n",
            "1500 of 3000, current mean perplex: 190.39\n",
            "1600 of 3000, current mean perplex: 190.38\n",
            "1700 of 3000, current mean perplex: 190.08\n",
            "1800 of 3000, current mean perplex: 190.73\n",
            "1900 of 3000, current mean perplex: 189.95\n",
            "2000 of 3000, current mean perplex: 190.69\n",
            "2100 of 3000, current mean perplex: 190.24\n",
            "2200 of 3000, current mean perplex: 190.28\n",
            "2300 of 3000, current mean perplex: 189.11\n",
            "2400 of 3000, current mean perplex: 189.86\n",
            "2500 of 3000, current mean perplex: 189.23\n",
            "2600 of 3000, current mean perplex: 189.46\n",
            "2700 of 3000, current mean perplex: 188.65\n",
            "2800 of 3000, current mean perplex: 188.35\n",
            "2900 of 3000, current mean perplex: 187.76\n",
            "3000 of 3000, current mean perplex: 187.62\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "187.62354"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXUxGLplTGaQ",
        "outputId": "e302233f-de13-44e0-a318-379a3ca05675"
      },
      "source": [
        "# test perplexity\n",
        "full_test_dataset = test_dataset[:]\n",
        "test_tensor_dataset = torch.utils.data.TensorDataset(full_test_dataset['input_ids'], full_test_dataset['attention_mask'], full_test_dataset['labels'])\n",
        "test_dataloader = torch.utils.data.DataLoader(test_tensor_dataset, batch_size=1, shuffle=True, num_workers=0, pin_memory=True) \n",
        "# faster to startup with num_workers=0\n",
        "\n",
        "average_perplexity_ode(odemodel, test_dataloader, print_results=True)"
      ],
      "id": "eXUxGLplTGaQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 3000, current mean perplex: 238.37\n",
            "100 of 3000, current mean perplex: 152.87\n",
            "200 of 3000, current mean perplex: 168.72\n",
            "300 of 3000, current mean perplex: 168.36\n",
            "400 of 3000, current mean perplex: 169.27\n",
            "500 of 3000, current mean perplex: 167.86\n",
            "600 of 3000, current mean perplex: 170.98\n",
            "700 of 3000, current mean perplex: 168.56\n",
            "800 of 3000, current mean perplex: 165.71\n",
            "900 of 3000, current mean perplex: 163.08\n",
            "1000 of 3000, current mean perplex: 164.71\n",
            "1100 of 3000, current mean perplex: 165.78\n",
            "1200 of 3000, current mean perplex: 166.36\n",
            "1300 of 3000, current mean perplex: 167.42\n",
            "1400 of 3000, current mean perplex: 167.97\n",
            "1500 of 3000, current mean perplex: 168.17\n",
            "1600 of 3000, current mean perplex: 167.83\n",
            "1700 of 3000, current mean perplex: 167.04\n",
            "1800 of 3000, current mean perplex: 167.05\n",
            "1900 of 3000, current mean perplex: 167.34\n",
            "2000 of 3000, current mean perplex: 167.79\n",
            "2100 of 3000, current mean perplex: 168.89\n",
            "2200 of 3000, current mean perplex: 168.41\n",
            "2300 of 3000, current mean perplex: 167.58\n",
            "2400 of 3000, current mean perplex: 167.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH5Akd2bSpSC"
      },
      "source": [
        "#### BLEU"
      ],
      "id": "rH5Akd2bSpSC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JD4iPeKTMbE",
        "outputId": "a09bcc5a-e6ef-465d-ae0c-81285bfea374"
      },
      "source": [
        "average_bleu_ode(odemodel, eval_dataloader, print_results=True)"
      ],
      "id": "1JD4iPeKTMbE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1208\n",
            "100 of 2000, current mean bleu: 0.1950\n",
            "200 of 2000, current mean bleu: 0.1927\n",
            "300 of 2000, current mean bleu: 0.1897\n",
            "400 of 2000, current mean bleu: 0.1883\n",
            "500 of 2000, current mean bleu: 0.1875\n",
            "600 of 2000, current mean bleu: 0.1865\n",
            "700 of 2000, current mean bleu: 0.1872\n",
            "800 of 2000, current mean bleu: 0.1875\n",
            "900 of 2000, current mean bleu: 0.1886\n",
            "1000 of 2000, current mean bleu: 0.1895\n",
            "1100 of 2000, current mean bleu: 0.1901\n",
            "1200 of 2000, current mean bleu: 0.1900\n",
            "1300 of 2000, current mean bleu: 0.1890\n",
            "1400 of 2000, current mean bleu: 0.1896\n",
            "1500 of 2000, current mean bleu: 0.1896\n",
            "1600 of 2000, current mean bleu: 0.1887\n",
            "1700 of 2000, current mean bleu: 0.1885\n",
            "1800 of 2000, current mean bleu: 0.1883\n",
            "1900 of 2000, current mean bleu: 0.1880\n",
            "2000 of 2000, current mean bleu: 0.1878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1877684656028121"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naMGIf1eTNtp",
        "outputId": "1cd0c6d9-6d39-4e72-85da-ad52f98414b1"
      },
      "source": [
        "average_bleu_ode(odemodel, val_dataloader, print_results=True)"
      ],
      "id": "naMGIf1eTNtp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1700\n",
            "100 of 2000, current mean bleu: 0.1986\n",
            "200 of 2000, current mean bleu: 0.1976\n",
            "300 of 2000, current mean bleu: 0.1961\n",
            "400 of 2000, current mean bleu: 0.1952\n",
            "500 of 2000, current mean bleu: 0.1930\n",
            "600 of 2000, current mean bleu: 0.1922\n",
            "700 of 2000, current mean bleu: 0.1913\n",
            "800 of 2000, current mean bleu: 0.1921\n",
            "900 of 2000, current mean bleu: 0.1925\n",
            "1000 of 2000, current mean bleu: 0.1930\n",
            "1100 of 2000, current mean bleu: 0.1922\n",
            "1200 of 2000, current mean bleu: 0.1927\n",
            "1300 of 2000, current mean bleu: 0.1931\n",
            "1400 of 2000, current mean bleu: 0.1921\n",
            "1500 of 2000, current mean bleu: 0.1920\n",
            "1600 of 2000, current mean bleu: 0.1913\n",
            "1700 of 2000, current mean bleu: 0.1916\n",
            "1800 of 2000, current mean bleu: 0.1914\n",
            "1900 of 2000, current mean bleu: 0.1911\n",
            "2000 of 2000, current mean bleu: 0.1913\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19126756326448108"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWEEdieaTRvM",
        "outputId": "8e21c1f2-e082-4d3a-f040-ebb3fc1c9193"
      },
      "source": [
        "average_bleu_ode(odemodel, test_dataloader, print_results=True)"
      ],
      "id": "xWEEdieaTRvM",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 of 2000, current mean bleu: 0.1421\n",
            "100 of 2000, current mean bleu: 0.1958\n",
            "200 of 2000, current mean bleu: 0.1912\n",
            "300 of 2000, current mean bleu: 0.1900\n",
            "400 of 2000, current mean bleu: 0.1910\n",
            "500 of 2000, current mean bleu: 0.1928\n",
            "600 of 2000, current mean bleu: 0.1933\n",
            "700 of 2000, current mean bleu: 0.1938\n",
            "800 of 2000, current mean bleu: 0.1938\n",
            "900 of 2000, current mean bleu: 0.1933\n",
            "1000 of 2000, current mean bleu: 0.1932\n",
            "1100 of 2000, current mean bleu: 0.1928\n",
            "1200 of 2000, current mean bleu: 0.1930\n",
            "1300 of 2000, current mean bleu: 0.1923\n",
            "1400 of 2000, current mean bleu: 0.1919\n",
            "1500 of 2000, current mean bleu: 0.1920\n",
            "1600 of 2000, current mean bleu: 0.1916\n",
            "1700 of 2000, current mean bleu: 0.1916\n",
            "1800 of 2000, current mean bleu: 0.1916\n",
            "1900 of 2000, current mean bleu: 0.1917\n",
            "2000 of 2000, current mean bleu: 0.1916\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19159754389063433"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 0
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoBYSe2zTSeb"
      },
      "source": [
        ""
      ],
      "id": "uoBYSe2zTSeb",
      "execution_count": null,
      "outputs": []
    }
  ]
}